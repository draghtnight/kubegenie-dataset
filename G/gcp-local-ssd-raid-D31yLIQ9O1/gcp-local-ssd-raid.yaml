apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-gcp-local-ssd-raid
  labels:
    helm.sh/chart: gcp-local-ssd-raid-0.1.3
    app.kubernetes.io/name: gcp-local-ssd-raid
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 0.1.0
    app.kubernetes.io/managed-by: Helm
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-gcp-local-ssd-raid-config
data:
  setPVOwnerRef: "true"
  nodeLabelsForPV: |
    - kubernetes.io/hostname
  storageClassMap: |
    local-storage:
      hostDir: /mnt/disks
      mountDir: /mnt/disks
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: release-name-gcp-local-ssd-raid-clusterrole
rules:
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: release-name-gcp-local-ssd-raid-pv-binding
subjects:
  - kind: ServiceAccount
    name: release-name-gcp-local-ssd-raid
    namespace: default
roleRef:
  kind: ClusterRole
  name: system:persistent-volume-provisioner
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: release-name-gcp-local-ssd-raid-node-binding
subjects:
  - kind: ServiceAccount
    name: release-name-gcp-local-ssd-raid
    namespace: default
roleRef:
  kind: ClusterRole
  name: release-name-gcp-local-ssd-raid-clusterrole
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: release-name-gcp-local-ssd-raid
  labels:
    helm.sh/chart: gcp-local-ssd-raid-0.1.3
    app.kubernetes.io/name: gcp-local-ssd-raid
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 0.1.0
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: gcp-local-ssd-raid
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      labels:
        app.kubernetes.io/name: gcp-local-ssd-raid
        app.kubernetes.io/instance: release-name
    spec:
      hostPID: true
      nodeSelector:
        cloud.google.com/gke-local-ssd: "true"
      serviceAccountName: release-name-gcp-local-ssd-raid
      initContainers:
        - name: local-ssd-startup
          image: alpine
          command:
            - /bin/sh
            - -c
            - nsenter -t 1 -m -u -i -n -p -- bash -c "${STARTUP_SCRIPT}"
          securityContext:
            privileged: true
          volumeMounts:
            - mountPath: /mnt/disks
              name: local-disks
              mountPropagation: Bidirectional
          env:
            - name: STARTUP_SCRIPT
              value: |
                #!/usr/bin/env bash
                set -euo pipefail
                set -x

                # discard,nobarrier are required to optimize local SSD
                # performance in GCP, see
                # https://cloud.google.com/compute/docs/disks/performance#optimize_local_ssd
                mnt_opts="defaults,nodelalloc,noatime,discard,nobarrier"

                # use /var because it is writeable on COS
                if ! findmnt -n -a -l | grep /mnt/disks/ssd ; then
                  if test -f /var/ssd_mounts ; then
                    ssd_mounts=$(cat /var/ssd_mounts)
                  else
                    echo "no ssds mounted yet"
                    exit 1
                  fi
                else
                  ssd_mounts=$(findmnt -n -a -l --nofsroot | grep /mnt/disks/ssd)
                  echo "$ssd_mounts" > /var/ssd_mounts
                fi

                # Re-mount all disks as a single logical volume with a UUID
                if old_mounts=$(findmnt -n -a -l --nofsroot | grep /mnt/disks/ssd) ; then
                  echo "$old_mounts" | awk '{print $1}' | while read -r ssd ; do
                    umount "$ssd"
                    new_fstab=$(grep -v "$ssd" /etc/fstab) || echo "fstab is now empty"
                    echo "$new_fstab" > /etc/fstab
                  done
                fi
                echo "$ssd_mounts" | awk '{print $1}' | while read -r ssd ; do
                  if test -d "$ssd"; then
                    rm -r "$ssd"
                  fi
                done

                devs=$(echo "$ssd_mounts" | awk '{print $2}')
                raid_dev=/dev/md1

                # If RAID or LVM is already in use, this may have been re-deployed.
                # Don't try to change the disks.
                pvs=$((test -x /sbin/pvs && /sbin/pvs) || echo "")
                if ! test -e $raid_dev && ! echo "$pvs" | grep volume_all_ssds ; then
                  # wipe all devices
                  echo "$devs" | while read -r dev ; do
                    dev_basename=$(basename "$dev")
                    mkdir -p /var/dev_wiped/
                    if ! test -f /var/dev_wiped/$dev_basename ; then
                      if findmnt -n -a -l --nofsroot | grep "$dev" ; then
                        echo "$dev" already individually mounted
                        exit 1
                      fi
                      /sbin/wipefs --all "$dev"
                      touch /var/dev_wiped/$dev_basename
                    fi
                  done

                  # Don't combine if there is 1 disk or the environment variable is set.
                  # lvm and mdadm do have overhead, so don't use them if there is just 1 disk
                  # remount with uuid, set mount options (nobarrier), and exit
                  NO_COMBINE_LOCAL_SSD="${NO_COMBINE_LOCAL_SSD:-""}"
                  if ! test -z "$NO_COMBINE_LOCAL_SSD" || [ "$(echo "$devs" | wc -l)" -eq 1 ] ; then
                    echo "$devs" | while read -r dev ; do
                      if ! findmnt -n -a -l --nofsroot | grep "$dev" ; then
                        if ! uuid=$(blkid -s UUID -o value "$dev") ; then
                          mkfs.ext4  "$dev"
                          uuid=$(blkid -s UUID -o value "$dev")
                        fi
                        mnt_dir="/mnt/disks/$uuid"
                        mkdir -p "$mnt_dir"
                        if ! grep "$uuid" /etc/fstab ; then
                          echo "UUID=$uuid $mnt_dir ext4 $mnt_opts" >> /etc/fstab
                        fi
                        mount -U "$uuid" -t ext4 --target "$mnt_dir" --options "$mnt_opts"
                        chmod a+w "$mnt_dir"
                      fi
                    done

                    exit 0
                  fi
                fi

                new_dev=
                USE_LVM="${USE_LVM:-""}"
                # If RAID is available use it because it performs better than LVM
                if test -e $raid_dev || (test -x /sbin/mdadm && test -z "$USE_LVM") ; then
                  if ! test -e $raid_dev ; then
                    echo "$devs" | xargs /sbin/mdadm --create $raid_dev --level=0 --raid-devices=$(echo "$devs" | wc -l)
                    sudo mkfs.ext4  -F $raid_dev
                    new_dev=$raid_dev
                  fi
                else
                  if ! echo "$pvs" | grep volume_all_ssds ; then
                    echo "$devs" | xargs /sbin/pvcreate
                  fi
                  /sbin/pvdisplay
                  if ! /sbin/vgs | grep volume_all_ssds ; then
                    echo "$devs" | xargs /sbin/vgcreate volume_all_ssds
                  fi
                  /sbin/vgdisplay
                  if ! /sbin/lvs | grep logical_all_ssds ; then
                    /sbin/lvcreate -l 100%FREE -n logical_all_ssds volume_all_ssds
                  fi
                  /sbin/lvdisplay
                  new_dev=/dev/volume_all_ssds/logical_all_ssds
                fi

                if ! uuid=$(blkid -s UUID -o value $new_dev) ; then
                  mkfs.ext4  $new_dev
                  uuid=$(blkid -s UUID -o value $new_dev)
                fi
                mnt_dir="/mnt/disks/$uuid"
                mkdir -p "$mnt_dir"

                if ! grep "$uuid" /etc/fstab ; then
                  echo "UUID=$uuid $mnt_dir ext4 $mnt_opts" >> /etc/fstab
                fi
                mount -U "$uuid" -t ext4 --target "$mnt_dir" --options "$mnt_opts"
                chmod a+w "$mnt_dir"
      containers:
        - image: quay.io/external_storage/local-volume-provisioner:v2.3.2
          name: provisioner
          securityContext:
            privileged: true
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                "": NET_RAW
            readOnlyRootFilesystem: true
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
            limits:
              cpu: 100m
              memory: 100Mi
          env:
            - name: MY_NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: MY_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: JOB_CONTAINER_IMAGE
              value: quay.io/external_storage/local-volume-provisioner:v2.3.2
          volumeMounts:
            - mountPath: /etc/provisioner/config
              name: provisioner-config
              readOnly: true
            - mountPath: /mnt/disks
              name: local-disks
              mountPropagation: HostToContainer
      tolerations:
        - effect: NoSchedule
          operator: Exists
      volumes:
        - name: provisioner-config
          configMap:
            name: release-name-gcp-local-ssd-raid-config
        - name: local-disks
          hostPath:
            path: /mnt/disks
