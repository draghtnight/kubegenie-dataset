[
  {
    "apiVersion": "v1",
    "kind": "ConfigMap",
    "metadata": {
      "name": "release-log-elasticsearch-configmap",
      "namespace": "lfEHqu",
      "labels": {
        "app": "log-elasticsearch",
        "chart": "log-elasticsearch-6.0.0",
        "release": "release",
        "heritage": "Helm"
      }
    },
    "data": {
      "elasticsearch.yml": "# Copyright © 2018  AT&T, Amdocs, Bell Canada Intellectual Property.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#       http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ======================== Elasticsearch Configuration =========================\n#\n# NOTE: Elasticsearch comes with reasonable defaults for most settings.\n#       Before you set out to tweak and tune the configuration, make sure you\n#       understand what are you trying to accomplish and the consequences.\n#\n# The primary way of configuring a node is via this file. This template lists\n# the most important settings you may want to configure for a production cluster.\n#\n# Please consult the documentation for further information on configuration options:\n# https://www.elastic.co/guide/en/elasticsearch/reference/index.html\n#\n# ---------------------------------- Cluster -----------------------------------\n#\n# Name of the Elasticsearch cluster.\n# A node can only join a cluster when it shares its cluster.name with all the other nodes in the cluster.\n# The default name is elasticsearch, but you should change it to an appropriate name which describes the\n# purpose of the cluster.\n#\ncluster.name: \"onap-log\"\n#\n# The port that other nodes in the cluster should use when communicating with this node.\n# Required for Elasticsearch's nodes running on different cluster nodes.\n# More : https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-transport.html\n#transport.publish_port:$transport.publish_port\n#\n# The host address to publish for nodes in the cluster to connect to.\n# Required for Elasticsearch's nodes running on different cluster nodes.\n# More : https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-transport.html\n#transport.publish_host:$transport.publish_host\n#\n# ------------------------------------ Node ------------------------------------\n#\n# It is better to provide different meaningfull names fot different elastic nodes.\n# By default, Elasticsearch will take the 7 first character of the randomly generated uuid used as the node id.\n# Note that the node id is persisted and does not change when a node restarts\n#\n#node.name: $node.name\n#\n# Add custom attributes to the node:\n#\n#node.attr.rack: r1\n#\n# ----------------------------------- Paths ------------------------------------\n#\n# The location of the data files of each index / shard allocated on the node. Can hold multiple locations separated by coma.\n# In production, we should not keep this default to \"/elasticsearch/data\", as on upgrading Elasticsearch, directory structure\n# may change & can deal to data loss.\npath.data: /usr/share/elasticsearch/data\n#\n# Elasticsearch's log files location. In production, we should not keep this default to \"/elasticsearch/logs\",\n# as on upgrading Elasticsearch, directory structure may change.\npath.logs: /usr/share/elasticsearch/logs\n#\n# ----------------------------------- Memory -----------------------------------\n#\n# It is vitally important to the health of your node that none of the JVM is ever swapped out to disk.\n# Lock the memory on startup.\n#\nbootstrap.memory_lock: false\n#\n# Make sure that the heap size is set to about half the memory available\n# on the system and that the owner of the process is allowed to use this\n# limit.\n#\n# Elasticsearch performs poorly when the system is swapping the memory.\n#\n# ---------------------------------- Network -----------------------------------\n#\n# Set the bind address to a specific IP (IPv4 or IPv6):\n# In order to communicate and to form a cluster with nodes on other servers, your node will need to bind to a\n# non-loopback address.\nnetwork.host: 0.0.0.0\n#\n# Set a custom port for HTTP: If required, default is 9200-9300\n#\n#http.port: $http.port\n#\n# For more information, consult the network module documentation.\n#\n# --------------------------------- Discovery ----------------------------------\n#\n# Pass an initial list of hosts to perform discovery when new node is started\n# To form a cluster with nodes on other servers, you have to provide a seed list of other nodes in the cluster\n# that are likely to be live and contactable.\n# By default, Elasticsearch will bind to the available loopback addresses and will scan ports 9300 to 9305 to try\n# to connect to other nodes running on the same server.\n#\n#$discovery.zen.ping.unicast.hosts\n#\n# This setting tells Elasticsearch to not elect a master unless there are enough master-eligible nodes\n# available. Only then will an election take place.\n# Prevent the \"split brain\" by configuring the majority of nodes (total number of master-eligible nodes / 2 + 1):\ndiscovery.zen.minimum_master_nodes: 1\n#\n# For more information, consult the zen discovery module documentation.\n#\n# ---------------------------------- Gateway -----------------------------------\n#\n# Block initial recovery after a full cluster restart until N nodes are started:\n#\n#gateway.recover_after_nodes: 3\n#\n# For more information, consult the gateway module documentation.\n#\n# ---------------------------------- Various -----------------------------------\n#\n# Require explicit names when deleting indices:\n#\n#action.destructive_requires_name: true\n# Set a custom port for HTTP: If required, default is 9200-9300\n# This is used for REST APIs\nhttp.port: 9200\n# Port to bind for communication between nodes. Accepts a single value or a range.\n# If a range is specified, the node will bind to the first available port in the range.\n# Defaults to 9300-9400.\n# More info:\ntransport.tcp.port: 9300\n\nxpack.graph.enabled: false\n#Set to false to disable X-Pack graph features.\n\nxpack.ml.enabled: false\n#Set to false to disable X-Pack machine learning features.\n\nxpack.monitoring.enabled: false\n#Set to false to disable X-Pack monitoring features.\n\nxpack.security.enabled: false\n#Set to false to disable X-Pack security features.\n\nxpack.watcher.enabled: false\n#Set to false to disable Watcher.\n"
    }
  },
  {
    "apiVersion": "v1",
    "kind": "ConfigMap",
    "metadata": {
      "name": "release-log-kibana",
      "namespace": "lfEHqu",
      "labels": {
        "app": "log-kibana",
        "chart": "log-kibana-6.0.0",
        "release": "release",
        "heritage": "Helm"
      }
    },
    "data": {
      "README.txt": "\"kibana-onboarding.json\" file contains initial setup of Kibana obtained using Elasticdump tool.",
      "kibana-onboarding.json": "",
      "kibana.yml": "# Copyright © 2018  AT&T, Amdocs, Bell Canada Intellectual Property.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#       http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nxpack.graph.enabled: false\n#Set to false to disable X-Pack graph features.\nxpack.ml.enabled: false\n#Set to false to disable X-Pack machine learning features.\nxpack.monitoring.enabled: false\n#Set to false to disable X-Pack monitoring features.\nxpack.reporting.enabled: false\n#Set to false to disable X-Pack reporting features.\nxpack.security.enabled: false\n#Set to false to disable X-Pack security features.\nxpack.watcher.enabled: false\n#Set to false to disable Watcher.\n# Kibana is served by a back end server. This setting specifies the port to use.\nserver.port: 5601\n\n# Specifies the address to which the Kibana server will bind. IP addresses and host names are both valid values.\n# The default is 'localhost', which usually means remote machines will not be able to connect.\n# To allow connections from remote users, set this parameter to a non-loopback address.\nserver.host: \"0\"\n\n# Enables you to specify a path to mount Kibana at if you are running behind a proxy. This only affects\n# the URLs generated by Kibana, your proxy is expected to remove the basePath value before forwarding requests\n# to Kibana. This setting cannot end in a slash.\n#server.basePath: \"\"\n\n# The maximum payload size in bytes for incoming server requests.\n#server.maxPayloadBytes: 1048576\n\n# The Kibana server's name.  This is used for display purposes.\nserver.name: \"Kibana\"\n\n# The URL of the Elasticsearch instance to use for all your queries.\nelasticsearch.url: \"http://log-es.default:9200\"\n# When this setting's value is true Kibana uses the hostname specified in the server.host\n# setting. When the value of this setting is false, Kibana uses the hostname of the host\n# that connects to this Kibana instance.\n#elasticsearch.preserveHost: true\n\n# Kibana uses an index in Elasticsearch to store saved searches, visualizations and\n# dashboards. Kibana creates a new index if the index doesn't already exist.\n#kibana.index: \".kibana\"\n\n# The default application to load.\n#kibana.defaultAppId: \"discover\"\n\n# If your Elasticsearch is protected with basic authentication, these settings provide\n# the username and password that the Kibana server uses to perform maintenance on the Kibana\n# index at startup. Your Kibana users still need to authenticate with Elasticsearch, which\n# is proxied through the Kibana server.\nelasticsearch.username: \"elastic\"\nelasticsearch.password: \"changeme\"\n# Enables SSL and paths to the PEM-format SSL certificate and SSL key files, respectively.\n# These settings enable SSL for outgoing requests from the Kibana server to the browser.\n#server.ssl.enabled: $server_ssl_enabled\n#server.ssl.certificate: $server_ssl_certificate\n#server.ssl.key: $server_ssl_key\n\n# Optional settings that provide the paths to the PEM-format SSL certificate and key files.\n# These files validate that your Elasticsearch backend uses the same key files.\n#elasticsearch.ssl.certificate: $elasticsearch_ssl_certificate\n#elasticsearch.ssl.key: $elasticsearch_ssl_key\n\n# Optional setting that enables you to specify a path to the PEM file for the certificate\n# authority for your Elasticsearch instance.\n#elasticsearch.ssl.certificateAuthorities: $elasticsearch_ssl_certificateAuthorities\n\n# To disregard the validity of SSL certificates, change this setting's value to 'none'.\n#elasticsearch.ssl.verificationMode: $elasticsearch_ssl_verificationMode\n\n# Time in milliseconds to wait for Elasticsearch to respond to pings. Defaults to the value of\n# the elasticsearch.requestTimeout setting.\n#elasticsearch.pingTimeout: 1500\n\n# Time in milliseconds to wait for responses from the back end or Elasticsearch. This value\n# must be a positive integer.\n#elasticsearch.requestTimeout: 30000\n\n# List of Kibana client-side headers to send to Elasticsearch. To send *no* client-side\n# headers, set this value to [] (an empty list).\n#elasticsearch.requestHeadersWhitelist: [ authorization ]\n\n# Header names and values that are sent to Elasticsearch. Any custom headers cannot be overwritten\n# by client-side headers, regardless of the elasticsearch.requestHeadersWhitelist configuration.\n#elasticsearch.customHeaders: {}\n\n# Time in milliseconds for Elasticsearch to wait for responses from shards. Set to 0 to disable.\n#elasticsearch.shardTimeout: 0\n\n# Time in milliseconds to wait for Elasticsearch at Kibana startup before retrying.\n#elasticsearch.startupTimeout: 5000\n\n# Specifies the path where Kibana creates the process ID file.\n#pid.file: /var/run/kibana.pid\n\n# Enables you specify a file where Kibana stores log output.\n#logging.dest: stdout\n\n# Set the value of this setting to true to suppress all logging output.\n#logging.silent: false\n\n# Set the value of this setting to true to suppress all logging output other than error messages.\n#logging.quiet: false\n\n# Set the value of this setting to true to log all events, including system usage information\n# and all requests.\n#logging.verbose: false\n\n# Set the interval in milliseconds to sample system and process performance\n# metrics. Minimum is 100ms. Defaults to 5000.\n#ops.interval: 5000\n\n# The default locale. This locale can be used in certain circumstances to substitute any missing\n# translations.\n#i18n.defaultLocale: \"en\"\n"
    }
  },
  {
    "apiVersion": "v1",
    "kind": "ConfigMap",
    "metadata": {
      "name": "release-log-logstash",
      "namespace": "lfEHqu",
      "labels": {
        "app": "log-logstash",
        "chart": "log-logstash-6.0.0",
        "release": "release",
        "heritage": "Helm"
      }
    },
    "data": {
      "logstash.yml": "# Copyright © 2018  AT&T, Amdocs, Bell Canada Intellectual Property.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#       http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nhttp.host: \"0.0.0.0\"\npipeline.workers: 3\n## Path where pipeline configurations reside\npath.config: /usr/share/logstash/pipeline\n\n## Type of queue : memeory based or file based\n#queue.type: persisted\n## Size of queue\n#queue.max_bytes: 1024mb\n## Setting true makes logstash check periodically for change in pipeline configurations\nconfig.reload.automatic: true\n\n## xpack configurations\n#xpack.monitoring.elasticsearch.url: [\"http://10.247.186.12:9200\", \"http://10.247.186.13:9200\"]\n#xpack.monitoring.elasticsearch.username: elastic\n#xpack.monitoring.elasticsearch.password: changeme\nxpack.monitoring.enabled: false\n",
      "onap-pipeline.conf": "# Copyright © 2018  AT&T, Amdocs, Bell Canada Intellectual Property.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#       http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\ninput {\n beats {\n\n ## Add a id to plugin configuration. Can be anything unique.\n id => 'beats_plugin'\n\n ######## Connection configurations ########\n\n ## The port to listen on.\n port => 5044\n\n ## Close Idle clients after the specified time in seconds. Default is 60 seconds\n #client_inactivity_timeout => 60\n\n ######## Security configurations ########\n\n ## Enable encryption. Default false.\n #ssl => $filebeat_ssl\n\n ## ssl certificate path.\n #ssl_certificate => $filebeat_ssl_certificate\n\n ## SSL key to use.\n #ssl_key => $filebeat_ssl_key\n\n ##SSL key passphrase to use.\n #ssl_key_passphrase => $filebeat_ssl_key_passphrase\n\n ## Value can be any of: none, peer, force_peer.\n #ssl_verify_mode => $filebeat_ssl_verify_mode\n\n ## Time in milliseconds for an incomplete ssl handshake to timeout. Default is 10000 ms.\n #ssl_handshake_timeout => 10000\n include_codec_tag => false\n }\n}\n\n\nfilter {\n  grok {\n    break_on_match => false\n    match => {\n      \"source\" => [\"/var/log/onap/(?<componentName>[^/]+)/\",\n                   \"/var/log/onap/%{GREEDYDATA:componentLogFile}\"\n                  ]\n    }\n  }\n\n # Filter for log4j xml events\n if \"</log4j:event>\" in [message] {\n\n   #mutate { add_field => { \"orgmsg_log4j\" => \"%{message}\" } }    # Copy of orginal msg for debug\n\n   #Filter to parse xml event and retrieve data\n   xml {\n     source => \"message\"\n     store_xml => false\n     remove_namespaces => true\n     target => \"xml_content\"\n     xpath => [ \"/event/message/text()\", \"logmsg\" ,\n                \"/event/@logger\", \"Logger\",\n                \"/event/@timestamp\", \"Timestamp\",\n                \"/event/@level\", \"loglevel\",\n                \"/event/@thread\", \"Thread\",\n                \"/event/throwable/text()\", \"Exceptionthrowable\",\n                \"/event/NDC/text()\", \"NDCs\",\n                \"/event/properties/data/@name\",\"mdcname\",\n                \"/event/properties/data/@value\",\"mdcvalue\"]\n\n    }\n\n   #Ruby filter to iterate and separate MDCs into documents\n   ruby {\n     code => '\n       $i = 0\n       $num = 0\n       if event.get(\"[mdcname]\")\n         $num = event.get(\"[mdcname]\").length\n       end\n\t   if $num != 0\n        until $i > $num do\n         if event.get(\"[mdcname]\").at($i) and event.get(\"[mdcvalue]\").at($i)\n            event.set(event.get(\"[mdcname]\").at($i), event.get(\"[mdcvalue]\").at($i))\n         end\n         $i=$i+1\n        end\n\t   end\n          '\n    }\n\n   #Validations\n   if [Exceptionthrowable]\n   {\n      mutate {\n        replace => {\n           \"exceptionmessage\" => \"%{[Exceptionthrowable]}\"\n        }\n      }\n    }\n\n   if [NDCs]\n   {\n      mutate {\n        replace => {\n          \"NDC\" => \"%{[NDCs]}\"\n        }\n      }\n   }\n\n   mutate {\n     replace => {\n        \"Logger\" =>\"%{[Logger]}\"\n        \"logmsg\" =>\"%{[logmsg]}\"\n        \"Timestamp\" =>\"%{[Timestamp]}\"\n        \"loglevel\" =>\"%{[loglevel]}\"\n        \"message\" => \"%{logmsg}\"\n        \"Thread\" => \"%{[Thread]}\"\n     }\n     remove_field => [\"mdcname\", \"mdcvalue\", \"logmsg\",\"Exceptionthrowable\",\"NDCs\"]\n   }\n\n   if [Timestamp]\n   {\n     date {\n        match => [\"Timestamp\", \"UNIX_MS\"]\n        target => \"Timestamp\"\n     }\n   }\n }\n # Filter for logback events\n else {\n\n  #mutate { add_field => { \"orgmsg\" => \"%{message}\" } }    # Copy of orginal msg for debug\n\n  mutate {\n    gsub => [\n      'message', ' = ', '=',\n      'message', '= ', '=null',\n      'message', '=\\t', '=null\t', #This null is followed by a tab\n      'message', '\\t$', '\\t'\n    ]\n  }\n  # The grok below parses the message field for all current logback patterns used by oom components.\n  # Example logback pattern: %d{&quot;yyyy-MM-dd'T'HH:mm:ss.SSSXXX&quot;, UTC}|%X{RequestId}|%msg\n  # Example grok pattern:    %{TIMESTAMP_ISO8601:Timestamp}\\|%{UUID:RequestId}\\|%{GREEDYDATA:message}\n  # Use the following command to find all logback patterns in oom directory: find oom -name \"logback*xml\" -exec grep \"property.*attern.*value\" {} \\;|sort|uniq\n  grok {\n    match => {\n      \"message\" => [\n                    \"%{TIMESTAMP_ISO8601:Timestamp}\\\\t[%{GREEDYDATA:Thread}]\\\\t%{GREEDYDATA:loglevel}\\\\t%{JAVACLASS:Logger}\\\\t%{GREEDYDATA:MDCs}\\\\t%{GREEDYDATA:message}\",\n                    \"%{TIMESTAMP_ISO8601:BeginTimestamp}\\|%{TIMESTAMP_ISO8601:EndTimestamp}\\|%{UUID:RequestId}\\|%{GREEDYDATA:ServiceInstanceId}\\|%{GREEDYDATA:Thread}\\|%{GREEDYDATA:Unknown1}\\|%{GREEDYDATA:ServiceName}\\|%{GREEDYDATA:PartnerName}\\|%{GREEDYDATA:TargetEntity}\\|%{GREEDYDATA:TargetServiceName}\\|%{GREEDYDATA:StatusCode}\\|%{GREEDYDATA:ResponseCode}\\|%{GREEDYDATA:ResponseDesc}\\|%{UUID:InstanceUUID}\\|%{GREEDYDATA:loglevel}\\|%{GREEDYDATA:AlertSeverity}\\|%{IP:ServerIPAddress}\\|%{GREEDYDATA:Timer}\\|%{HOSTNAME:ServerFQDN}\\|%{IPORHOST:RemoteHost}\\|%{GREEDYDATA:Unknown2}\\|%{GREEDYDATA:Unknown3}\\|%{GREEDYDATA:Unknown4}\\|%{GREEDYDATA:TargetVirtualEntity}\\|%{GREEDYDATA:Unknown5}\\|%{GREEDYDATA:Unknown6}\\|%{GREEDYDATA:Unknown7}\\|%{GREEDYDATA:Unknown8}\\|%{GREEDYDATA:message}\",\n                    \"%{TIMESTAMP_ISO8601:BeginTimestamp}\\|%{TIMESTAMP_ISO8601:EndTimestamp}\\|%{UUID:RequestId}\\|%{GREEDYDATA:ServiceInstanceId}\\|%{GREEDYDATA:Thread}\\|%{GREEDYDATA:Unknown1}\\|%{GREEDYDATA:ServiceName}\\|%{GREEDYDATA:PartnerName}\\|%{GREEDYDATA:StatusCode}\\|%{GREEDYDATA:ResponseCode}\\|%{GREEDYDATA:ResponseDesc}\\|%{UUID:InstanceUUID}\\|%{GREEDYDATA:loglevel}\\|%{GREEDYDATA:AlertSeverity}\\|%{IP:ServerIPAddress}\\|%{GREEDYDATA:Timer}\\|%{HOSTNAME:ServerFQDN}\\|%{IPORHOST:RemoteHost}\\|%{GREEDYDATA:Unknown2}\\|%{GREEDYDATA:Unknown3}\\|%{GREEDYDATA:Unknown4}\\|%{GREEDYDATA:Unknown5}\\|%{GREEDYDATA:Unknown6}\\|%{GREEDYDATA:Unknown7}\\|%{GREEDYDATA:Unknown8}\\|%{GREEDYDATA:message}\",\n                    \"%{TIMESTAMP_ISO8601:Timestamp}\\|%{UUID:RequestId}\\|%{GREEDYDATA:ServiceInstanceId}\\|%{GREEDYDATA:Thread}\\|%{GREEDYDATA:ServiceName}\\|%{UUID:InstanceUUID}\\|%{GREEDYDATA:loglevel}\\|%{GREEDYDATA:AlertSeverity}\\|%{IP:ServerIPAddress}\\|%{HOSTNAME:ServerFQDN}\\|%{IPORHOST:RemoteHost}\\|%{GREEDYDATA:Timer}\\|\\[%{GREEDYDATA:caller}\\]\\|%{GREEDYDATA:message}\",\n                    \"%{TIMESTAMP_ISO8601:Timestamp}\\|%{GREEDYDATA:RequestId}\\|%{GREEDYDATA:Thread}\\|%{GREEDYDATA:ServiceName}\\|%{GREEDYDATA:PartnerName}\\|%{GREEDYDATA:TargetEntity}\\|%{GREEDYDATA:TargetServiceName}\\|%{GREEDYDATA:loglevel}\\|%{GREEDYDATA:ErrorCode}\\|%{GREEDYDATA:ErrorDesc}\\|%{GREEDYDATA:message}\",\n                    \"%{TIMESTAMP_ISO8601:Timestamp}\\|%{GREEDYDATA:RequestId}\\|%{GREEDYDATA:Thread}\\|%{GREEDYDATA:ClassName}\\|%{GREEDYDATA:message}\",\n                    \"%{TIMESTAMP_ISO8601:Timestamp}\\|%{UUID:RequestId}\\|%{GREEDYDATA:message}\",\n                    \"\\[%{TIMESTAMP_ISO8601:Timestamp}\\|%{LOGLEVEL:loglevel}\\|%{GREEDYDATA:Logger}\\|%{GREEDYDATA:Thread}\\] %{GREEDYDATA:message}\"\n      ]\n    }\n    overwrite => [\"message\"]\n  }\n  # The MDCs are key value pairs that are seperated by \",\" or \"\\t\". Extra space characters are trimmed from the keys and values.\n  kv {\n    source => \"MDCs\"\n    field_split => \",\\t\"\n    trim_key => \"\\s\"\n    trim_value => \"\\s\"\n    remove_field => [ \"MDCs\" ]\n  }\n\n  if (![Timestamp] and [EndTimestamp]) {\n    mutate { add_field => { \"Timestamp\" => \"%{EndTimestamp}\" } }\n  }\n  date {\n    match => [ \"Timestamp\", \"ISO8601\", \"yyyy-MM-dd HH:mm:ss,SSS\" ]\n    target => \"Timestamp\"\n  }\n\n  mutate {\n    remove_field => [\"DuplicateRequestID\", \"Unknown1\", \"Unknown2\", \"Unknown3\", \"Unknown4\", \"Unknown5\", \"Unknown6\", \"Unknown7\", \"Unknown8\"]\n  }\n\n  if ([source] == \"/var/log/onap/sdc/sdc-be/audit.log\") {\n    #Parse kvps in message\n    kv {\n      field_split => \"\\s\"\n      trim_key => \"\\s\"\n      trim_value => \"\\s\"\n    }\n\n    #If Request Id is missing and DID is present use as RequestId\n    if (![RequestId] and [DID] =~ /.+/) {\n      mutate { add_field => { \"RequestId\" => \"%{DID}\" } }\n    }\n  }\n\n } #Close else statement for logback events\n} #Close filter\n\n\noutput {\n elasticsearch {\n id => 'onap_es'\n\n ######### Security configurations #########\n\n user => \"elastic\"\n password => \"changeme\"\n\n ## The .cer or .pem file to validate the server's certificate\n #cacert => $es_cacert\n\n ## The keystore used to present a certificate to the server. It can be either .jks or .p12\n #keystore => $es_keystore\n #keystore_password => $es_keystore_password\n\n ## Enable SSL/TLS secured communication to Elasticsearch cluster.\n ## Default is not set which in that case depends on the protocol specidfied in hosts list\n #ssl => $es_ssl\n\n ## Option to validate the server's certificate. Default is true\n #ssl_certificate_verification => $es_ssl_certificate_verification\n\n ## The JKS truststore to validate the server's certificate.\n #truststore => $es_truststore\n #truststore_password => $es_truststore_password\n\n\n ######### Elasticsearchcluster and host configurations #########\n\n ##can specify one or a list of hosts. If sniffing is set, one is enough and others will be auto-discovered\n hosts => [\"http://log-es.default:9200\"]\n\n\n ## This setting asks Elasticsearch for the list of all cluster nodes and adds them to the hosts list. Default is false.\n sniffing => true\n\n ## How long to wait, in seconds, between sniffing attempts. Default is 5 seconds.\n #sniffing_delay => 5\n\n ## Set the address of a forward HTTP proxy.\n #proxy => $es_proxy\n\n ##Use this if you must run Elasticsearch behind a proxy that remaps the root path for the Elasticsearch HTTP API lives\n #path => $es_path\n\n ######### Elasticsearch request configurations #########\n\n ## This setting defines the maximum sized bulk request Logstash will make.\n #flush_size => ?\n\n ######### Document configurations #########\n\n index => \"logstash-%{+YYYY.MM.dd}\"\n document_type => \"logs\"\n\n ## This can be used to associate child documents with a parent using the parent ID.\n #parent => \"abcd'\n }\n}\n\n"
    }
  },
  {
    "kind": "PersistentVolume",
    "apiVersion": "v1",
    "metadata": {
      "name": "release-log-elasticsearch-data",
      "namespace": "default",
      "labels": {
        "app": "log-elasticsearch",
        "chart": "log-elasticsearch-6.0.0",
        "release": "release",
        "heritage": "Helm",
        "name": "release-log-elasticsearch"
      }
    },
    "spec": {
      "capacity": {
        "storage": "1Gi"
      },
      "accessModes": [
        "ReadWriteOnce"
      ],
      "persistentVolumeReclaimPolicy": "Retain",
      "storageClassName": "release-log-elasticsearch-data",
      "hostPath": {
        "path": "/dockerdata-nfs/release/log/elasticsearch/data"
      }
    }
  },
  {
    "kind": "PersistentVolumeClaim",
    "apiVersion": "v1",
    "metadata": {
      "name": "release-log-elasticsearch",
      "namespace": "default",
      "labels": {
        "app": "log-elasticsearch",
        "chart": "log-elasticsearch-6.0.0",
        "release": "release",
        "heritage": "Helm"
      }
    },
    "spec": {
      "accessModes": [
        "ReadWriteOnce"
      ],
      "storageClassName": "release-log-elasticsearch-data",
      "resources": {
        "requests": {
          "storage": "1Gi"
        }
      }
    }
  },
  {
    "apiVersion": "v1",
    "kind": "Service",
    "metadata": {
      "name": "log-es",
      "namespace": "lfEHqu",
      "labels": {
        "app": "log-elasticsearch",
        "chart": "log-elasticsearch-6.0.0",
        "release": "release",
        "heritage": "Helm"
      },
      "annotations": null
    },
    "spec": {
      "type": "NodePort",
      "ports": [
        {
          "port": 9200,
          "nodePort": 30254,
          "name": "log-es"
        }
      ],
      "selector": {
        "app": "log-elasticsearch",
        "release": "release"
      }
    }
  },
  {
    "apiVersion": "v1",
    "kind": "Service",
    "metadata": {
      "name": "log-es-tcp",
      "namespace": "lfEHqu",
      "labels": {
        "app": "log-elasticsearch",
        "chart": "log-elasticsearch-6.0.0",
        "release": "release",
        "heritage": "Helm"
      },
      "annotations": null
    },
    "spec": {
      "type": "ClusterIP",
      "ports": [
        {
          "port": 9300,
          "targetPort": 9300,
          "name": "log-es-tcp"
        }
      ],
      "selector": {
        "app": "log-elasticsearch",
        "release": "release"
      }
    }
  },
  {
    "apiVersion": "v1",
    "kind": "Service",
    "metadata": {
      "name": "log-kibana",
      "namespace": "lfEHqu",
      "labels": {
        "app": "log-kibana",
        "chart": "log-kibana-6.0.0",
        "release": "release",
        "heritage": "Helm"
      },
      "annotations": null
    },
    "spec": {
      "type": "NodePort",
      "ports": [
        {
          "port": 5601,
          "nodePort": 30253,
          "name": "log-kibana"
        }
      ],
      "selector": {
        "app": "log-kibana",
        "release": "release"
      }
    }
  },
  {
    "apiVersion": "v1",
    "kind": "Service",
    "metadata": {
      "name": "log-ls",
      "namespace": "lfEHqu",
      "labels": {
        "app": "log-logstash",
        "chart": "log-logstash-6.0.0",
        "release": "release",
        "heritage": "Helm"
      },
      "annotations": null
    },
    "spec": {
      "type": "NodePort",
      "ports": [
        {
          "port": 5044,
          "nodePort": 30255,
          "name": "log-ls"
        }
      ],
      "selector": {
        "app": "log-logstash",
        "release": "release"
      }
    }
  },
  {
    "apiVersion": "v1",
    "kind": "Service",
    "metadata": {
      "name": "log-ls-http",
      "namespace": "lfEHqu",
      "labels": {
        "app": "log-logstash",
        "chart": "log-logstash-6.0.0",
        "release": "release",
        "heritage": "Helm"
      },
      "annotations": null
    },
    "spec": {
      "type": "ClusterIP",
      "ports": [
        {
          "port": 9600,
          "targetPort": 9600,
          "name": "log-ls-http"
        }
      ],
      "selector": {
        "app": "log-logstash",
        "release": "release"
      }
    }
  },
  {
    "apiVersion": "extensions/v1beta1",
    "kind": "Deployment",
    "metadata": {
      "name": "release-log-elasticsearch",
      "namespace": "lfEHqu",
      "labels": {
        "app": "log-elasticsearch",
        "chart": "log-elasticsearch-6.0.0",
        "release": "release",
        "heritage": "Helm"
      }
    },
    "spec": {
      "replicas": 1,
      "template": {
        "metadata": {
          "labels": {
            "app": "log-elasticsearch",
            "release": "release"
          }
        },
        "spec": {
          "initContainers": [
            {
              "command": [
                "/bin/sh",
                "-c",
                "sysctl -w vm.max_map_count=262144\nmkdir -p /logroot/elasticsearch/logs\nmkdir -p /logroot/elasticsearch/data\nchmod -R 777 /logroot/elasticsearch\nchown -R root:root /logroot\n"
              ],
              "env": [
                {
                  "name": "NAMESPACE",
                  "valueFrom": {
                    "fieldRef": {
                      "apiVersion": "v1",
                      "fieldPath": "metadata.namespace"
                    }
                  }
                }
              ],
              "securityContext": {
                "privileged": true
              },
              "image": "registry.hub.docker.com/library/busybox:latest",
              "imagePullPolicy": "Always",
              "name": "init-sysctl",
              "volumeMounts": [
                {
                  "name": "release-log-elasticsearch-logs",
                  "mountPath": "/logroot/"
                }
              ]
            }
          ],
          "containers": [
            {
              "name": "log-elasticsearch",
              "image": "docker.elastic.co/elasticsearch/elasticsearch:5.5.0",
              "imagePullPolicy": "Always",
              "resources": {
                "limits": {
                  "cpu": 1,
                  "memory": "4Gi"
                },
                "requests": {
                  "cpu": 1,
                  "memory": "2Gi"
                }
              },
              "ports": [
                {
                  "containerPort": 9200,
                  "name": "log-es"
                },
                {
                  "containerPort": 9300,
                  "name": "log-es-tcp"
                }
              ],
              "livenessProbe": {
                "tcpSocket": {
                  "port": 9200
                },
                "initialDelaySeconds": 120,
                "periodSeconds": 10
              },
              "readinessProbe": {
                "tcpSocket": {
                  "port": 9300
                },
                "initialDelaySeconds": 120,
                "periodSeconds": 10
              },
              "env": null,
              "volumeMounts": [
                {
                  "mountPath": "/etc/localtime",
                  "name": "localtime",
                  "readOnly": true
                },
                {
                  "mountPath": "/usr/share/elasticsearch/config/elasticsearch.yml",
                  "name": "release-log-elasticsearch-config",
                  "subPath": "elasticsearch.yml"
                },
                {
                  "mountPath": "/usr/share/elasticsearch/data/",
                  "name": "release-log-elasticsearch-data"
                }
              ]
            }
          ],
          "volumes": [
            {
              "name": "localtime",
              "hostPath": {
                "path": "/etc/localtime"
              }
            },
            {
              "name": "release-log-elasticsearch-config",
              "configMap": {
                "name": "release-log-elasticsearch-configmap",
                "items": [
                  {
                    "key": "elasticsearch.yml",
                    "path": "elasticsearch.yml"
                  }
                ]
              }
            },
            {
              "name": "release-log-elasticsearch-data",
              "persistentVolumeClaim": {
                "claimName": "release-log-elasticsearch"
              }
            },
            {
              "name": "release-log-elasticsearch-logs",
              "hostPath": {
                "path": "/dockerdata-nfs/release/log"
              }
            }
          ],
          "imagePullSecrets": [
            {
              "name": "default-docker-registry-key"
            }
          ]
        }
      }
    }
  },
  {
    "apiVersion": "extensions/v1beta1",
    "kind": "Deployment",
    "metadata": {
      "name": "release-log-kibana",
      "namespace": "lfEHqu",
      "labels": {
        "app": "log-kibana",
        "chart": "log-kibana-6.0.0",
        "release": "release",
        "heritage": "Helm"
      }
    },
    "spec": {
      "replicas": 1,
      "template": {
        "metadata": {
          "labels": {
            "app": "log-kibana",
            "release": "release"
          }
        },
        "spec": {
          "initContainers": [
            {
              "command": [
                "/root/ready.py"
              ],
              "args": [
                "--container-name",
                "log-elasticsearch"
              ],
              "env": [
                {
                  "name": "NAMESPACE",
                  "valueFrom": {
                    "fieldRef": {
                      "apiVersion": "v1",
                      "fieldPath": "metadata.namespace"
                    }
                  }
                }
              ],
              "image": "oomk8s/readiness-check:2.0.0",
              "imagePullPolicy": "Always",
              "name": "log-kibana-readiness"
            },
            {
              "args": [
                "--input=/config/kibana-onboarding.json",
                "--output=http://log-es.default:9200/.kibana"
              ],
              "env": [
                {
                  "name": "NAMESPACE",
                  "valueFrom": {
                    "fieldRef": {
                      "apiVersion": "v1",
                      "fieldPath": "metadata.namespace"
                    }
                  }
                }
              ],
              "image": "docker.io/taskrabbit/elasticsearch-dump",
              "imagePullPolicy": "Always",
              "name": "log-kibana-elasticdump",
              "volumeMounts": [
                {
                  "mountPath": "/config/kibana-onboarding.json",
                  "name": "release-log-kibana",
                  "subPath": "kibana-onboarding.json"
                }
              ]
            }
          ],
          "containers": [
            {
              "name": "log-kibana",
              "image": "docker.elastic.co/kibana/kibana:5.5.0",
              "imagePullPolicy": "Always",
              "resources": {
                "limits": {
                  "cpu": 2,
                  "memory": "4Gi"
                },
                "requests": {
                  "cpu": 1,
                  "memory": "2Gi"
                }
              },
              "ports": [
                {
                  "containerPort": 5601,
                  "name": "log-kibana"
                }
              ],
              "readinessProbe": {
                "httpGet": {
                  "path": "/",
                  "port": 5601
                },
                "initialDelaySeconds": 300,
                "periodSeconds": 10,
                "timeoutSeconds": 1
              },
              "livenessProbe": {
                "httpGet": {
                  "path": "/",
                  "port": 5601
                },
                "initialDelaySeconds": 300,
                "periodSeconds": 10,
                "timeoutSeconds": 1
              },
              "env": null,
              "volumeMounts": [
                {
                  "mountPath": "/etc/localtime",
                  "name": "localtime",
                  "readOnly": true
                },
                {
                  "mountPath": "/usr/share/kibana/config/",
                  "name": "release-log-kibana"
                }
              ]
            }
          ],
          "volumes": [
            {
              "name": "localtime",
              "hostPath": {
                "path": "/etc/localtime"
              }
            },
            {
              "name": "release-log-kibana",
              "configMap": {
                "name": "release-log-kibana",
                "items": [
                  {
                    "key": "kibana.yml",
                    "path": "kibana.yml"
                  },
                  {
                    "key": "kibana-onboarding.json",
                    "path": "kibana-onboarding.json"
                  }
                ]
              }
            }
          ],
          "imagePullSecrets": [
            {
              "name": "default-docker-registry-key"
            }
          ]
        }
      }
    }
  },
  {
    "apiVersion": "extensions/v1beta1",
    "kind": "Deployment",
    "metadata": {
      "name": "release-log-logstash",
      "namespace": "lfEHqu",
      "labels": {
        "app": "log-logstash",
        "chart": "log-logstash-6.0.0",
        "release": "release",
        "heritage": "Helm"
      }
    },
    "spec": {
      "replicas": 3,
      "template": {
        "metadata": {
          "labels": {
            "app": "log-logstash",
            "release": "release"
          }
        },
        "spec": {
          "initContainers": [
            {
              "command": [
                "/root/ready.py"
              ],
              "args": [
                "--container-name",
                "log-elasticsearch"
              ],
              "env": [
                {
                  "name": "NAMESPACE",
                  "valueFrom": {
                    "fieldRef": {
                      "apiVersion": "v1",
                      "fieldPath": "metadata.namespace"
                    }
                  }
                }
              ],
              "image": "oomk8s/readiness-check:2.0.0",
              "imagePullPolicy": "Always",
              "name": "log-logstash-readiness"
            }
          ],
          "containers": [
            {
              "name": "log-logstash",
              "image": "docker.elastic.co/logstash/logstash:5.4.3",
              "imagePullPolicy": "Always",
              "resources": {
                "limits": {
                  "cpu": 1,
                  "memory": "2Gi"
                },
                "requests": {
                  "cpu": 0.5,
                  "memory": "1Gi"
                }
              },
              "ports": [
                {
                  "containerPort": 5044,
                  "name": "log-ls"
                },
                {
                  "containerPort": 9600,
                  "name": "log-ls-http"
                }
              ],
              "readinessProbe": {
                "tcpSocket": {
                  "port": 5044
                },
                "initialDelaySeconds": 10,
                "periodSeconds": 10
              },
              "livenessProbe": {
                "tcpSocket": {
                  "port": 5044
                },
                "initialDelaySeconds": 120,
                "periodSeconds": 10
              },
              "env": null,
              "volumeMounts": [
                {
                  "mountPath": "/etc/localtime",
                  "name": "localtime",
                  "readOnly": true
                },
                {
                  "mountPath": "/usr/share/logstash/config/",
                  "name": "release-log-logstash-config"
                },
                {
                  "mountPath": "/usr/share/logstash/pipeline/",
                  "name": "release-log-logstash-pipeline"
                }
              ]
            }
          ],
          "volumes": [
            {
              "name": "localtime",
              "hostPath": {
                "path": "/etc/localtime"
              }
            },
            {
              "name": "release-log-logstash-config",
              "configMap": {
                "name": "release-log-logstash",
                "items": [
                  {
                    "key": "logstash.yml",
                    "path": "logstash.yml"
                  }
                ]
              }
            },
            {
              "name": "release-log-logstash-pipeline",
              "configMap": {
                "name": "release-log-logstash",
                "items": [
                  {
                    "key": "onap-pipeline.conf",
                    "path": "onap-pipeline.conf"
                  }
                ]
              }
            }
          ],
          "imagePullSecrets": [
            {
              "name": "default-docker-registry-key"
            }
          ]
        }
      }
    }
  }
]