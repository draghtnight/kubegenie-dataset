apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: elasticsearch-master-pdb
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app: elasticsearch-master
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-release-name-zeebe-gateway
  labels:
    app.kubernetes.io/name: zeebe-cluster-helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: gateway
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-zeebe-cluster-helm
  labels:
    app.kubernetes.io/name: zeebe-cluster-helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: broker
    app: zeebe
---
kind: ConfigMap
metadata:
  name: release-name-zeebe-cluster-helm
  labels:
    app.kubernetes.io/name: zeebe-cluster-helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
apiVersion: v1
data:
  startup.sh: "#!/usr/bin/env bash\nset -eux -o pipefail\n\nexport ZEEBE_BROKER_NETWORK_ADVERTISEDHOST=${ZEEBE_BROKER_NETWORK_ADVERTISEDHOST:-$(hostname -f)}\nexport ZEEBE_BROKER_CLUSTER_NODEID=${ZEEBE_BROKER_CLUSTER_NODEID:-${K8S_POD_NAME##*-}}\n\n# As the number of replicas or the DNS is not obtainable from the downward API yet,\n# defined them here based on conventions\nexport ZEEBE_BROKER_CLUSTER_CLUSTERSIZE=${ZEEBE_BROKER_CLUSTER_CLUSTERSIZE:-1}\ncontactPointPrefix=${K8S_POD_NAME%-*}\ncontactPoints=${ZEEBE_BROKER_CLUSTER_INITIALCONTACTPOINTS:-\"\"}\nif [[ -z \"${contactPoints}\" ]]; then\n  for ((i=0; i<${ZEEBE_BROKER_CLUSTER_CLUSTERSIZE}; i++))\n  do\n    contactPoints=\"${contactPoints},${contactPointPrefix}-$i.$(hostname -d):26502\"\n  done\n\n  export ZEEBE_BROKER_CLUSTER_INITIALCONTACTPOINTS=\"${contactPoints}\"\nfi\n\nif [ \"$(ls -A /exporters/)\" ]; then\n  mkdir /usr/local/zeebe/exporters/\n  cp -a /exporters/*.jar /usr/local/zeebe/exporters/\nelse  \n  echo \"No exporters available.\"\nfi\n\nexec /usr/local/zeebe/bin/broker\n"
  application.yaml: ""
  broker-log4j2.xml: ""
  gateway-log4j2.xml: ""
---
kind: Service
apiVersion: v1
metadata:
  name: elasticsearch-master
  labels:
    heritage: Helm
    release: release-name
    chart: elasticsearch
    app: elasticsearch-master
  annotations: {}
spec:
  type: ClusterIP
  selector:
    release: release-name
    chart: elasticsearch
    app: elasticsearch-master
  ports:
    - name: http
      protocol: TCP
      port: 9200
    - name: transport
      protocol: TCP
      port: 9300
---
kind: Service
apiVersion: v1
metadata:
  name: elasticsearch-master-headless
  labels:
    heritage: Helm
    release: release-name
    chart: elasticsearch
    app: elasticsearch-master
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  clusterIP: None
  publishNotReadyAddresses: true
  selector:
    app: elasticsearch-master
  ports:
    - name: http
      port: 9200
    - name: transport
      port: 9300
---
apiVersion: v1
kind: Service
metadata:
  name: release-name-zeebe-gateway
  labels:
    app.kubernetes.io/name: zeebe-cluster-helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: gateway
  annotations: {}
spec:
  type: ClusterIP
  selector:
    app.kubernetes.io/name: zeebe-cluster-helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: gateway
  ports:
    - port: 9600
      protocol: TCP
      name: http
    - port: 26500
      protocol: TCP
      name: gateway
---
apiVersion: v1
kind: Service
metadata:
  name: release-name-zeebe
  labels:
    app.kubernetes.io/name: zeebe-cluster-helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: broker
    app: zeebe
  annotations: {}
spec:
  clusterIP: None
  publishNotReadyAddresses: true
  type: ClusterIP
  ports:
    - port: 9600
      protocol: TCP
      name: http
    - port: 26502
      protocol: TCP
      name: internal
    - port: 26501
      protocol: TCP
      name: command
  selector:
    app.kubernetes.io/name: zeebe-cluster-helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: broker
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-zeebe-gateway
  labels:
    app.kubernetes.io/name: zeebe-cluster-helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: gateway
  annotations: null
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: zeebe-cluster-helm
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/component: gateway
  template:
    metadata:
      labels:
        app.kubernetes.io/name: zeebe-cluster-helm
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: gateway
      annotations: null
    spec:
      containers:
        - name: zeebe-cluster-helm
          image: camunda/zeebe:1.3.4
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 9600
              name: http
            - containerPort: 26500
              name: gateway
            - containerPort: 26502
              name: internal
          env:
            - name: ZEEBE_STANDALONE_GATEWAY
              value: "true"
            - name: ZEEBE_GATEWAY_CLUSTER_CLUSTERNAME
              value: release-name-zeebe
            - name: ZEEBE_GATEWAY_CLUSTER_MEMBERID
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: ZEEBE_LOG_LEVEL
              value: info
            - name: JAVA_TOOL_OPTIONS
              value: -XX:MaxRAMPercentage=25.0 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/usr/local/zeebe/data -XX:ErrorFile=/usr/local/zeebe/data/zeebe_error%p.log -XX:+ExitOnOutOfMemoryError
            - name: ZEEBE_GATEWAY_CLUSTER_CONTACTPOINT
              value: release-name-zeebe:26502
            - name: ZEEBE_GATEWAY_NETWORK_HOST
              value: 0.0.0.0
            - name: ZEEBE_GATEWAY_NETWORK_PORT
              value: "26500"
            - name: ZEEBE_GATEWAY_CLUSTER_HOST
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: ZEEBE_GATEWAY_CLUSTER_PORT
              value: "26502"
            - name: ZEEBE_GATEWAY_MONITORING_HOST
              value: 0.0.0.0
            - name: ZEEBE_GATEWAY_MONITORING_PORT
              value: "9600"
          volumeMounts: null
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                "": NET_RAW
            readOnlyRootFilesystem: true
          readinessProbe:
            tcpSocket:
              port: gateway
            initialDelaySeconds: 20
            periodSeconds: 5
      volumes:
        - name: config
          configMap:
            name: release-name-zeebe-cluster-helm
            defaultMode: 484
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: elasticsearch-master
  labels:
    heritage: Helm
    release: release-name
    chart: elasticsearch
    app: elasticsearch-master
  annotations:
    esMajorVersion: "7"
spec:
  serviceName: elasticsearch-master-headless
  selector:
    matchLabels:
      app: elasticsearch-master
  replicas: 3
  podManagementPolicy: Parallel
  updateStrategy:
    type: RollingUpdate
  volumeClaimTemplates:
    - metadata:
        name: elasticsearch-master
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 30Gi
  template:
    metadata:
      name: elasticsearch-master
      labels:
        release: release-name
        chart: elasticsearch
        app: elasticsearch-master
      annotations: null
    spec:
      securityContext:
        fsGroup: 1000
        runAsUser: 1000
      automountServiceAccountToken: true
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - elasticsearch-master
              topologyKey: kubernetes.io/hostname
      terminationGracePeriodSeconds: 120
      volumes: null
      enableServiceLinks: true
      initContainers:
        - name: configure-sysctl
          securityContext:
            runAsUser: 10476
            privileged: true
            capabilities:
              drop:
                "": NET_RAW
          image: docker.elastic.co/elasticsearch/elasticsearch:7.16.2
          imagePullPolicy: IfNotPresent
          command:
            - sysctl
            - -w
            - vm.max_map_count=262144
          resources:
            seccompProfile:
              type: RuntimeDefault
      containers:
        - name: elasticsearch
          securityContext:
            capabilities:
              drop:
                - ALL
            runAsNonRoot: true
            runAsUser: 10146
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
          image: docker.elastic.co/elasticsearch/elasticsearch:7.16.2
          imagePullPolicy: IfNotPresent
          readinessProbe:
            exec:
              command:
                - bash
                - -c
                - |
                  set -e
                  # If the node is starting up wait for the cluster to be ready (request params: "wait_for_status=green&timeout=1s" )
                  # Once it has started only check that the node itself is responding
                  START_FILE=/tmp/.es_start_file

                  # Disable nss cache to avoid filling dentry cache when calling curl
                  # This is required with Elasticsearch Docker using nss < 3.52
                  export NSS_SDB_USE_CACHE=no

                  http () {
                    local path="${1}"
                    local args="${2}"
                    set -- -XGET -s

                    if [ "$args" != "" ]; then
                      set -- "$@" $args
                    fi

                    if [ -n "${ELASTIC_PASSWORD}" ]; then
                      set -- "$@" -u "elastic:${ELASTIC_PASSWORD}"
                    fi

                    curl --output /dev/null -k "$@" "http://127.0.0.1:9200${path}"
                  }

                  if [ -f "${START_FILE}" ]; then
                    echo 'Elasticsearch is already running, lets check the node is healthy'
                    HTTP_CODE=$(http "/" "-w %{http_code}")
                    RC=$?
                    if [[ ${RC} -ne 0 ]]; then
                      echo "curl --output /dev/null -k -XGET -s -w '%{http_code}' \${BASIC_AUTH} http://127.0.0.1:9200/ failed with RC ${RC}"
                      exit ${RC}
                    fi
                    # ready if HTTP code 200, 503 is tolerable if ES version is 6.x
                    if [[ ${HTTP_CODE} == "200" ]]; then
                      exit 0
                    elif [[ ${HTTP_CODE} == "503" && "7" == "6" ]]; then
                      exit 0
                    else
                      echo "curl --output /dev/null -k -XGET -s -w '%{http_code}' \${BASIC_AUTH} http://127.0.0.1:9200/ failed with HTTP code ${HTTP_CODE}"
                      exit 1
                    fi

                  else
                    echo 'Waiting for elasticsearch cluster to become ready (request params: "wait_for_status=green&timeout=1s" )'
                    if http "/_cluster/health?wait_for_status=green&timeout=1s" "--fail" ; then
                      touch ${START_FILE}
                      exit 0
                    else
                      echo 'Cluster is not yet ready (request params: "wait_for_status=green&timeout=1s" )'
                      exit 1
                    fi
                  fi
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 3
            timeoutSeconds: 5
          ports:
            - name: http
              containerPort: 9200
            - name: transport
              containerPort: 9300
          resources:
            limits:
              cpu: 1000m
              memory: 2Gi
            requests:
              cpu: 1000m
              memory: 2Gi
          env:
            - name: node.name
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: cluster.initial_master_nodes
              value: elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,
            - name: discovery.seed_hosts
              value: elasticsearch-master-headless
            - name: cluster.name
              value: elasticsearch
            - name: network.host
              value: 0.0.0.0
            - name: cluster.deprecation_indexing.enabled
              value: "false"
            - name: node.data
              value: "true"
            - name: node.ingest
              value: "true"
            - name: node.master
              value: "true"
            - name: node.ml
              value: "true"
            - name: node.remote_cluster_client
              value: "true"
          volumeMounts:
            - name: elasticsearch-master
              mountPath: /usr/share/elasticsearch/data
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-zeebe
  labels:
    app.kubernetes.io/name: zeebe-cluster-helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: broker
    app: zeebe
  annotations: null
spec:
  replicas: 3
  selector:
    matchLabels:
      app.kubernetes.io/name: zeebe-cluster-helm
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/component: broker
  serviceName: release-name-zeebe
  updateStrategy:
    type: RollingUpdate
  podManagementPolicy: Parallel
  template:
    metadata:
      labels:
        app.kubernetes.io/name: zeebe-cluster-helm
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: broker
      annotations: null
    spec:
      initContainers: null
      containers:
        - name: zeebe-cluster-helm
          image: camunda/zeebe:1.3.4
          imagePullPolicy: IfNotPresent
          env:
            - name: LC_ALL
              value: C.UTF-8
            - name: ZEEBE_BROKER_CLUSTER_CLUSTERNAME
              value: release-name-zeebe
            - name: ZEEBE_LOG_LEVEL
              value: info
            - name: ZEEBE_BROKER_CLUSTER_PARTITIONSCOUNT
              value: "3"
            - name: ZEEBE_BROKER_CLUSTER_CLUSTERSIZE
              value: "3"
            - name: ZEEBE_BROKER_CLUSTER_REPLICATIONFACTOR
              value: "3"
            - name: ZEEBE_BROKER_THREADS_CPUTHREADCOUNT
              value: "2"
            - name: ZEEBE_BROKER_THREADS_IOTHREADCOUNT
              value: "2"
            - name: ZEEBE_BROKER_GATEWAY_ENABLE
              value: "false"
            - name: ZEEBE_BROKER_EXPORTERS_ELASTICSEARCH_CLASSNAME
              value: io.camunda.zeebe.exporter.ElasticsearchExporter
            - name: ZEEBE_BROKER_EXPORTERS_ELASTICSEARCH_ARGS_URL
              value: http://elasticsearch-master:9200
            - name: ZEEBE_BROKER_NETWORK_COMMANDAPI_PORT
              value: "26501"
            - name: ZEEBE_BROKER_NETWORK_INTERNALAPI_PORT
              value: "26502"
            - name: ZEEBE_BROKER_NETWORK_MONITORINGAPI_PORT
              value: "9600"
            - name: K8S_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: JAVA_TOOL_OPTIONS
              value: -XX:MaxRAMPercentage=25.0 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/usr/local/zeebe/data -XX:ErrorFile=/usr/local/zeebe/data/zeebe_error%p.log -XX:+ExitOnOutOfMemoryError
          ports:
            - containerPort: 9600
              name: http
            - containerPort: 26501
              name: command
            - containerPort: 26502
              name: internal
          readinessProbe:
            httpGet:
              path: /ready
              port: 9600
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 1000m
              memory: 4Gi
            requests:
              cpu: 500m
              memory: 2Gi
          volumeMounts:
            - name: config
              mountPath: /usr/local/zeebe/config/application.yaml
              subPath: application.yaml
            - name: config
              mountPath: /usr/local/bin/startup.sh
              subPath: startup.sh
            - name: data
              mountPath: /usr/local/zeebe/data
            - name: exporters
              mountPath: /exporters
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                "": NET_RAW
            readOnlyRootFilesystem: true
            seccompProfile:
              type: RuntimeDefault
      volumes:
        - name: config
          configMap:
            name: release-name-zeebe-cluster-helm
            defaultMode: 484
        - name: exporters
          emptyDir: {}
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - ReadWriteOnce
        storageClassName: null
        resources:
          requests:
            storage: 10Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: release-name-itwhy-test
  annotations:
    helm.sh/hook: test
    helm.sh/hook-delete-policy: hook-succeeded
spec:
  securityContext:
    fsGroup: 1000
    runAsUser: 1000
  containers:
    - name: release-name-avrnh-test
      image: docker.elastic.co/elasticsearch/elasticsearch:7.16.2
      imagePullPolicy: IfNotPresent
      command:
        - sh
        - -c
        - |
          #!/usr/bin/env bash -e
          curl -XGET --fail 'elasticsearch-master:9200/_cluster/health?wait_for_status=green&timeout=1s'
  restartPolicy: Never
---
apiVersion: v1
kind: Pod
metadata:
  name: release-name-zeebe-test-connection
  labels:
    app.kubernetes.io/name: zeebe-cluster-helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
  annotations:
    helm.sh/hook: test-success
spec:
  containers:
    - name: wget
      image: busybox
      command:
        - wget
      args:
        - release-name-zeebe:9600
  restartPolicy: Never
