apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-starboard-operator
  labels:
    helm.sh/chart: starboard-operator-0.10.4
    app.kubernetes.io/name: starboard-operator
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 0.15.4
    app.kubernetes.io/managed-by: Helm
---
apiVersion: v1
kind: Secret
metadata:
  name: starboard
  labels:
    helm.sh/chart: starboard-operator-0.10.4
    app.kubernetes.io/name: starboard-operator
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 0.15.4
    app.kubernetes.io/managed-by: Helm
---
apiVersion: v1
kind: Secret
metadata:
  name: starboard-trivy-config
  labels:
    helm.sh/chart: starboard-operator-0.10.4
    app.kubernetes.io/name: starboard-operator
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 0.15.4
    app.kubernetes.io/managed-by: Helm
data: null
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: starboard
  labels:
    helm.sh/chart: starboard-operator-0.10.4
    app.kubernetes.io/name: starboard-operator
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 0.15.4
    app.kubernetes.io/managed-by: Helm
data:
  vulnerabilityReports.scanner: Trivy
  kube-bench.imageRef: docker.io/aquasec/kube-bench:v0.6.6
  compliance.failEntriesLimit: "10"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: starboard-trivy-config
  labels:
    helm.sh/chart: starboard-operator-0.10.4
    app.kubernetes.io/name: starboard-operator
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 0.15.4
    app.kubernetes.io/managed-by: Helm
data:
  trivy.imageRef: docker.io/aquasec/trivy:0.25.2
  trivy.mode: Standalone
  trivy.severity: UNKNOWN,LOW,MEDIUM,HIGH,CRITICAL
  trivy.dbRepository: ghcr.io/aquasecurity/trivy-db
  trivy.ignoreUnfixed: "false"
  trivy.timeout: 5m0s
  trivy.resources.requests.cpu: 100m
  trivy.resources.requests.memory: 100M
  trivy.resources.limits.cpu: 500m
  trivy.resources.limits.memory: 500M
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: starboard-polaris-config
  labels:
    helm.sh/chart: starboard-operator-0.10.4
    app.kubernetes.io/name: starboard-operator
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 0.15.4
    app.kubernetes.io/managed-by: Helm
data:
  polaris.imageRef: quay.io/fairwinds/polaris:4.2
  polaris.resources.requests.cpu: 50m
  polaris.resources.requests.memory: 50M
  polaris.resources.limits.cpu: 300m
  polaris.resources.limits.memory: 300M
  polaris.config.yaml: |
    checks:
      cpuLimitsMissing: warning
      cpuRequestsMissing: warning
      dangerousCapabilities: danger
      hostIPCSet: danger
      hostNetworkSet: warning
      hostPIDSet: danger
      hostPortSet: warning
      insecureCapabilities: warning
      livenessProbeMissing: warning
      memoryLimitsMissing: warning
      memoryRequestsMissing: warning
      multipleReplicasForDeployment: ignore
      notReadOnlyRootFilesystem: warning
      priorityClassNotSet: ignore
      privilegeEscalationAllowed: danger
      pullPolicyNotAlways: ignore
      readinessProbeMissing: warning
      runAsPrivileged: danger
      runAsRootAllowed: warning
      tagNotSpecified: danger
    exemptions:
    - controllerNames:
      - kube-apiserver
      - kube-proxy
      - kube-scheduler
      - etcd-manager-events
      - kube-controller-manager
      - kube-dns
      - etcd-manager-main
      rules:
      - hostPortSet
      - hostNetworkSet
      - readinessProbeMissing
      - livenessProbeMissing
      - cpuRequestsMissing
      - cpuLimitsMissing
      - memoryRequestsMissing
      - memoryLimitsMissing
      - runAsRootAllowed
      - runAsPrivileged
      - notReadOnlyRootFilesystem
      - hostPIDSet
    - controllerNames:
      - kube-flannel-ds
      rules:
      - notReadOnlyRootFilesystem
      - runAsRootAllowed
      - notReadOnlyRootFilesystem
      - readinessProbeMissing
      - livenessProbeMissing
      - cpuLimitsMissing
    - controllerNames:
      - cert-manager
      rules:
      - notReadOnlyRootFilesystem
      - runAsRootAllowed
      - readinessProbeMissing
      - livenessProbeMissing
    - controllerNames:
      - cluster-autoscaler
      rules:
      - notReadOnlyRootFilesystem
      - runAsRootAllowed
      - readinessProbeMissing
    - controllerNames:
      - vpa
      rules:
      - runAsRootAllowed
      - readinessProbeMissing
      - livenessProbeMissing
      - notReadOnlyRootFilesystem
    - controllerNames:
      - datadog
      rules:
      - runAsRootAllowed
      - readinessProbeMissing
      - livenessProbeMissing
      - notReadOnlyRootFilesystem
    - controllerNames:
      - nginx-ingress-controller
      rules:
      - privilegeEscalationAllowed
      - insecureCapabilities
      - runAsRootAllowed
    - controllerNames:
      - dns-controller
      - datadog-datadog
      - kube-flannel-ds
      - kube2iam
      - aws-iam-authenticator
      - datadog
      - kube2iam
      rules:
      - hostNetworkSet
    - controllerNames:
      - aws-iam-authenticator
      - aws-cluster-autoscaler
      - kube-state-metrics
      - dns-controller
      - external-dns
      - dnsmasq
      - autoscaler
      - kubernetes-dashboard
      - install-cni
      - kube2iam
      rules:
      - readinessProbeMissing
      - livenessProbeMissing
    - controllerNames:
      - aws-iam-authenticator
      - nginx-ingress-default-backend
      - aws-cluster-autoscaler
      - kube-state-metrics
      - dns-controller
      - external-dns
      - kubedns
      - dnsmasq
      - autoscaler
      - tiller
      - kube2iam
      rules:
      - runAsRootAllowed
    - controllerNames:
      - aws-iam-authenticator
      - nginx-ingress-controller
      - nginx-ingress-default-backend
      - aws-cluster-autoscaler
      - kube-state-metrics
      - dns-controller
      - external-dns
      - kubedns
      - dnsmasq
      - autoscaler
      - tiller
      - kube2iam
      rules:
      - notReadOnlyRootFilesystem
    - controllerNames:
      - cert-manager
      - dns-controller
      - kubedns
      - dnsmasq
      - autoscaler
      - insights-agent-goldilocks-vpa-install
      - datadog
      rules:
      - cpuRequestsMissing
      - cpuLimitsMissing
      - memoryRequestsMissing
      - memoryLimitsMissing
    - controllerNames:
      - kube2iam
      - kube-flannel-ds
      rules:
      - runAsPrivileged
    - controllerNames:
      - kube-hunter
      rules:
      - hostPIDSet
    - controllerNames:
      - polaris
      - kube-hunter
      - goldilocks
      - insights-agent-goldilocks-vpa-install
      rules:
      - notReadOnlyRootFilesystem
    - controllerNames:
      - insights-agent-goldilocks-controller
      rules:
      - livenessProbeMissing
      - readinessProbeMissing
    - controllerNames:
      - insights-agent-goldilocks-vpa-install
      - kube-hunter
      rules:
      - runAsRootAllowed
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: starboard-policies-config
  labels:
    helm.sh/chart: starboard-operator-0.10.4
    app.kubernetes.io/name: starboard-operator
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 0.15.4
    app.kubernetes.io/managed-by: Helm
data:
  library.kubernetes.rego: |
    package lib.kubernetes

    default is_gatekeeper = false

    is_gatekeeper {
    	has_field(input, "review")
    	has_field(input.review, "object")
    }

    object = input {
    	not is_gatekeeper
    }

    object = input.review.object {
    	is_gatekeeper
    }

    format(msg) = gatekeeper_format {
    	is_gatekeeper
    	gatekeeper_format = {"msg": msg}
    }

    format(msg) = msg {
    	not is_gatekeeper
    }

    name = object.metadata.name

    default namespace = "default"

    namespace = object.metadata.namespace

    #annotations = object.metadata.annotations

    kind = object.kind

    is_pod {
    	kind = "Pod"
    }

    is_cronjob {
    	kind = "CronJob"
    }

    default is_controller = false

    is_controller {
    	kind = "Deployment"
    }

    is_controller {
    	kind = "StatefulSet"
    }

    is_controller {
    	kind = "DaemonSet"
    }

    is_controller {
    	kind = "ReplicaSet"
    }

    is_controller {
    	kind = "ReplicationController"
    }

    is_controller {
    	kind = "Job"
    }

    split_image(image) = [image, "latest"] {
    	not contains(image, ":")
    }

    split_image(image) = [image_name, tag] {
    	[image_name, tag] = split(image, ":")
    }

    pod_containers(pod) = all_containers {
    	keys = {"containers", "initContainers"}
    	all_containers = [c | keys[k]; c = pod.spec[k][_]]
    }

    containers[container] {
    	pods[pod]
    	all_containers = pod_containers(pod)
    	container = all_containers[_]
    }

    containers[container] {
    	all_containers = pod_containers(object)
    	container = all_containers[_]
    }

    pods[pod] {
    	is_pod
    	pod = object
    }

    pods[pod] {
    	is_controller
    	pod = object.spec.template
    }

    pods[pod] {
    	is_cronjob
    	pod = object.spec.jobTemplate.spec.template
    }

    volumes[volume] {
    	pods[pod]
    	volume = pod.spec.volumes[_]
    }

    dropped_capability(container, cap) {
    	container.securityContext.capabilities.drop[_] == cap
    }

    added_capability(container, cap) {
    	container.securityContext.capabilities.add[_] == cap
    }

    has_field(obj, field) {
    	obj[field]
    }

    no_read_only_filesystem(c) {
    	not has_field(c, "securityContext")
    }

    no_read_only_filesystem(c) {
    	has_field(c, "securityContext")
    	not has_field(c.securityContext, "readOnlyRootFilesystem")
    }

    priviledge_escalation_allowed(c) {
    	not has_field(c, "securityContext")
    }

    priviledge_escalation_allowed(c) {
    	has_field(c, "securityContext")
    	has_field(c.securityContext, "allowPrivilegeEscalation")
    }

    annotations[annotation] {
    	pods[pod]
    	annotation = pod.metadata.annotations
    }

    host_ipcs[host_ipc] {
    	pods[pod]
    	host_ipc = pod.spec.hostIPC
    }

    host_networks[host_network] {
    	pods[pod]
    	host_network = pod.spec.hostNetwork
    }

    host_pids[host_pid] {
    	pods[pod]
    	host_pid = pod.spec.hostPID
    }

    host_aliases[host_alias] {
    	pods[pod]
    	host_alias = pod.spec
    }
  library.utils.rego: |
    package lib.utils

    has_key(x, k) {
    	_ = x[k]
    }
  policy.1_host_ipc.kinds: Workload
  policy.1_host_ipc.rego: |
    package appshield.kubernetes.KSV008

    import data.lib.kubernetes

    default failHostIPC = false

    __rego_metadata__ := {
    	"id": "KSV008",
    	"avd_id": "AVD-KSV-0008",
    	"title": "Access to host IPC namespace",
    	"short_code": "no-shared-ipc-namespace",
    	"version": "v1.0.0",
    	"severity": "HIGH",
    	"type": "Kubernetes Security Check",
    	"description": "Sharing the host’s IPC namespace allows container processes to communicate with processes on the host.",
    	"recommended_actions": "Do not set 'spec.template.spec.hostIPC' to true.",
    	"url": "https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline",
    }

    __rego_input__ := {
    	"combine": false,
    	"selector": [{"type": "kubernetes"}],
    }

    # failHostIPC is true if spec.hostIPC is set to true (on all resources)
    failHostIPC {
    	kubernetes.host_ipcs[_] == true
    }

    deny[res] {
    	failHostIPC

    	msg := kubernetes.format(sprintf("%s '%s' should not set 'spec.template.spec.hostIPC' to true", [kubernetes.kind, kubernetes.name]))

    	res := {
    		"msg": msg,
    		"id": __rego_metadata__.id,
    		"title": __rego_metadata__.title,
    		"severity": __rego_metadata__.severity,
    		"type": __rego_metadata__.type,
    	}
    }
  policy.1_host_network.kinds: Workload
  policy.1_host_network.rego: |
    package appshield.kubernetes.KSV009

    import data.lib.kubernetes

    default failHostNetwork = false

    __rego_metadata__ := {
    	"id": "KSV009",
    	"avd_id": "AVD-KSV-0009",
    	"title": "Access to host network",
    	"short_code": "no-host-network",
    	"version": "v1.0.0",
    	"severity": "HIGH",
    	"type": "Kubernetes Security Check",
    	"description": "Sharing the host’s network namespace permits processes in the pod to communicate with processes bound to the host’s loopback adapter.",
    	"recommended_actions": "Do not set 'spec.template.spec.hostNetwork' to true.",
    	"url": "https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline",
    }

    __rego_input__ := {
    	"combine": false,
    	"selector": [{"type": "kubernetes"}],
    }

    # failHostNetwork is true if spec.hostNetwork is set to true (on all controllers)
    failHostNetwork {
    	kubernetes.host_networks[_] == true
    }

    deny[res] {
    	failHostNetwork

    	msg := kubernetes.format(sprintf("%s '%s' should not set 'spec.template.spec.hostNetwork' to true", [kubernetes.kind, kubernetes.name]))

    	res := {
    		"msg": msg,
    		"id": __rego_metadata__.id,
    		"title": __rego_metadata__.title,
    		"severity": __rego_metadata__.severity,
    		"type": __rego_metadata__.type,
    	}
    }
  policy.1_host_pid.kinds: Workload
  policy.1_host_pid.rego: |
    package appshield.kubernetes.KSV010

    import data.lib.kubernetes

    default failHostPID = false

    __rego_metadata__ := {
    	"id": "KSV010",
    	"avd_id": "AVD-KSV-0010",
    	"title": "Access to host PID",
    	"short_code": "no-host-pid",
    	"version": "v1.0.0",
    	"severity": "HIGH",
    	"type": "Kubernetes Security Check",
    	"description": "Sharing the host’s PID namespace allows visibility on host processes, potentially leaking information such as environment variables and configuration.",
    	"recommended_actions": "Do not set 'spec.template.spec.hostPID' to true.",
    	"url": "https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline",
    }

    __rego_input__ := {
    	"combine": false,
    	"selector": [{"type": "kubernetes"}],
    }

    # failHostPID is true if spec.hostPID is set to true (on all controllers)
    failHostPID {
    	kubernetes.host_pids[_] == true
    }

    deny[res] {
    	failHostPID

    	msg := kubernetes.format(sprintf("%s '%s' should not set 'spec.template.spec.hostPID' to true", [kubernetes.kind, kubernetes.name]))

    	res := {
    		"msg": msg,
    		"id": __rego_metadata__.id,
    		"title": __rego_metadata__.title,
    		"severity": __rego_metadata__.severity,
    		"type": __rego_metadata__.type,
    	}
    }
  policy.1_non_core_volume_types.kinds: Workload
  policy.1_non_core_volume_types.rego: |
    package appshield.kubernetes.KSV028

    import data.lib.kubernetes
    import data.lib.utils

    __rego_metadata__ := {
    	"id": "KSV028",
    	"avd_id": "AVD-KSV-0028",
    	"title": "Non-ephemeral volume types used",
    	"short_code": "no-non-ephemeral-volumes",
    	"version": "v1.0.0",
    	"severity": "LOW",
    	"type": "Kubernetes Security Check",
    	"description": "In addition to restricting HostPath volumes, usage of non-ephemeral volume types should be limited to those defined through PersistentVolumes.",
    	"recommended_actions": "Do not Set 'spec.volumes[*]' to any of the disallowed volume types.",
    	"url": "https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted",
    }

    __rego_input__ := {
    	"combine": false,
    	"selector": [{"type": "kubernetes"}],
    }

    # Add disallowed volume type
    disallowed_volume_types = [
    	"gcePersistentDisk",
    	"awsElasticBlockStore",
    	# "hostPath", Baseline detects spec.volumes[*].hostPath
    	"gitRepo",
    	"nfs",
    	"iscsi",
    	"glusterfs",
    	"rbd",
    	"flexVolume",
    	"cinder",
    	"cephFS",
    	"flocker",
    	"fc",
    	"azureFile",
    	"vsphereVolume",
    	"quobyte",
    	"azureDisk",
    	"portworxVolume",
    	"scaleIO",
    	"storageos",
    	"csi",
    ]

    # getDisallowedVolumes returns a list of volume names
    # which set volume type to any of the disallowed volume types
    getDisallowedVolumes[name] {
    	volume := kubernetes.volumes[_]
    	type := disallowed_volume_types[_]
    	utils.has_key(volume, type)
    	name := volume.name
    }

    # failVolumeTypes is true if any of volume has a disallowed
    # volume type
    failVolumeTypes {
    	count(getDisallowedVolumes) > 0
    }

    deny[res] {
    	failVolumeTypes

    	msg := kubernetes.format(sprintf("%s '%s' should set 'spec.volumes[*]' to type 'PersistentVolumeClaim'", [kubernetes.kind, kubernetes.name]))

    	res := {
    		"msg": msg,
    		"id": __rego_metadata__.id,
    		"title": __rego_metadata__.title,
    		"severity": __rego_metadata__.severity,
    		"type": __rego_metadata__.type,
    	}
    }
  policy.2_can_elevate_its_own_privileges.kinds: Workload
  policy.2_can_elevate_its_own_privileges.rego: |
    package appshield.kubernetes.KSV001

    import data.lib.kubernetes
    import data.lib.utils

    default checkAllowPrivilegeEscalation = false

    __rego_metadata__ := {
    	"id": "KSV001",
    	"avd_id": "AVD-KSV-0001",
    	"title": "Process can elevate its own privileges",
    	"short_code": "no-self-privesc",
    	"version": "v1.0.0",
    	"severity": "MEDIUM",
    	"type": "Kubernetes Security Check",
    	"description": "A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node.",
    	"recommended_actions": "Set 'set containers[].securityContext.allowPrivilegeEscalation' to 'false'.",
    	"url": "https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted",
    }

    __rego_input__ := {
    	"combine": false,
    	"selector": [{"type": "kubernetes"}],
    }

    # getNoPrivilegeEscalationContainers returns the names of all containers which have
    # securityContext.allowPrivilegeEscalation set to false.
    getNoPrivilegeEscalationContainers[container] {
    	allContainers := kubernetes.containers[_]
    	allContainers.securityContext.allowPrivilegeEscalation == false
    	container := allContainers.name
    }

    # getPrivilegeEscalationContainers returns the names of all containers which have
    # securityContext.allowPrivilegeEscalation set to true or not set.
    getPrivilegeEscalationContainers[container] {
    	container := kubernetes.containers[_].name
    	not getNoPrivilegeEscalationContainers[container]
    }

    # checkAllowPrivilegeEscalation is true if any container has
    # securityContext.allowPrivilegeEscalation set to true or not set.
    checkAllowPrivilegeEscalation {
    	count(getPrivilegeEscalationContainers) > 0
    }

    deny[res] {
    	checkAllowPrivilegeEscalation

    	msg := kubernetes.format(sprintf("Container '%s' of %s '%s' should set 'securityContext.allowPrivilegeEscalation' to false", [getPrivilegeEscalationContainers[_], kubernetes.kind, kubernetes.name]))

    	res := {
    		"msg": msg,
    		"id": __rego_metadata__.id,
    		"title": __rego_metadata__.title,
    		"severity": __rego_metadata__.severity,
    		"type": __rego_metadata__.type,
    	}
    }
  policy.2_privileged.kinds: Workload
  policy.2_privileged.rego: |
    package appshield.kubernetes.KSV017

    import data.lib.kubernetes

    default failPrivileged = false

    __rego_metadata__ := {
    	"id": "KSV017",
    	"avd_id": "AVD-KSV-0017",
    	"title": "Privileged container",
    	"short_code": "no-privileged-containers",
    	"version": "v1.0.0",
    	"severity": "HIGH",
    	"type": "Kubernetes Security Check",
    	"description": "Privileged containers share namespaces with the host system and do not offer any security. They should be used exclusively for system containers that require high privileges.",
    	"recommended_actions": "Change 'containers[].securityContext.privileged' to 'false'.",
    	"url": "https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline",
    }

    __rego_input__ := {
    	"combine": false,
    	"selector": [{"type": "kubernetes"}],
    }

    # getPrivilegedContainers returns all containers which have
    # securityContext.privileged set to true.
    getPrivilegedContainers[container] {
    	allContainers := kubernetes.containers[_]
    	allContainers.securityContext.privileged == true
    	container := allContainers.name
    }

    # failPrivileged is true if there is ANY container with securityContext.privileged
    # set to true.
    failPrivileged {
    	count(getPrivilegedContainers) > 0
    }

    deny[res] {
    	failPrivileged

    	msg := kubernetes.format(sprintf("Container '%s' of %s '%s' should set 'securityContext.privileged' to false", [getPrivilegedContainers[_], kubernetes.kind, kubernetes.name]))
    	res := {
    		"msg": msg,
    		"id": __rego_metadata__.id,
    		"title": __rego_metadata__.title,
    		"severity": __rego_metadata__.severity,
    		"type": __rego_metadata__.type,
    	}
    }
  policy.3_runs_as_root.kinds: Workload
  policy.3_runs_as_root.rego: |
    package appshield.kubernetes.KSV012

    import data.lib.kubernetes
    import data.lib.utils

    default checkRunAsNonRoot = false

    __rego_metadata__ := {
    	"id": "KSV012",
    	"avd_id": "AVD-KSV-0012",
    	"title": "Runs as root user",
    	"short_code": "no-root",
    	"version": "v1.0.0",
    	"severity": "MEDIUM",
    	"type": "Kubernetes Security Check",
    	"description": "'runAsNonRoot' forces the running image to run as a non-root user to ensure least privileges.",
    	"recommended_actions": "Set 'containers[].securityContext.runAsNonRoot' to true.",
    	"url": "https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted",
    }

    __rego_input__ := {
    	"combine": false,
    	"selector": [{"type": "kubernetes"}],
    }

    # getNonRootContainers returns the names of all containers which have
    # securityContext.runAsNonRoot set to true.
    getNonRootContainers[container] {
    	allContainers := kubernetes.containers[_]
    	allContainers.securityContext.runAsNonRoot == true
    	container := allContainers.name
    }

    # getRootContainers returns the names of all containers which have
    # securityContext.runAsNonRoot set to false or not set.
    getRootContainers[container] {
    	container := kubernetes.containers[_].name
    	not getNonRootContainers[container]
    }

    # checkRunAsNonRoot is true if securityContext.runAsNonRoot is set to false
    # or if securityContext.runAsNonRoot is not set.
    checkRunAsNonRootContainers {
    	count(getRootContainers) > 0
    }

    checkRunAsNonRootPod {
    	allPods := kubernetes.pods[_]
    	not allPods.spec.securityContext.runAsNonRoot
    }

    deny[res] {
    	checkRunAsNonRootPod

    	checkRunAsNonRootContainers

    	msg := kubernetes.format(sprintf("Container '%s' of %s '%s' should set 'securityContext.runAsNonRoot' to true", [getRootContainers[_], kubernetes.kind, kubernetes.name]))

    	res := {
    		"msg": msg,
    		"id": __rego_metadata__.id,
    		"title": __rego_metadata__.title,
    		"severity": __rego_metadata__.severity,
    		"type": __rego_metadata__.type,
    	}
    }
  policy.3_specific_capabilities_added.kinds: Workload
  policy.3_specific_capabilities_added.rego: |
    package appshield.kubernetes.KSV022

    import data.lib.kubernetes

    default failAdditionalCaps = false

    __rego_metadata__ := {
    	"id": "KSV022",
    	"avd_id": "AVD-KSV-0022",
    	"title": "Non-default capabilities added",
    	"short_code": "no-non-default-capabilities",
    	"version": "v1.0.0",
    	"severity": "MEDIUM",
    	"type": "Kubernetes Security Check",
    	"description": "Adding NET_RAW or capabilities beyond the default set must be disallowed.",
    	"recommended_actions": "Do not set spec.containers[*].securityContext.capabilities.add and spec.initContainers[*].securityContext.capabilities.add",
    	"url": "https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline",
    }

    __rego_input__ := {
    	"combine": false,
    	"selector": [{"type": "kubernetes"}],
    }

    # Add allowed capabilities to this set
    allowed_caps = set()

    # getContainersWithDisallowedCaps returns a list of containers which have
    # additional capabilities not included in the allowed capabilities list
    getContainersWithDisallowedCaps[container] {
    	allContainers := kubernetes.containers[_]
    	set_caps := {cap | cap := allContainers.securityContext.capabilities.add[_]}
    	caps_not_allowed := set_caps - allowed_caps
    	count(caps_not_allowed) > 0
    	container := allContainers.name
    }

    # cap_msg is a string of allowed capabilities to be print as part of deny message
    caps_msg = "" {
    	count(allowed_caps) == 0
    } else = msg {
    	msg := sprintf(" or set it to the following allowed values: %s", [concat(", ", allowed_caps)])
    }

    # failAdditionalCaps is true if there are containers which set additional capabilities
    # not included in the allowed capabilities list
    failAdditionalCaps {
    	count(getContainersWithDisallowedCaps) > 0
    }

    deny[res] {
    	failAdditionalCaps

    	msg := sprintf("Container '%s' of %s '%s' should not set 'securityContext.capabilities.add'%s", [getContainersWithDisallowedCaps[_], kubernetes.kind, kubernetes.name, caps_msg])
    	res := {
    		"msg": msg,
    		"id": __rego_metadata__.id,
    		"title": __rego_metadata__.title,
    		"severity": __rego_metadata__.severity,
    		"type": __rego_metadata__.type,
    	}
    }
  policy.4_hostpath_volumes_mounted.kinds: Workload
  policy.4_hostpath_volumes_mounted.rego: |
    package appshield.kubernetes.KSV023

    import data.lib.kubernetes
    import data.lib.utils

    default failHostPathVolume = false

    __rego_metadata__ := {
    	"id": "KSV023",
    	"avd_id": "AVD-KSV-0023",
    	"title": "hostPath volumes mounted",
    	"short_code": "no-mounted-hostpath",
    	"version": "v1.0.0",
    	"severity": "MEDIUM",
    	"type": "Kubernetes Security Check",
    	"description": "HostPath volumes must be forbidden.",
    	"recommended_actions": "Do not set 'spec.volumes[*].hostPath'.",
    	"url": "https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline",
    }

    __rego_input__ := {
    	"combine": false,
    	"selector": [{"type": "kubernetes"}],
    }

    failHostPathVolume {
    	volumes := kubernetes.volumes
    	utils.has_key(volumes[_], "hostPath")
    }

    deny[res] {
    	failHostPathVolume

    	msg := kubernetes.format(sprintf("%s '%s' should not set 'spec.template.volumes.hostPath'", [kubernetes.kind, kubernetes.name]))

    	res := {
    		"msg": msg,
    		"id": __rego_metadata__.id,
    		"title": __rego_metadata__.title,
    		"severity": __rego_metadata__.severity,
    		"type": __rego_metadata__.type,
    	}
    }
  policy.4_runs_with_a_root_gid.kinds: Workload
  policy.4_runs_with_a_root_gid.rego: |
    package appshield.kubernetes.KSV029

    import data.lib.kubernetes
    import data.lib.utils

    default failRootGroupId = false

    __rego_metadata__ := {
    	"id": "KSV029",
    	"avd_id": "AVD-KSV-0029",
    	"title": "A root primary or supplementary GID set",
    	"short_code": "no-run-root-gid",
    	"version": "v1.0.0",
    	"severity": "LOW",
    	"type": "Kubernetes Security Check",
    	"description": "Containers should be forbidden from running with a root primary or supplementary GID.",
    	"recommended_actions": "Set 'containers[].securityContext.runAsGroup' to a non-zero integer or leave undefined.",
    	"url": "https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted",
    }

    __rego_input__ := {
    	"combine": false,
    	"selector": [{"type": "kubernetes"}],
    }

    # getContainersWithRootGroupId returns a list of containers
    # with root group id set
    getContainersWithRootGroupId[name] {
    	container := kubernetes.containers[_]
    	container.securityContext.runAsGroup == 0
    	name := container.name
    }

    # failRootGroupId is true if root group id is set on pod
    failRootGroupId {
    	pod := kubernetes.pods[_]
    	pod.spec.securityContext.runAsGroup == 0
    }

    # failRootGroupId is true if root group id is set on pod
    failRootGroupId {
    	pod := kubernetes.pods[_]
    	utils.has_key(pod.spec.securityContext, "supplementalGroups")
    }

    # failRootGroupId is true if root group id is set on pod
    failRootGroupId {
    	pod := kubernetes.pods[_]
    	utils.has_key(pod.spec.securityContext, "fsGroup")
    }

    deny[res] {
    	failRootGroupId

    	msg := kubernetes.format(sprintf("%s '%s' should set 'spec.securityContext.runAsGroup', 'spec.securityContext.supplementalGroups[*]' and 'spec.securityContext.fsGroup' to integer greater than 0", [kubernetes.kind, kubernetes.name]))

    	res := {
    		"msg": msg,
    		"id": __rego_metadata__.id,
    		"title": __rego_metadata__.title,
    		"severity": __rego_metadata__.severity,
    		"type": __rego_metadata__.type,
    	}
    }

    deny[res] {
    	count(getContainersWithRootGroupId) > 0

    	msg := kubernetes.format(sprintf("Container '%s' of %s '%s' should set 'spec.securityContext.runAsGroup' to integer greater than  0", [getContainersWithRootGroupId[_], kubernetes.kind, kubernetes.name]))

    	res := {
    		"msg": msg,
    		"id": __rego_metadata__.id,
    		"title": __rego_metadata__.title,
    		"severity": __rego_metadata__.severity,
    		"type": __rego_metadata__.type,
    	}
    }
  policy.5_access_to_host_ports.kinds: Workload
  policy.5_access_to_host_ports.rego: |
    package appshield.kubernetes.KSV024

    import data.lib.kubernetes

    default failHostPorts = false

    __rego_metadata__ := {
    	"id": "KSV024",
    	"avd_id": "AVD-KSV-0024",
    	"title": "Access to host ports",
    	"short_code": "no-host-port-access",
    	"version": "v1.0.0",
    	"severity": "HIGH",
    	"type": "Kubernetes Security Check",
    	"description": "HostPorts should be disallowed, or at minimum restricted to a known list.",
    	"recommended_actions": "Do not set spec.containers[*].ports[*].hostPort and spec.initContainers[*].ports[*].hostPort.",
    	"url": "https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline",
    }

    __rego_input__ := {
    	"combine": false,
    	"selector": [{"type": "kubernetes"}],
    }

    # Add allowed host ports to this set
    allowed_host_ports = set()

    # getContainersWithDisallowedHostPorts returns a list of containers which have
    # host ports not included in the allowed host port list
    getContainersWithDisallowedHostPorts[container] {
    	allContainers := kubernetes.containers[_]
    	set_host_ports := {port | port := allContainers.ports[_].hostPort}
    	host_ports_not_allowed := set_host_ports - allowed_host_ports
    	count(host_ports_not_allowed) > 0
    	container := allContainers.name
    }

    # host_ports_msg is a string of allowed host ports to be print as part of deny message
    host_ports_msg = "" {
    	count(allowed_host_ports) == 0
    } else = msg {
    	msg := sprintf(" or set it to the following allowed values: %s", [concat(", ", allowed_host_ports)])
    }

    # failHostPorts is true if there are containers which set host ports
    # not included in the allowed host ports list
    failHostPorts {
    	count(getContainersWithDisallowedHostPorts) > 0
    }

    deny[res] {
    	failHostPorts

    	msg := sprintf("Container '%s' of %s '%s' should not set host ports, 'ports[*].hostPort'%s", [getContainersWithDisallowedHostPorts[_], kubernetes.kind, kubernetes.name, host_ports_msg])
    	res := {
    		"msg": msg,
    		"id": __rego_metadata__.id,
    		"title": __rego_metadata__.title,
    		"severity": __rego_metadata__.severity,
    		"type": __rego_metadata__.type,
    	}
    }
  policy.5_runtime_default_seccomp_profile_not_set.kinds: Workload
  policy.5_runtime_default_seccomp_profile_not_set.rego: |
    package appshield.kubernetes.KSV030

    import data.lib.kubernetes
    import data.lib.utils

    default failSeccompProfileType = false

    __rego_metadata__ := {
    	"id": "KSV030",
    	"avd_id": "AVD-KSV-0030",
    	"title": "Default Seccomp profile not set",
    	"short_code": "use-default-seccomp",
    	"version": "v1.0.0",
    	"severity": "LOW",
    	"type": "Kubernetes Security Check",
    	"description": "The RuntimeDefault seccomp profile must be required, or allow specific additional profiles.",
    	"recommended_actions": "Set 'spec.securityContext.seccompProfile.type', 'spec.containers[*].securityContext.seccompProfile' and 'spec.initContainers[*].securityContext.seccompProfile' to 'RuntimeDefault' or undefined.",
    	"url": "https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted",
    }

    __rego_input__ := {
    	"combine": false,
    	"selector": [{"type": "kubernetes"}],
    }

    # containers
    getContainersWithDisallowedSeccompProfileType[name] {
    	container := kubernetes.containers[_]
    	type := container.securityContext.seccompProfile.type
    	not type == "RuntimeDefault"
    	name := container.name
    }

    # pods
    failSeccompProfileType {
    	pod := kubernetes.pods[_]
    	type := pod.spec.securityContext.seccompProfile.type
    	not type == "RuntimeDefault"
    }

    # annotations (Kubernetes pre-v1.19)
    failSeccompAnnotation {
    	annotations := kubernetes.annotations[_]
    	val := annotations["seccomp.security.alpha.kubernetes.io/pod"]
    	val != "runtime/default"
    }

    # annotations
    deny[res] {
    	failSeccompAnnotation

    	msg := kubernetes.format(sprintf("%s '%s' should set 'seccomp.security.alpha.kubernetes.io/pod' to 'runtime/default'", [kubernetes.kind, kubernetes.name]))

    	res := {
    		"msg": msg,
    		"id": __rego_metadata__.id,
    		"title": __rego_metadata__.title,
    		"severity": __rego_metadata__.severity,
    		"type": __rego_metadata__.type,
    	}
    }

    # pods
    deny[res] {
    	failSeccompProfileType

    	msg := kubernetes.format(sprintf("%s '%s' should set 'spec.securityContext.seccompProfile.type' to 'RuntimeDefault'", [kubernetes.kind, kubernetes.name]))

    	res := {
    		"msg": msg,
    		"id": __rego_metadata__.id,
    		"title": __rego_metadata__.title,
    		"severity": __rego_metadata__.severity,
    		"type": __rego_metadata__.type,
    	}
    }

    # containers
    deny[res] {
    	count(getContainersWithDisallowedSeccompProfileType) > 0

    	msg := kubernetes.format(sprintf("Container '%s' of %s '%s' should set 'spec.containers[*].securityContext.seccompProfile.type' to 'RuntimeDefault'", [getContainersWithDisallowedSeccompProfileType[_], kubernetes.kind, kubernetes.name]))

    	res := {
    		"msg": msg,
    		"id": __rego_metadata__.id,
    		"title": __rego_metadata__.title,
    		"severity": __rego_metadata__.severity,
    		"type": __rego_metadata__.type,
    	}
    }
  policy.6_apparmor_policy_disabled.kinds: Workload
  policy.6_apparmor_policy_disabled.rego: |
    package appshield.kubernetes.KSV002

    import data.lib.kubernetes

    default failAppArmor = false

    __rego_metadata__ := {
    	"id": "KSV002",
    	"avd_id": "AVD-KSV-0002",
    	"title": "Default AppArmor profile not set",
    	"short_code": "use-default-apparmor-profile",
    	"version": "v1.0.0",
    	"severity": "MEDIUM",
    	"type": "Kubernetes Security Check",
    	"description": "A program inside the container can bypass AppArmor protection policies.",
    	"recommended_actions": "Remove 'container.apparmor.security.beta.kubernetes.io' annotation or set it to 'runtime/default'.",
    	"url": "https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline",
    }

    __rego_input__ := {
    	"combine": false,
    	"selector": [{"type": "kubernetes"}],
    }

    apparmor_keys[container] = key {
    	container := kubernetes.containers[_].name
    	key := sprintf("%s/%s", ["container.apparmor.security.beta.kubernetes.io", container])
    }

    custom_apparmor_containers[container] {
    	key := apparmor_keys[container]
    	annotations := kubernetes.annotations[_]
    	val := annotations[key]
    	val != "runtime/default"
    }

    deny[res] {
    	container := custom_apparmor_containers[_]

    	msg := kubernetes.format(sprintf("Container '%s' of %s '%s' should specify an AppArmor profile", [container, kubernetes.kind, kubernetes.name]))

    	res := {
    		"msg": msg,
    		"id": __rego_metadata__.id,
    		"title": __rego_metadata__.title,
    		"severity": __rego_metadata__.severity,
    		"type": __rego_metadata__.type,
    	}
    }
  policy.7_selinux_custom_options_set.kinds: Workload
  policy.7_selinux_custom_options_set.rego: |
    package appshield.kubernetes.KSV025

    import data.lib.kubernetes
    import data.lib.utils

    default failSELinux = false

    __rego_metadata__ := {
    	"id": "KSV025",
    	"avd_id": "AVD-KSV-0025",
    	"title": "SELinux custom options set",
    	"short_code": "no-custom-selinux-options",
    	"version": "v1.0.0",
    	"severity": "MEDIUM",
    	"type": "Kubernetes Security Check",
    	"description": "Setting a custom SELinux user or role option should be forbidden.",
    	"recommended_actions": "Do not set 'spec.securityContext.seLinuxOptions', spec.containers[*].securityContext.seLinuxOptions and spec.initContainers[*].securityContext.seLinuxOptions.",
    	"url": "https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline",
    }

    __rego_input__ := {
    	"combine": false,
    	"selector": [{"type": "kubernetes"}],
    }

    allowed_selinux_types := ["container_t", "container_init_t", "container_kvm_t"]

    getAllSecurityContexts[context] {
    	context := kubernetes.containers[_].securityContext
    }

    getAllSecurityContexts[context] {
    	context := kubernetes.pods[_].spec.securityContext
    }

    failSELinuxType[type] {
    	context := getAllSecurityContexts[_]

    	trace(context.seLinuxOptions.type)
    	context.seLinuxOptions != null
    	context.seLinuxOptions.type != null

    	not hasAllowedType(context.seLinuxOptions)

    	type := context.seLinuxOptions.type
    }

    failForbiddenSELinuxProperties[key] {
    	context := getAllSecurityContexts[_]

    	context.seLinuxOptions != null

    	forbiddenProps := getForbiddenSELinuxProperties(context)
    	key := forbiddenProps[_]
    }

    getForbiddenSELinuxProperties(context) = keys {
    	forbiddenProperties = ["role", "user"]
    	keys := {msg |
    		key := forbiddenProperties[_]
    		utils.has_key(context.seLinuxOptions, key)
    		msg := sprintf("'%s'", [key])
    	}
    }

    hasAllowedType(options) {
    	allowed_selinux_types[_] == options.type
    }

    deny[res] {
    	type := failSELinuxType[_]

    	msg := kubernetes.format(sprintf("%s '%s' uses invalid seLinux type '%s'", [kubernetes.kind, kubernetes.name, type]))

    	res := {
    		"msg": msg,
    		"id": __rego_metadata__.id,
    		"title": __rego_metadata__.title,
    		"severity": __rego_metadata__.severity,
    		"type": __rego_metadata__.type,
    	}
    }

    deny[res] {
    	keys := failForbiddenSELinuxProperties

    	count(keys) > 0

    	msg := kubernetes.format(sprintf("%s '%s' uses restricted properties in seLinuxOptions: (%s)", [kubernetes.kind, kubernetes.name, concat(", ", keys)]))

    	res := {
    		"msg": msg,
    		"id": __rego_metadata__.id,
    		"title": __rego_metadata__.title,
    		"severity": __rego_metadata__.severity,
    		"type": __rego_metadata__.type,
    	}
    }
  policy.8_non_default_proc_masks_set.kinds: Workload
  policy.8_non_default_proc_masks_set.rego: |
    package appshield.kubernetes.KSV027

    import data.lib.kubernetes
    import data.lib.utils

    default failProcMount = false

    __rego_metadata__ := {
    	"id": "KSV027",
    	"avd_id": "AVD-KSV-0027",
    	"title": "Non-default /proc masks set",
    	"short_code": "no-custom-proc-mask",
    	"version": "v1.0.0",
    	"severity": "MEDIUM",
    	"type": "Kubernetes Security Check",
    	"description": "The default /proc masks are set up to reduce attack surface, and should be required.",
    	"recommended_actions": "Do not set spec.containers[*].securityContext.procMount and spec.initContainers[*].securityContext.procMount.",
    	"url": "https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline",
    }

    __rego_input__ := {
    	"combine": false,
    	"selector": [{"type": "kubernetes"}],
    }

    # failProcMountOpts is true if securityContext.procMount is set in any container
    failProcMountOpts {
    	allContainers := kubernetes.containers[_]
    	utils.has_key(allContainers.securityContext, "procMount")
    }

    deny[res] {
    	failProcMountOpts

    	msg := kubernetes.format(sprintf("%s '%s' should not set 'spec.containers[*].securityContext.procMount' or 'spec.initContainers[*].securityContext.procMount'", [kubernetes.kind, kubernetes.name]))

    	res := {
    		"msg": msg,
    		"id": __rego_metadata__.id,
    		"title": __rego_metadata__.title,
    		"severity": __rego_metadata__.severity,
    		"type": __rego_metadata__.type,
    	}
    }
  policy.9_unsafe_sysctl_options_set.kinds: Workload
  policy.9_unsafe_sysctl_options_set.rego: |
    package appshield.kubernetes.KSV026

    import data.lib.kubernetes
    import data.lib.utils

    default failSysctls = false

    __rego_metadata__ := {
    	"id": "KSV026",
    	"avd_id": "AVD-KSV-0026",
    	"title": "Unsafe sysctl options set",
    	"short_code": "no-unsafe-sysctl",
    	"version": "v1.0.0",
    	"severity": "MEDIUM",
    	"type": "Kubernetes Security Check",
    	"description": "Sysctls can disable security mechanisms or affect all containers on a host, and should be disallowed except for an allowed 'safe' subset. A sysctl is considered safe if it is namespaced in the container or the Pod, and it is isolated from other Pods or processes on the same Node.",
    	"recommended_actions": "Do not set 'spec.securityContext.sysctls' or set to values in an allowed subset",
    	"url": "https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline",
    }

    __rego_input__ := {
    	"combine": false,
    	"selector": [{"type": "kubernetes"}],
    }

    # Add allowed sysctls
    allowed_sysctls = {
    	"kernel.shm_rmid_forced",
    	"net.ipv4.ip_local_port_range",
    	"net.ipv4.tcp_syncookies",
    	"net.ipv4.ping_group_range",
    }

    # failSysctls is true if a disallowed sysctl is set
    failSysctls {
    	pod := kubernetes.pods[_]
    	set_sysctls := {sysctl | sysctl := pod.spec.securityContext.sysctls[_].name}
    	sysctls_not_allowed := set_sysctls - allowed_sysctls
    	count(sysctls_not_allowed) > 0
    }

    deny[res] {
    	failSysctls

    	msg := kubernetes.format(sprintf("%s '%s' should set 'securityContext.sysctl' to the allowed values", [kubernetes.kind, kubernetes.name]))

    	res := {
    		"msg": msg,
    		"id": __rego_metadata__.id,
    		"title": __rego_metadata__.title,
    		"severity": __rego_metadata__.severity,
    		"type": __rego_metadata__.type,
    	}
    }
  policy.CPU_not_limited.kinds: Workload
  policy.CPU_not_limited.rego: |
    package appshield.kubernetes.KSV011

    import data.lib.kubernetes
    import data.lib.utils

    default failLimitsCPU = false

    __rego_metadata__ := {
    	"id": "KSV011",
    	"avd_id": "AVD-KSV-0011",
    	"title": "CPU not limited",
    	"short_code": "limit-cpu",
    	"version": "v1.0.0",
    	"severity": "LOW",
    	"type": "Kubernetes Security Check",
    	"description": "Enforcing CPU limits prevents DoS via resource exhaustion.",
    	"recommended_actions": "Set a limit value under 'containers[].resources.limits.cpu'.",
    	"url": "https://cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-resource-requests-and-limits",
    }

    __rego_input__ := {
    	"combine": false,
    	"selector": [{"type": "kubernetes"}],
    }

    # getLimitsCPUContainers returns all containers which have set resources.limits.cpu
    getLimitsCPUContainers[container] {
    	allContainers := kubernetes.containers[_]
    	utils.has_key(allContainers.resources.limits, "cpu")
    	container := allContainers.name
    }

    # getNoLimitsCPUContainers returns all containers which have not set
    # resources.limits.cpu
    getNoLimitsCPUContainers[container] {
    	container := kubernetes.containers[_].name
    	not getLimitsCPUContainers[container]
    }

    # failLimitsCPU is true if containers[].resources.limits.cpu is not set
    # for ANY container
    failLimitsCPU {
    	count(getNoLimitsCPUContainers) > 0
    }

    deny[res] {
    	failLimitsCPU

    	msg := kubernetes.format(sprintf("Container '%s' of %s '%s' should set 'resources.limits.cpu'", [getNoLimitsCPUContainers[_], kubernetes.kind, kubernetes.name]))

    	res := {
    		"msg": msg,
    		"id": __rego_metadata__.id,
    		"title": __rego_metadata__.title,
    		"severity": __rego_metadata__.severity,
    		"type": __rego_metadata__.type,
    	}
    }
  policy.CPU_requests_not_specified.kinds: Workload
  policy.CPU_requests_not_specified.rego: |
    package appshield.kubernetes.KSV015

    import data.lib.kubernetes
    import data.lib.utils

    default failRequestsCPU = false

    __rego_metadata__ := {
    	"id": "KSV015",
    	"avd_id": "AVD-KSV-0015",
    	"title": "CPU requests not specified",
    	"short_code": "no-unspecified-cpu-requests",
    	"version": "v1.0.0",
    	"severity": "LOW",
    	"type": "Kubernetes Security Check",
    	"description": "When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.",
    	"recommended_actions": "Set 'containers[].resources.requests.cpu'.",
    	"url": "https://cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-resource-requests-and-limits",
    }

    __rego_input__ := {
    	"combine": false,
    	"selector": [{"type": "kubernetes"}],
    }

    # getRequestsCPUContainers returns all containers which have set resources.requests.cpu
    getRequestsCPUContainers[container] {
    	allContainers := kubernetes.containers[_]
    	utils.has_key(allContainers.resources.requests, "cpu")
    	container := allContainers.name
    }

    # getNoRequestsCPUContainers returns all containers which have not set
    # resources.requests.cpu
    getNoRequestsCPUContainers[container] {
    	container := kubernetes.containers[_].name
    	not getRequestsCPUContainers[container]
    }

    # failRequestsCPU is true if containers[].resources.requests.cpu is not set
    # for ANY container
    failRequestsCPU {
    	count(getNoRequestsCPUContainers) > 0
    }

    deny[res] {
    	failRequestsCPU

    	msg := kubernetes.format(sprintf("Container '%s' of %s '%s' should set 'resources.requests.cpu'", [getNoRequestsCPUContainers[_], kubernetes.kind, kubernetes.name]))

    	res := {
    		"msg": msg,
    		"id": __rego_metadata__.id,
    		"title": __rego_metadata__.title,
    		"severity": __rego_metadata__.severity,
    		"type": __rego_metadata__.type,
    	}
    }
  policy.SYS_ADMIN_capability.kinds: Workload
  policy.SYS_ADMIN_capability.rego: |
    package appshield.kubernetes.KSV005

    import data.lib.kubernetes

    default failCapsSysAdmin = false

    __rego_metadata__ := {
    	"id": "KSV005",
    	"avd_id": "AVD-KSV-0005",
    	"title": "SYS_ADMIN capability added",
    	"short_code": "no-sysadmin-capability",
    	"version": "v1.0.0",
    	"severity": "HIGH",
    	"type": "Kubernetes Security Check",
    	"description": "SYS_ADMIN gives the processes running inside the container privileges that are equivalent to root.",
    	"recommended_actions": "Remove the SYS_ADMIN capability from 'containers[].securityContext.capabilities.add'.",
    	"url": "https://kubesec.io/basics/containers-securitycontext-capabilities-add-index-sys-admin/",
    }

    __rego_input__ := {
    	"combine": false,
    	"selector": [{"type": "kubernetes"}],
    }

    # getCapsSysAdmin returns the names of all containers which include
    # 'SYS_ADMIN' in securityContext.capabilities.add.
    getCapsSysAdmin[container] {
    	allContainers := kubernetes.containers[_]
    	allContainers.securityContext.capabilities.add[_] == "SYS_ADMIN"
    	container := allContainers.name
    }

    # failCapsSysAdmin is true if securityContext.capabilities.add
    # includes 'SYS_ADMIN'.
    failCapsSysAdmin {
    	count(getCapsSysAdmin) > 0
    }

    deny[res] {
    	failCapsSysAdmin

    	msg := kubernetes.format(sprintf("Container '%s' of %s '%s' should not include 'SYS_ADMIN' in 'securityContext.capabilities.add'", [getCapsSysAdmin[_], kubernetes.kind, kubernetes.name]))

    	res := {
    		"msg": msg,
    		"id": __rego_metadata__.id,
    		"title": __rego_metadata__.title,
    		"severity": __rego_metadata__.severity,
    		"type": __rego_metadata__.type,
    	}
    }
  policy.capabilities_no_drop_all.kinds: Workload
  policy.capabilities_no_drop_all.rego: |
    package appshield.kubernetes.KSV003

    import data.lib.kubernetes

    default checkCapsDropAll = false

    __rego_metadata__ := {
    	"id": "KSV003",
    	"avd_id": "AVD-KSV-0003",
    	"title": "Default capabilities not dropped",
    	"short_code": "drop-default-capabilities",
    	"version": "v1.0.0",
    	"severity": "LOW",
    	"type": "Kubernetes Security Check",
    	"description": "The container should drop all default capabilities and add only those that are needed for its execution.",
    	"recommended_actions": "Add 'ALL' to containers[].securityContext.capabilities.drop.",
    	"url": "https://kubesec.io/basics/containers-securitycontext-capabilities-drop-index-all/",
    }

    __rego_input__ := {
    	"combine": false,
    	"selector": [{"type": "kubernetes"}],
    }

    # Get all containers which include 'ALL' in security.capabilities.drop
    getCapsDropAllContainers[container] {
    	allContainers := kubernetes.containers[_]
    	allContainers.securityContext.capabilities.drop[_] == "ALL"
    	container := allContainers.name
    }

    # Get all containers which don't include 'ALL' in security.capabilities.drop
    getCapsNoDropAllContainers[container] {
    	container := kubernetes.containers[_].name
    	not getCapsDropAllContainers[container]
    }

    # checkCapsDropAll is true if capabilities drop does not include 'ALL',
    # or if capabilities drop is not specified at all.
    checkCapsDropAll {
    	count(getCapsNoDropAllContainers) > 0
    }

    deny[res] {
    	checkCapsDropAll

    	msg := kubernetes.format(sprintf("Container '%s' of %s '%s' should add 'ALL' to 'securityContext.capabilities.drop'", [getCapsNoDropAllContainers[_], kubernetes.kind, kubernetes.name]))

    	res := {
    		"msg": msg,
    		"id": __rego_metadata__.id,
    		"title": __rego_metadata__.title,
    		"severity": __rego_metadata__.severity,
    		"type": __rego_metadata__.type,
    	}
    }
  policy.capabilities_no_drop_at_least_one.kinds: Workload
  policy.capabilities_no_drop_at_least_one.rego: |
    package appshield.kubernetes.KSV004

    import data.lib.kubernetes
    import data.lib.utils

    default failCapsDropAny = false

    __rego_metadata__ := {
    	"id": "KSV004",
    	"avd_id": "AVD-KSV-0004",
    	"title": "Unused capabilities should be dropped (drop any)",
    	"short_code": "drop-unused-capabilities",
    	"version": "v0.0.0",
    	"severity": "LOW",
    	"type": "Kubernetes Security Check",
    	"description": "Security best practices require containers to run with minimal required capabilities.",
    	"recommended_actions": "Specify at least one unneeded capability in 'containers[].securityContext.capabilities.drop'",
    	"url": "https://kubesec.io/basics/containers-securitycontext-capabilities-drop-index-all/",
    }

    __rego_input__ := {
    	"combine": false,
    	"selector": [{"type": "kubernetes"}],
    }

    # getCapsDropAnyContainers returns names of all containers
    # which set securityContext.capabilities.drop
    getCapsDropAnyContainers[container] {
    	allContainers := kubernetes.containers[_]
    	utils.has_key(allContainers.securityContext.capabilities, "drop")
    	container := allContainers.name
    }

    # getNoCapsDropContainers returns names of all containers which
    # do not set securityContext.capabilities.drop
    getNoCapsDropContainers[container] {
    	container := kubernetes.containers[_].name
    	not getCapsDropAnyContainers[container]
    }

    # failCapsDropAny is true if ANY container does not
    # set securityContext.capabilities.drop
    failCapsDropAny {
    	count(getNoCapsDropContainers) > 0
    }

    deny[res] {
    	failCapsDropAny

    	msg := kubernetes.format(sprintf("Container '%s' of '%s' '%s' in '%s' namespace should set securityContext.capabilities.drop", [getNoCapsDropContainers[_], lower(kubernetes.kind), kubernetes.name, kubernetes.namespace]))

    	res := {
    		"msg": msg,
    		"id": __rego_metadata__.id,
    		"title": __rego_metadata__.title,
    		"severity": __rego_metadata__.severity,
    		"type": __rego_metadata__.type,
    	}
    }
  policy.file_system_not_read_only.kinds: Workload
  policy.file_system_not_read_only.rego: |
    package appshield.kubernetes.KSV014

    import data.lib.kubernetes

    default failReadOnlyRootFilesystem = false

    __rego_metadata__ := {
    	"id": "KSV014",
    	"avd_id": "AVD-KSV-0014",
    	"title": "Root file system is not read-only",
    	"short_code": "use-readonly-filesystem",
    	"version": "v1.0.0",
    	"severity": "LOW",
    	"type": "Kubernetes Security Check",
    	"description": "An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk.",
    	"recommended_actions": "Change 'containers[].securityContext.readOnlyRootFilesystem' to 'true'.",
    	"url": "https://kubesec.io/basics/containers-securitycontext-readonlyrootfilesystem-true/",
    }

    __rego_input__ := {
    	"combine": false,
    	"selector": [{"type": "kubernetes"}],
    }

    # getReadOnlyRootFilesystemContainers returns all containers that have
    # securityContext.readOnlyFilesystem set to true.
    getReadOnlyRootFilesystemContainers[container] {
    	allContainers := kubernetes.containers[_]
    	allContainers.securityContext.readOnlyRootFilesystem == true
    	container := allContainers.name
    }

    # getNotReadOnlyRootFilesystemContainers returns all containers that have
    # securityContext.readOnlyRootFilesystem set to false or not set at all.
    getNotReadOnlyRootFilesystemContainers[container] {
    	container := kubernetes.containers[_].name
    	not getReadOnlyRootFilesystemContainers[container]
    }

    # failReadOnlyRootFilesystem is true if ANY container sets
    # securityContext.readOnlyRootFilesystem set to false or not set at all.
    failReadOnlyRootFilesystem {
    	count(getNotReadOnlyRootFilesystemContainers) > 0
    }

    deny[res] {
    	failReadOnlyRootFilesystem

    	msg := kubernetes.format(sprintf("Container '%s' of %s '%s' should set 'securityContext.readOnlyRootFilesystem' to true", [getNotReadOnlyRootFilesystemContainers[_], kubernetes.kind, kubernetes.name]))

    	res := {
    		"msg": msg,
    		"id": __rego_metadata__.id,
    		"title": __rego_metadata__.title,
    		"severity": __rego_metadata__.severity,
    		"type": __rego_metadata__.type,
    	}
    }
  policy.manages_etc_hosts.kinds: Workload
  policy.manages_etc_hosts.rego: |
    package appshield.kubernetes.KSV007

    import data.lib.kubernetes
    import data.lib.utils

    default failHostAliases = false

    __rego_metadata__ := {
    	"id": "KSV007",
    	"avd_id": "AVD-KSV-0007",
    	"title": "hostAliases is set",
    	"short_code": "no-hostaliases",
    	"version": "v1.0.0",
    	"severity": "LOW",
    	"type": "Kubernetes Security Check",
    	"description": "Managing /etc/hosts aliases can prevent the container engine from modifying the file after a pod’s containers have already been started.",
    	"recommended_actions": "Do not set 'spec.template.spec.hostAliases'.",
    }

    __rego_input__ := {
    	"combine": false,
    	"selector": [{"type": "kubernetes"}],
    }

    # failHostAliases is true if spec.hostAliases is set (on all controllers)
    failHostAliases {
    	utils.has_key(kubernetes.host_aliases[_], "hostAliases")
    }

    deny[res] {
    	failHostAliases

    	msg := kubernetes.format(sprintf("'%s' '%s' in '%s' namespace should not set spec.template.spec.hostAliases", [lower(kubernetes.kind), kubernetes.name, kubernetes.namespace]))

    	res := {
    		"msg": msg,
    		"id": __rego_metadata__.id,
    		"title": __rego_metadata__.title,
    		"severity": __rego_metadata__.severity,
    		"type": __rego_metadata__.type,
    	}
    }
  policy.memory_not_limited.kinds: Workload
  policy.memory_not_limited.rego: |
    package appshield.kubernetes.KSV018

    import data.lib.kubernetes
    import data.lib.utils

    default failLimitsMemory = false

    __rego_metadata__ := {
    	"id": "KSV018",
    	"avd_id": "AVD-KSV-0018",
    	"title": "Memory not limited",
    	"short_code": "limit-memory",
    	"version": "v1.0.0",
    	"severity": "LOW",
    	"type": "Kubernetes Security Check",
    	"description": "Enforcing memory limits prevents DoS via resource exhaustion.",
    	"recommended_actions": "Set a limit value under 'containers[].resources.limits.memory'.",
    	"url": "https://kubesec.io/basics/containers-resources-limits-memory/",
    }

    __rego_input__ := {
    	"combine": false,
    	"selector": [{"type": "kubernetes"}],
    }

    # getLimitsMemoryContainers returns all containers which have set resources.limits.memory
    getLimitsMemoryContainers[container] {
    	allContainers := kubernetes.containers[_]
    	utils.has_key(allContainers.resources.limits, "memory")
    	container := allContainers.name
    }

    # getNoLimitsMemoryContainers returns all containers which have not set
    # resources.limits.memory
    getNoLimitsMemoryContainers[container] {
    	container := kubernetes.containers[_].name
    	not getLimitsMemoryContainers[container]
    }

    # failLimitsMemory is true if containers[].resources.limits.memory is not set
    # for ANY container
    failLimitsMemory {
    	count(getNoLimitsMemoryContainers) > 0
    }

    deny[res] {
    	failLimitsMemory

    	msg := kubernetes.format(sprintf("Container '%s' of %s '%s' should set 'resources.limits.memory'", [getNoLimitsMemoryContainers[_], kubernetes.kind, kubernetes.name]))
    	res := {
    		"msg": msg,
    		"id": __rego_metadata__.id,
    		"title": __rego_metadata__.title,
    		"severity": __rego_metadata__.severity,
    		"type": __rego_metadata__.type,
    	}
    }
  policy.memory_requests_not_specified.kinds: Workload
  policy.memory_requests_not_specified.rego: |
    package appshield.kubernetes.KSV016

    import data.lib.kubernetes
    import data.lib.utils

    default failRequestsMemory = false

    __rego_metadata__ := {
    	"id": "KSV016",
    	"avd_id": "AVD-KSV-0016",
    	"title": "Memory requests not specified",
    	"short_code": "no-unspecified-memory-requests",
    	"version": "v1.0.0",
    	"severity": "LOW",
    	"type": "Kubernetes Security Check",
    	"description": "When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention.",
    	"recommended_actions": "Set 'containers[].resources.requests.memory'.",
    	"url": "https://kubesec.io/basics/containers-resources-limits-memory/",
    }

    __rego_input__ := {
    	"combine": false,
    	"selector": [{"type": "kubernetes"}],
    }

    # getRequestsMemoryContainers returns all containers which have set resources.requests.memory
    getRequestsMemoryContainers[container] {
    	allContainers := kubernetes.containers[_]
    	utils.has_key(allContainers.resources.requests, "memory")
    	container := allContainers.name
    }

    # getNoRequestsMemoryContainers returns all containers which have not set
    # resources.requests.memory
    getNoRequestsMemoryContainers[container] {
    	container := kubernetes.containers[_].name
    	not getRequestsMemoryContainers[container]
    }

    # failRequestsMemory is true if containers[].resources.requests.memory is not set
    # for ANY container
    failRequestsMemory {
    	count(getNoRequestsMemoryContainers) > 0
    }

    deny[res] {
    	failRequestsMemory

    	msg := kubernetes.format(sprintf("Container '%s' of %s '%s' should set 'resources.requests.memory'", [getNoRequestsMemoryContainers[_], kubernetes.kind, kubernetes.name]))

    	res := {
    		"msg": msg,
    		"id": __rego_metadata__.id,
    		"title": __rego_metadata__.title,
    		"severity": __rego_metadata__.severity,
    		"type": __rego_metadata__.type,
    	}
    }
  policy.mounts_docker_socket.kinds: Workload
  policy.mounts_docker_socket.rego: |
    package appshield.kubernetes.KSV006

    import data.lib.kubernetes

    name = input.metadata.name

    default checkDockerSocket = false

    __rego_metadata__ := {
    	"id": "KSV006",
    	"avd_id": "AVD-KSV-0006",
    	"title": "hostPath volume mounted with docker.sock",
    	"short_code": "no-docker-sock-mount",
    	"version": "v1.0.0",
    	"severity": "HIGH",
    	"type": "Kubernetes Security Check",
    	"description": "Mounting docker.sock from the host can give the container full root access to the host.",
    	"recommended_actions": "Do not specify /var/run/docker.socket in 'spec.template.volumes.hostPath.path'.",
    	"url": "https://kubesec.io/basics/spec-volumes-hostpath-path-var-run-docker-sock/",
    }

    __rego_input__ := {
    	"combine": false,
    	"selector": [{"type": "kubernetes"}],
    }

    # checkDockerSocket is true if volumes.hostPath.path is set to /var/run/docker.sock
    # and is false if volumes.hostPath is set to some other path or not set.
    checkDockerSocket {
    	volumes := kubernetes.volumes
    	volumes[_].hostPath.path == "/var/run/docker.sock"
    }

    deny[res] {
    	checkDockerSocket

    	msg := kubernetes.format(sprintf("%s '%s' should not specify '/var/run/docker.socker' in 'spec.template.volumes.hostPath.path'", [kubernetes.kind, kubernetes.name]))

    	res := {
    		"msg": msg,
    		"id": __rego_metadata__.id,
    		"title": __rego_metadata__.title,
    		"severity": __rego_metadata__.severity,
    		"type": __rego_metadata__.type,
    	}
    }
  policy.protect_core_components_namespace.kinds: Workload
  policy.protect_core_components_namespace.rego: |
    package appshield.kubernetes.KSV037

    import data.lib.kubernetes
    import data.lib.utils

    __rego_metadata__ := {
    	"id": "KSV037",
    	"avd_id": "AVD-KSV-0037",
    	"title": "User Pods should not be placed in kube-system namespace",
    	"short_code": "no-user-pods-in-system-namespace",
    	"version": "v1.0.0",
    	"severity": "MEDIUM",
    	"type": "Kubernetes Security Check",
    	"description": "ensure that User pods are not placed in kube-system namespace",
    	"recommended_actions": "Deploy the use pods into a designated namespace which is not kube-system.",
    	"url": "https://kubernetes.io/docs/reference/setup-tools/kubeadm/implementation-details/",
    }

    __rego_input__ := {
    	"combine": false,
    	"selector": [{"type": "kubernetes"}],
    }

    deny[res] {
    	systemNamespaceInUse(input.metadata, input.spec)
    	msg := sprintf("%s '%s' should not be set with 'kube-system' namespace", [kubernetes.kind, kubernetes.name])
    	res := {
    		"msg": msg,
    		"id": __rego_metadata__.id,
    		"title": __rego_metadata__.title,
    		"severity": __rego_metadata__.severity,
    		"type": __rego_metadata__.type,
    	}
    }

    systemNamespaceInUse(metadata, spec) {
    	kubernetes.namespace == "kube-system"
    	not core_component(metadata, spec)
    }

    core_component(metadata, spec) {
    	kubernetes.has_field(metadata.labels, "tier")
    	metadata.labels.tier == "control-plane"
    	kubernetes.has_field(spec, "priorityClassName")
    	spec.priorityClassName == "system-node-critical"
    	kubernetes.has_field(metadata.labels, "component")
    	coreComponentLabels := ["kube-apiserver", "etcd", "kube-controller-manager", "kube-scheduler"]
    	metadata.labels.component = coreComponentLabels[_]
    }
  policy.protecting_pod_service_account_tokens.kinds: Workload
  policy.protecting_pod_service_account_tokens.rego: |
    package appshield.kubernetes.KSV036

    import data.lib.kubernetes
    import data.lib.utils

    __rego_metadata__ := {
    	"id": "KSV036",
    	"avd_id": "AVD-KSV-0036",
    	"title": "Protecting Pod service account tokens",
    	"short_code": "no-auto-mount-service-token",
    	"version": "v1.0.0",
    	"severity": "MEDIUM",
    	"type": "Kubernetes Security Check",
    	"description": "ensure that Pod specifications disable the secret token being mounted by setting automountServiceAccountToken: false",
    	"recommended_actions": "Remove 'container.apparmor.security.beta.kubernetes.io' annotation or set it to 'runtime/default'.",
    	"url": "https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/#serviceaccount-admission-controller",
    }

    __rego_input__ := {
    	"combine": false,
    	"selector": [{"type": "kubernetes"}],
    }

    deny[res] {
    	mountServiceAccountToken(input.spec)
    	msg := kubernetes.format(sprintf("Container of %s '%s' should set 'spec.automountServiceAccountToken' to false", [kubernetes.kind, kubernetes.name]))

    	res := {
    		"msg": msg,
    		"id": __rego_metadata__.id,
    		"title": __rego_metadata__.title,
    		"severity": __rego_metadata__.severity,
    		"type": __rego_metadata__.type,
    	}
    }

    mountServiceAccountToken(spec) {
    	has_key(spec, "automountServiceAccountToken")
    	spec.automountServiceAccountToken == true
    }

    # if there is no automountServiceAccountToken spec, check on volumeMount in containers. Service Account token is mounted on /var/run/secrets/kubernetes.io/serviceaccount
    mountServiceAccountToken(spec) {
    	not has_key(spec, "automountServiceAccountToken")
    	"/var/run/secrets/kubernetes.io/serviceaccount" == kubernetes.containers[_].volumeMounts[_].mountPath
    }

    has_key(x, k) {
    	_ = x[k]
    }
  policy.runs_with_GID_le_10000.kinds: Workload
  policy.runs_with_GID_le_10000.rego: |
    package appshield.kubernetes.KSV021

    import data.lib.kubernetes
    import data.lib.utils

    default failRunAsGroup = false

    __rego_metadata__ := {
    	"id": "KSV021",
    	"avd_id": "AVD-KSV-0021",
    	"title": "Runs with low group ID",
    	"short_code": "use-high-gid",
    	"version": "v1.0.0",
    	"severity": "MEDIUM",
    	"type": "Kubernetes Security Check",
    	"description": "Force the container to run with group ID > 10000 to avoid conflicts with the host’s user table.",
    	"recommended_actions": "Set 'containers[].securityContext.runAsGroup' to an integer > 10000.",
    	"url": "https://kubesec.io/basics/containers-securitycontext-runasuser/",
    }

    __rego_input__ := {
    	"combine": false,
    	"selector": [{"type": "kubernetes"}],
    }

    # getGroupIdContainers returns the names of all containers which have
    # securityContext.runAsGroup less than or equal to 10000.
    getGroupIdContainers[container] {
    	allContainers := kubernetes.containers[_]
    	allContainers.securityContext.runAsGroup <= 10000
    	container := allContainers.name
    }

    # getGroupIdContainers returns the names of all containers which do
    # not have securityContext.runAsGroup set.
    getGroupIdContainers[container] {
    	allContainers := kubernetes.containers[_]
    	not utils.has_key(allContainers.securityContext, "runAsGroup")
    	container := allContainers.name
    }

    # getGroupIdContainers returns the names of all containers which do
    # not have securityContext set.
    getGroupIdContainers[container] {
    	allContainers := kubernetes.containers[_]
    	not utils.has_key(allContainers, "securityContext")
    	container := allContainers.name
    }

    # failRunAsGroup is true if securityContext.runAsGroup is less than or
    # equal to 10000 or if securityContext.runAsGroup is not set.
    failRunAsGroup {
    	count(getGroupIdContainers) > 0
    }

    deny[res] {
    	failRunAsGroup

    	msg := kubernetes.format(sprintf("Container '%s' of %s '%s' should set 'securityContext.runAsGroup' > 10000", [getGroupIdContainers[_], kubernetes.kind, kubernetes.name]))
    	res := {
    		"msg": msg,
    		"id": __rego_metadata__.id,
    		"title": __rego_metadata__.title,
    		"severity": __rego_metadata__.severity,
    		"type": __rego_metadata__.type,
    	}
    }
  policy.runs_with_UID_le_10000.kinds: Workload
  policy.runs_with_UID_le_10000.rego: |
    package appshield.kubernetes.KSV020

    import data.lib.kubernetes
    import data.lib.utils

    default failRunAsUser = false

    __rego_metadata__ := {
    	"id": "KSV020",
    	"avd_id": "AVD-KSV-0020",
    	"title": "Runs with low user ID",
    	"short_code": "use-high-uid",
    	"version": "v1.0.0",
    	"severity": "MEDIUM",
    	"type": "Kubernetes Security Check",
    	"description": "Force the container to run with user ID > 10000 to avoid conflicts with the host’s user table.",
    	"recommended_actions": "Set 'containers[].securityContext.runAsUser' to an integer > 10000.",
    	"url": "https://kubesec.io/basics/containers-securitycontext-runasuser/",
    }

    __rego_input__ := {
    	"combine": false,
    	"selector": [{"type": "kubernetes"}],
    }

    # getUserIdContainers returns the names of all containers which have
    # securityContext.runAsUser less than or equal to 100000.
    getUserIdContainers[container] {
    	allContainers := kubernetes.containers[_]
    	allContainers.securityContext.runAsUser <= 10000
    	container := allContainers.name
    }

    # getUserIdContainers returns the names of all containers which do
    # not have securityContext.runAsUser set.
    getUserIdContainers[container] {
    	allContainers := kubernetes.containers[_]
    	not utils.has_key(allContainers.securityContext, "runAsUser")
    	container := allContainers.name
    }

    # getUserIdContainers returns the names of all containers which do
    # not have securityContext set.
    getUserIdContainers[container] {
    	allContainers := kubernetes.containers[_]
    	not utils.has_key(allContainers, "securityContext")
    	container := allContainers.name
    }

    # failRunAsUser is true if securityContext.runAsUser is less than or
    # equal to 10000 or if securityContext.runAsUser is not set.
    failRunAsUser {
    	count(getUserIdContainers) > 0
    }

    deny[res] {
    	failRunAsUser

    	msg := kubernetes.format(sprintf("Container '%s' of %s '%s' should set 'securityContext.runAsUser' > 10000", [getUserIdContainers[_], kubernetes.kind, kubernetes.name]))

    	res := {
    		"msg": msg,
    		"id": __rego_metadata__.id,
    		"title": __rego_metadata__.title,
    		"severity": __rego_metadata__.severity,
    		"type": __rego_metadata__.type,
    	}
    }
  policy.selector_usage_in_network_policies.kinds: NetworkPolicy
  policy.selector_usage_in_network_policies.rego: |
    package appshield.kubernetes.KSV038

    import data.lib.kubernetes
    import data.lib.utils

    __rego_metadata__ := {
    	"id": "KSV038",
    	"avd_id": "AVD-KSV-0038",
    	"title": "Selector usage in network policies",
    	"short_code": "selector-usage-in-network-policies",
    	"version": "v1.0.0",
    	"severity": "MEDIUM",
    	"type": "Kubernetes Security Check",
    	"description": "ensure that network policies selectors are applied to pods or namespaces to restricted ingress and egress traffic within the pod network",
    	"recommended_actions": "create network policies and ensure that pods are selected using the podSelector and/or the namespaceSelector options",
    	"url": "https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/",
    }

    __rego_input__ := {
    	"combine": false,
    	"selector": [{"type": "kubernetes"}],
    }

    deny[res] {
    	not hasSelector(input.spec)
    	msg := "Network policy should uses podSelector and/or the namespaceSelector to restrict ingress and egress traffic within the Pod network"
    	res := {
    		"msg": msg,
    		"id": __rego_metadata__.id,
    		"title": __rego_metadata__.title,
    		"severity": __rego_metadata__.severity,
    		"type": __rego_metadata__.type,
    	}
    }

    hasSelector(spec) {
    	lower(kubernetes.kind) == "networkpolicy"
    	kubernetes.has_field(spec, "podSelector")
    	kubernetes.has_field(spec.podSelector, "matchLabels")
    }

    hasSelector(spec) {
    	lower(kubernetes.kind) == "networkpolicy"
    	kubernetes.has_field(spec, "namespaceSelector")
    }

    hasSelector(spec) {
    	lower(kubernetes.kind) == "networkpolicy"
    	kubernetes.has_field(spec, "podSelector")
    }

    hasSelector(spec) {
    	lower(kubernetes.kind) == "networkpolicy"
    	kubernetes.has_field(spec, "ingress")
    	kubernetes.has_field(spec.ingress[_], "from")
    	kubernetes.has_field(spec.ingress[_].from[_], "namespaceSelector")
    }

    hasSelector(spec) {
    	lower(kubernetes.kind) == "networkpolicy"
    	kubernetes.has_field(spec, "ingress")
    	kubernetes.has_field(spec.ingress[_], "from")
    	kubernetes.has_field(spec.ingress[_].from[_], "podSelector")
    }

    hasSelector(spec) {
    	lower(kubernetes.kind) == "networkpolicy"
    	kubernetes.has_field(spec, "egress")
    	kubernetes.has_field(spec.egress[_], "to")
    	kubernetes.has_field(spec.egress[_].to[_], "podSelector")
    }

    hasSelector(spec) {
    	lower(kubernetes.kind) == "networkpolicy"
    	kubernetes.has_field(spec, "egress")
    	kubernetes.has_field(spec.egress[_], "to")
    	kubernetes.has_field(spec.egress[_].to[_], "namespaceSelector")
    }

    hasSelector(spec) {
    	lower(kubernetes.kind) == "networkpolicy"
    	kubernetes.spec.podSelector == {}
    	contains(input.spec.policyType, "Egress")
    }

    hasSelector(spec) {
    	lower(kubernetes.kind) == "networkpolicy"
    	kubernetes.spec.podSelector == {}
    	contains(input.spec.policyType, "Ingress")
    }

    contains(arr, elem) {
    	arr[_] = elem
    }
  policy.tiller_is_deployed.kinds: Workload
  policy.tiller_is_deployed.rego: |
    package appshield.kubernetes.KSV202

    import data.lib.kubernetes

    __rego_metadata__ := {
    	"id": "KSV102",
    	"avd_id": "AVD-KSV-0102",
    	"title": "Tiller Is Deployed",
    	"short_code": "no-tiller",
    	"version": "v1.0.0",
    	"severity": "Critical",
    	"type": "Kubernetes Security Check",
    	"description": "Check if Helm Tiller component is deployed.",
    	"recommended_actions": "Migrate to Helm v3 which no longer has Tiller component",
    }

    __rego_input__ := {
    	"combine": false,
    	"selector": [{"type": "kubernetes"}],
    }

    # Get all containers and check kubernetes metadata for tiller
    tillerDeployed[container] {
    	currentContainer := kubernetes.containers[_]
    	checkMetadata(input.metadata)
    	container := currentContainer.name
    }

    # Get all containers and check each image for tiller
    tillerDeployed[container] {
    	currentContainer := kubernetes.containers[_]
    	contains(currentContainer.image, "tiller")
    	container := currentContainer.name
    }

    # Get all pods and check each metadata for tiller
    tillerDeployed[pod] {
    	currentPod := kubernetes.pods[_]
    	checkMetadata(currentPod.metadata)
    	pod := currentPod.metadata.name
    }

    deny[res] {
    	msg := kubernetes.format(sprintf("container '%s' of %s '%s' in '%s' namespace shouldn't have tiller deployed", [tillerDeployed[_], lower(kubernetes.kind), kubernetes.name, kubernetes.namespace]))

    	res := {
    		"msg": msg,
    		"id": __rego_metadata__.id,
    		"title": __rego_metadata__.title,
    		"severity": __rego_metadata__.severity,
    		"type": __rego_metadata__.type,
    	}
    }

    # Check for tiller by resource name
    checkMetadata(metadata) {
    	contains(metadata.name, "tiller")
    }

    # Check for tiller by app label
    checkMetadata(metadata) {
    	metadata.labels.app == "helm"
    }

    # Check for tiller by name label
    checkMetadata(metadata) {
    	metadata.labels.name == "tiller"
    }
  policy.use_limit_range.kinds: LimitRange
  policy.use_limit_range.rego: |
    package appshield.kubernetes.KSV039

    import data.lib.kubernetes
    import data.lib.utils

    __rego_metadata__ := {
    	"id": "KSV039",
    	"avd_id": "AVD-KSV-0039",
    	"title": "limit range usage",
    	"short_code": "limit-range-usage",
    	"severity": "LOW",
    	"type": "Kubernetes Security Check",
    	"description": "ensure limit range policy has configure in order to limit resource usage for namespaces or nodes",
    	"recommended_actions": "create limit range policy with a default request and limit, min and max request, for each container.",
    	"url": "https://kubernetes.io/docs/concepts/policy/limit-range/",
    }

    __rego_input__ := {
    	"combine": false,
    	"selector": [{"type": "kubernetes"}],
    }

    deny[res] {
    	not limitRangeConfigure
    	msg := "limit range policy with a default request and limit, min and max request, for each container should be configure"
    	res := {
    		"msg": msg,
    		"id": __rego_metadata__.id,
    		"title": __rego_metadata__.title,
    		"severity": __rego_metadata__.severity,
    		"type": __rego_metadata__.type,
    	}
    }

    limitRangeConfigure {
    	lower(input.kind) == "limitrange"
    	input.spec[limits]
    	kubernetes.has_field(input.spec.limits[_], "type")
    	kubernetes.has_field(input.spec.limits[_], "max")
    	kubernetes.has_field(input.spec.limits[_], "min")
    	kubernetes.has_field(input.spec.limits[_], "default")
    	kubernetes.has_field(input.spec.limits[_], "defaultRequest")
    }
  policy.use_resource_quota.kinds: ResourceQuota
  policy.use_resource_quota.rego: |
    package appshield.kubernetes.KSV040

    import data.lib.kubernetes
    import data.lib.utils

    __rego_metadata__ := {
    	"id": "KSV040",
    	"avd_id": "AVD-KSV-0040",
    	"title": "resource quota usage",
    	"short_code": "resource-quota-usage",
    	"severity": "LOW",
    	"type": "Kubernetes Security Check",
    	"description": "ensure resource quota policy has configure in order to limit aggregate resource usage within namespace",
    	"recommended_actions": "create resource quota policy with mem and cpu quota per each namespace",
    	"url": "https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/",
    }

    __rego_input__ := {
    	"combine": false,
    	"selector": [{"type": "kubernetes"}],
    }

    deny[res] {
    	not resourceQuotaConfigure
    	msg := "resource quota policy with hard memory and cpu quota per namespace should be configure"
    	res := {
    		"msg": msg,
    		"id": __rego_metadata__.id,
    		"title": __rego_metadata__.title,
    		"severity": __rego_metadata__.severity,
    		"type": __rego_metadata__.type,
    	}
    }

    resourceQuotaConfigure {
    	lower(input.kind) == "resourcequota"
    	input.spec[hard]
    	kubernetes.has_field(input.spec.hard, "requests.cpu")
    	kubernetes.has_field(input.spec.hard, "requests.memory")
    	kubernetes.has_field(input.spec.hard, "limits.cpu")
    	kubernetes.has_field(input.spec.hard, "limits.memory")
    }
  policy.uses_image_tag_latest.kinds: Workload
  policy.uses_image_tag_latest.rego: |
    package appshield.kubernetes.KSV013

    import data.lib.kubernetes

    default checkUsingLatestTag = false

    __rego_metadata__ := {
    	"id": "KSV013",
    	"avd_id": "AVD-KSV-0013",
    	"title": "Image tag ':latest' used",
    	"short_code": "use-specific-tags",
    	"version": "v1.0.0",
    	"severity": "LOW",
    	"type": "Kubernetes Security Check",
    	"description": "It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version.",
    	"recommended_actions": "Use a specific container image tag that is not 'latest'.",
    	"url": "https://kubernetes.io/docs/concepts/configuration/overview/#container-images",
    }

    __rego_input__ := {
    	"combine": false,
    	"selector": [{"type": "kubernetes"}],
    }

    # getTaggedContainers returns the names of all containers which
    # have tagged images.
    getTaggedContainers[container] {
    	# If the image defines a digest value, we don't care about the tag
    	allContainers := kubernetes.containers[_]
    	digest := split(allContainers.image, "@")[1]
    	container := allContainers.name
    }

    getTaggedContainers[container] {
    	# No digest, look at tag
    	allContainers := kubernetes.containers[_]
    	tag := split(allContainers.image, ":")[1]
    	tag != "latest"
    	container := allContainers.name
    }

    # getUntaggedContainers returns the names of all containers which
    # have untagged images or images with the latest tag.
    getUntaggedContainers[container] {
    	container := kubernetes.containers[_].name
    	not getTaggedContainers[container]
    }

    # checkUsingLatestTag is true if there is a container whose image tag
    # is untagged or uses the latest tag.
    checkUsingLatestTag {
    	count(getUntaggedContainers) > 0
    }

    deny[res] {
    	checkUsingLatestTag

    	msg := kubernetes.format(sprintf("Container '%s' of %s '%s' should specify an image tag", [getUntaggedContainers[_], kubernetes.kind, kubernetes.name]))

    	res := {
    		"msg": msg,
    		"id": __rego_metadata__.id,
    		"title": __rego_metadata__.title,
    		"severity": __rego_metadata__.severity,
    		"type": __rego_metadata__.type,
    	}
    }
  policy.uses_untrusted_azure_registry.kinds: Workload
  policy.uses_untrusted_azure_registry.rego: |
    package appshield.kubernetes.KSV032

    import data.lib.kubernetes
    import data.lib.utils

    default failTrustedAzureRegistry = false

    __rego_metadata__ := {
    	"id": "KSV032",
    	"avd_id": "AVD-KSV-0032",
    	"title": "All container images must start with the *.azurecr.io domain",
    	"short_code": "use-azure-image-prefix",
    	"version": "v1.0.0",
    	"severity": "MEDIUM",
    	"type": "Kubernetes Security Check",
    	"description": "Containers should only use images from trusted registries.",
    	"recommended_actions": "Use images from trusted Azure registries.",
    }

    __rego_input__ := {
    	"combine": false,
    	"selector": [{"type": "kubernetes"}],
    }

    # getContainersWithTrustedAzureRegistry returns a list of containers
    # with image from a trusted Azure registry
    getContainersWithTrustedAzureRegistry[name] {
    	container := kubernetes.containers[_]
    	image := container.image

    	# get image registry/repo parts
    	image_parts := split(image, "/")

    	# images with only one part do not specify a registry
    	count(image_parts) > 1
    	registry = image_parts[0]
    	endswith(registry, "azurecr.io")
    	name := container.name
    }

    # getContainersWithUntrustedAzureRegistry returns a list of containers
    # with image from an untrusted Azure registry
    getContainersWithUntrustedAzureRegistry[name] {
    	name := kubernetes.containers[_].name
    	not getContainersWithTrustedAzureRegistry[name]
    }

    # failTrustedAzureRegistry is true if a container uses an image from an
    # untrusted Azure registry
    failTrustedAzureRegistry {
    	count(getContainersWithUntrustedAzureRegistry) > 0
    }

    deny[res] {
    	failTrustedAzureRegistry

    	msg := kubernetes.format(sprintf("container %s of %s %s in %s namespace should restrict container image to your specific registry domain. For Azure any domain ending in 'azurecr.io'", [getContainersWithUntrustedAzureRegistry[_], lower(kubernetes.kind), kubernetes.name, kubernetes.namespace]))

    	res := {
    		"msg": msg,
    		"id": __rego_metadata__.id,
    		"title": __rego_metadata__.title,
    		"severity": __rego_metadata__.severity,
    		"type": __rego_metadata__.type,
    	}
    }
  policy.uses_untrusted_gcr_registry.kinds: Workload
  policy.uses_untrusted_gcr_registry.rego: |
    package appshield.kubernetes.KSV033

    import data.lib.kubernetes
    import data.lib.utils

    default failTrustedGCRRegistry = false

    __rego_metadata__ := {
    	"id": "KSV033",
    	"avd_id": "AVD-KSV-0033",
    	"title": "All container images must start with a GCR domain",
    	"short_code": "use-gcr-domain",
    	"version": "v1.0.0",
    	"severity": "MEDIUM",
    	"type": "Kubernetes Security Check",
    	"description": "Containers should only use images from trusted GCR registries.",
    	"recommended_actions": "Use images from trusted GCR registries.",
    }

    __rego_input__ := {
    	"combine": false,
    	"selector": [{"type": "kubernetes"}],
    }

    # list of trusted GCR registries
    trusted_gcr_registries = [
    	"gcr.io",
    	"us.gcr.io",
    	"eu.gcr.io",
    	"asia.gcr.io",
    ]

    # getContainersWithTrustedGCRRegistry returns a list of containers
    # with image from a trusted gcr registry
    getContainersWithTrustedGCRRegistry[name] {
    	container := kubernetes.containers[_]
    	image := container.image

    	# get image registry/repo parts
    	image_parts := split(image, "/")

    	# images with only one part do not specify a registry
    	count(image_parts) > 1
    	registry = image_parts[0]
    	trusted := trusted_gcr_registries[_]
    	endswith(registry, trusted)
    	name := container.name
    }

    # getContainersWithUntrustedGCRRegistry returns a list of containers
    # with image from an untrusted gcr registry
    getContainersWithUntrustedGCRRegistry[name] {
    	name := kubernetes.containers[_].name
    	not getContainersWithTrustedGCRRegistry[name]
    }

    # failTrustedGCRRegistry is true if a container uses an image from an
    # untrusted gcr registry
    failTrustedGCRRegistry {
    	count(getContainersWithUntrustedGCRRegistry) > 0
    }

    deny[res] {
    	failTrustedGCRRegistry

    	msg := kubernetes.format(sprintf("container %s of %s %s in %s namespace should restrict container image to your specific registry domain. See the full GCR list here: https://cloud.google.com/container-registry/docs/overview#registries", [getContainersWithUntrustedGCRRegistry[_], lower(kubernetes.kind), kubernetes.name, kubernetes.namespace]))

    	res := {
    		"msg": msg,
    		"id": __rego_metadata__.id,
    		"title": __rego_metadata__.title,
    		"severity": __rego_metadata__.severity,
    		"type": __rego_metadata__.type,
    	}
    }
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: release-name-starboard-operator
  labels:
    helm.sh/chart: starboard-operator-0.10.4
    app.kubernetes.io/name: starboard-operator
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 0.15.4
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups:
      - ""
    resources:
      - pods
      - pods/log
      - replicationcontrollers
      - services
      - resourcequotas
      - limitranges
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - configmaps
      - secrets
      - serviceaccounts
    verbs:
      - list
      - watch
      - get
      - create
      - update
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - delete
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
  - apiGroups:
      - apps
    resources:
      - replicasets
      - statefulsets
      - daemonsets
      - deployments
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - batch
    resources:
      - jobs
      - cronjobs
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - rbac.authorization.k8s.io
    resources:
      - roles
      - rolebindings
      - clusterroles
      - clusterrolebindings
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - batch
    resources:
      - jobs
    verbs:
      - create
      - delete
  - apiGroups:
      - networking.k8s.io
    resources:
      - networkpolicies
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - policy
    resources:
      - podsecuritypolicies
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - aquasecurity.github.io
    resources:
      - vulnerabilityreports
      - configauditreports
      - clusterconfigauditreports
      - ciskubebenchreports
      - clustercompliancereports
      - clustercompliancedetailreports
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - delete
  - apiGroups:
      - aquasecurity.github.io
    resources:
      - clustercompliancereports/status
    verbs:
      - update
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: release-name-starboard-operator
  labels:
    helm.sh/chart: starboard-operator-0.10.4
    app.kubernetes.io/name: starboard-operator
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 0.15.4
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: release-name-starboard-operator
subjects:
  - kind: ServiceAccount
    name: release-name-starboard-operator
    namespace: default
---
apiVersion: v1
kind: Service
metadata:
  name: release-name-starboard-operator
  labels:
    helm.sh/chart: starboard-operator-0.10.4
    app.kubernetes.io/name: starboard-operator
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 0.15.4
    app.kubernetes.io/managed-by: Helm
  annotations:
    prometheus.io/path: /metrics
    prometheus.io/scrape: "true"
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: metrics
      name: metrics
  selector:
    app.kubernetes.io/name: starboard-operator
    app.kubernetes.io/instance: release-name
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-starboard-operator
  labels:
    helm.sh/chart: starboard-operator-0.10.4
    app.kubernetes.io/name: starboard-operator
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 0.15.4
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app.kubernetes.io/name: starboard-operator
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      labels:
        app.kubernetes.io/name: starboard-operator
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: release-name-starboard-operator
      automountServiceAccountToken: true
      containers:
        - name: starboard-operator
          image: docker.io/aquasec/starboard-operator:0.15.4
          imagePullPolicy: IfNotPresent
          env:
            - name: OPERATOR_NAMESPACE
              value: default
            - name: OPERATOR_TARGET_NAMESPACES
              value: ""
            - name: OPERATOR_EXCLUDE_NAMESPACES
              value: kube-system,default
            - name: OPERATOR_SERVICE_ACCOUNT
              value: release-name-starboard-operator
            - name: OPERATOR_LOG_DEV_MODE
              value: "false"
            - name: OPERATOR_SCAN_JOB_TIMEOUT
              value: 5m
            - name: OPERATOR_CONCURRENT_SCAN_JOBS_LIMIT
              value: "10"
            - name: OPERATOR_SCAN_JOB_RETRY_AFTER
              value: 30s
            - name: OPERATOR_BATCH_DELETE_LIMIT
              value: "10"
            - name: OPERATOR_BATCH_DELETE_DELAY
              value: 10s
            - name: OPERATOR_METRICS_BIND_ADDRESS
              value: :8080
            - name: OPERATOR_HEALTH_PROBE_BIND_ADDRESS
              value: :9090
            - name: OPERATOR_CIS_KUBERNETES_BENCHMARK_ENABLED
              value: "true"
            - name: OPERATOR_VULNERABILITY_SCANNER_ENABLED
              value: "true"
            - name: OPERATOR_VULNERABILITY_SCANNER_SCAN_ONLY_CURRENT_REVISIONS
              value: "false"
            - name: OPERATOR_VULNERABILITY_SCANNER_REPORT_TTL
              value: ""
            - name: OPERATOR_CONFIG_AUDIT_SCANNER_ENABLED
              value: "false"
            - name: OPERATOR_CONFIG_AUDIT_SCANNER_SCAN_ONLY_CURRENT_REVISIONS
              value: "false"
            - name: OPERATOR_CONFIG_AUDIT_SCANNER_BUILTIN
              value: "true"
            - name: OPERATOR_CLUSTER_COMPLIANCE_ENABLED
              value: "true"
          ports:
            - name: metrics
              containerPort: 8080
            - name: probes
              containerPort: 9090
          readinessProbe:
            httpGet:
              path: /readyz/
              port: probes
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            failureThreshold: 3
          livenessProbe:
            httpGet:
              path: /healthz/
              port: probes
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            failureThreshold: 10
          resources:
            seccompProfile:
              type: RuntimeDefault
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            privileged: false
            readOnlyRootFilesystem: true
      securityContext: {}
---
apiVersion: aquasecurity.github.io/v1alpha1
kind: ClusterComplianceReport
metadata:
  name: nsa
  labels:
    app.kubernetes.io/name: starboard-operator
    app.kubernetes.io/instance: starboard-operator
    app.kubernetes.io/version: 0.15.1
    app.kubernetes.io/managed-by: kubectl
spec:
  name: nsa
  description: National Security Agency - Kubernetes Hardening Guidance
  version: "1.0"
  cron: 0 */3 * * *
  controls:
    - name: Non-root containers
      description: Check that container is not running as root
      id: "1.0"
      kinds:
        - Workload
      mapping:
        scanner: config-audit
        checks:
          - id: KSV012
      severity: MEDIUM
    - name: Immutable container file systems
      description: Check that container root file system is immutable
      id: "1.1"
      kinds:
        - Workload
      mapping:
        scanner: config-audit
        checks:
          - id: KSV014
      severity: LOW
    - name: Preventing privileged containers
      description: Controls whether Pods can run privileged containers
      id: "1.2"
      kinds:
        - Workload
      mapping:
        scanner: config-audit
        checks:
          - id: KSV017
      severity: HIGH
    - name: Share containers process namespaces
      description: Controls whether containers can share process namespaces
      id: "1.3"
      kinds:
        - Workload
      mapping:
        scanner: config-audit
        checks:
          - id: KSV008
      severity: HIGH
    - name: Share host process namespaces
      description: Controls whether share host process namespaces
      id: "1.4"
      kinds:
        - Workload
      mapping:
        scanner: config-audit
        checks:
          - id: KSV009
      severity: HIGH
    - name: Use the host network
      description: Controls whether containers can use the host network
      id: "1.5"
      kinds:
        - Workload
      mapping:
        scanner: config-audit
        checks:
          - id: KSV010
      severity: HIGH
    - name: Run with root privileges or with root group membership
      description: Controls whether container applications can run with root privileges or with root group membership
      id: "1.6"
      kinds:
        - Workload
      mapping:
        scanner: config-audit
        checks:
          - id: KSV029
      severity: LOW
    - name: Restricts escalation to root privileges
      description: Control check restrictions escalation to root privileges
      id: "1.7"
      kinds:
        - Workload
      mapping:
        scanner: config-audit
        checks:
          - id: KSV001
      severity: MEDIUM
    - name: Sets the SELinux context of the container
      description: Control checks if pod sets the SELinux context of the container
      id: "1.8"
      kinds:
        - Workload
      mapping:
        scanner: config-audit
        checks:
          - id: KSV002
      severity: MEDIUM
    - name: Restrict a container's access to resources with AppArmor
      description: Control checks the restriction of containers access to resources with AppArmor
      id: "1.9"
      kinds:
        - Workload
      mapping:
        scanner: config-audit
        checks:
          - id: KSV030
      severity: MEDIUM
    - name: Sets the seccomp profile used to sandbox containers.
      description: Control checks the sets the seccomp profile used to sandbox containers
      id: "1.10"
      kinds:
        - Workload
      mapping:
        scanner: config-audit
        checks:
          - id: KSV030
      severity: LOW
    - name: Protecting Pod service account tokens
      description: 'Control check whether disable secret token been mount ,automountServiceAccountToken: false'
      id: "1.11"
      kinds:
        - Workload
      mapping:
        scanner: config-audit
        checks:
          - id: KSV036
      severity: MEDIUM
    - name: Namespace kube-system should not be used by users
      description: Control check whether Namespace kube-system is not be used by users
      id: "1.12"
      kinds:
        - NetworkPolicy
      defaultStatus: FAIL
      mapping:
        scanner: config-audit
        checks:
          - id: KSV037
      severity: MEDIUM
    - name: Pod and/or namespace Selectors usage
      description: Control check validate the pod and/or namespace Selectors usage
      id: "2.0"
      kinds:
        - NetworkPolicy
      defaultStatus: FAIL
      mapping:
        scanner: config-audit
        checks:
          - id: KSV038
      severity: MEDIUM
    - name: Use CNI plugin that supports NetworkPolicy API
      description: Control check whether check cni plugin installed
      id: "3.0"
      kinds:
        - Node
      mapping:
        scanner: kube-bench
        checks:
          - id: 5.3.1
      severity: CRITICAL
    - name: Use ResourceQuota policies to limit resources
      description: Control check the use of ResourceQuota policy to limit aggregate resource usage within namespace
      id: "4.0"
      kinds:
        - ResourceQuota
      defaultStatus: FAIL
      mapping:
        scanner: config-audit
        checks:
          - id: KSV040
      severity: MEDIUM
    - name: Use LimitRange policies to limit resources
      description: Control check the use of LimitRange policy limit resource usage for namespaces or nodes
      id: "4.1"
      kinds:
        - LimitRange
      defaultStatus: FAIL
      mapping:
        scanner: config-audit
        checks:
          - id: KSV039
      severity: MEDIUM
    - name: Control plan disable insecure port
      description: Control check whether control plan disable insecure port
      id: "5.0"
      kinds:
        - Node
      mapping:
        scanner: kube-bench
        checks:
          - id: 1.2.19
      severity: CRITICAL
    - name: Encrypt etcd communication
      description: Control check whether etcd communication is encrypted
      id: "5.1"
      kinds:
        - Node
      mapping:
        scanner: kube-bench
        checks:
          - id: "2.1"
      severity: CRITICAL
    - name: Ensure kube config file permission
      description: Control check whether kube config file permissions
      id: "6.0"
      kinds:
        - Node
      mapping:
        scanner: kube-bench
        checks:
          - id: 4.1.3
          - id: 4.1.4
      severity: CRITICAL
    - name: Check that encryption resource has been set
      description: Control checks whether encryption resource has been set
      id: "6.1"
      kinds:
        - Node
      mapping:
        scanner: kube-bench
        checks:
          - id: 1.2.31
          - id: 1.2.32
      severity: CRITICAL
    - name: Check encryption provider
      description: Control checks whether encryption provider has been set
      id: "6.2"
      kinds:
        - Node
      mapping:
        scanner: kube-bench
        checks:
          - id: 1.2.3
      severity: CRITICAL
    - name: Make sure anonymous-auth is unset
      description: Control checks whether anonymous-auth is unset
      id: "7.0"
      kinds:
        - Node
      mapping:
        scanner: kube-bench
        checks:
          - id: 1.2.1
      severity: CRITICAL
    - name: Make sure -authorization-mode=RBAC
      description: Control check whether RBAC permission is in use
      id: "7.1"
      kinds:
        - Node
      mapping:
        scanner: kube-bench
        checks:
          - id: 1.2.7
          - id: 1.2.8
      severity: CRITICAL
    - name: Audit policy is configure
      description: Control check whether audit policy is configure
      id: "8.0"
      kinds:
        - Node
      mapping:
        scanner: kube-bench
        checks:
          - id: 3.2.1
      severity: HIGH
    - name: Audit log path is configure
      description: Control check whether audit log path is configure
      id: "8.1"
      kinds:
        - Node
      mapping:
        scanner: kube-bench
        checks:
          - id: 1.2.22
      severity: MEDIUM
    - name: Audit log aging
      description: Control check whether audit log aging is configure
      id: "8.2"
      kinds:
        - Node
      mapping:
        scanner: kube-bench
        checks:
          - id: 1.2.23
      severity: MEDIUM
