apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: release-name-atomix-pdb
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app: release-name-atomix
      chart: atomix-0.1.9
      release: release-name
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: bbsim-sadis-server
  namespace: 9oD12SZW
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-kafka
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-14.9.0
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: kafka
automountServiceAccountToken: true
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: onos-config-loader
---
apiVersion: v1
kind: Secret
metadata:
  name: release-name-etcd-jwt-token
  namespace: 9oD12SZW
  labels:
    app.kubernetes.io/name: etcd
    helm.sh/chart: etcd-6.13.9
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  jwt-token.pem: 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
---
apiVersion: v1
data:
  clients.conf: |
    client 0.0.0.0/0 {
      secret = SECRET
    }
  users: |
    user Cleartext-Password := "password", MS-CHAP-Use-NTLM-Auth := 0
kind: ConfigMap
metadata:
  name: freeradius-config
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-kafka-scripts
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-14.9.0
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
data:
  setup.sh: |-
    #!/bin/bash

    ID="${MY_POD_NAME#"release-name-kafka-"}"
    if [[ -f "/bitnami/kafka/data/meta.properties" ]]; then
        export KAFKA_CFG_BROKER_ID="$(grep "broker.id" /bitnami/kafka/data/meta.properties | awk -F '=' '{print $2}')"
    else
        export KAFKA_CFG_BROKER_ID="$((ID + 0))"
    fi

    exec /entrypoint.sh /run.sh
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-atomix-config
  labels:
    app: release-name-atomix
    chart: atomix-0.1.9
    release: release-name
    heritage: Helm
data:
  atomix.conf: |
    cluster {
      node: ${atomix.node}

      discovery {
        type: bootstrap
        nodes: ${atomix.nodes}
      }
    }
    managementGroup {
      type: raft
      name: system
      partitionSize: 3
      partitions: 1
      members: ${atomix.members}
    }
    partitionGroups.raft {
      type: raft
      name: raft
      partitionSize: 3
      partitions: ${atomix.replicas}
      members: ${atomix.members}
    }
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-atomix-init-scripts
  labels:
    app: release-name-atomix
    chart: atomix-0.1.9
    release: release-name
    heritage: Helm
data:
  create-config.sh: |
    #!/usr/bin/env bash

    HOST=`hostname -s`
    DOMAIN=`hostname -d`
    SERVICE=${DOMAIN%%.}
    NODES=1

    function print_usage() {
    echo "\
    Usage: create-config [OPTIONS]
    Creates an Atomix configuration from the supplied arguments.
        --nodes             The number of nodes in the cluster. The default value is 1.
    "
    }

    function print_variables() {
        echo "atomix.service=$SERVICE"
        echo "atomix.node.id=$NAME-$ORD"
        echo "atomix.node.address=$NAME-$ORD.$DOMAIN:5679"
        echo "atomix.replicas=$NODES"
        for (( i=0; i<$NODES; i++ ))
        do
            echo "atomix.members.$i=$NAME-$((i))"
            echo "atomix.nodes.$i.id=$NAME-$((i))"
            echo "atomix.nodes.$i.address=$NAME-$((i)).$DOMAIN:5679"
        done
    }

    function create_config() {
        print_variables
    }

    optspec=":hv-:"
    while getopts "$optspec" optchar; do

        case "${optchar}" in
            -)
                case "${OPTARG}" in
                    nodes=*)
                        NODES=${OPTARG##*=}
                        ;;
                    *)
                        echo "Unknown option --${OPTARG}" >&2
                        exit 1
                        ;;
                esac;;
            h)
                print_usage
                exit
                ;;
            v)
                echo "Parsing option: '-${optchar}'" >&2
                ;;
            *)
                if [ "$OPTERR" != 1 ] || [ "${optspec:0:1}" = ":" ]; then
                    echo "Non-option argument: '-${OPTARG}'" >&2
                fi
                ;;
        esac
    done

    if [[ $HOST =~ (.*)-([0-9]+)$ ]]; then
        NAME=${BASH_REMATCH[1]}
        ORD=${BASH_REMATCH[2]}
    else
        echo "Failed to parse name and ordinal of Pod"
        exit 1
        #NAME=test
        #ORD=0
    fi

    create_config
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-onos-classic-init-scripts
  labels:
    app: onos-classic
    chart: onos-classic-0.1.29
    release: release-name
    heritage: Helm
data:
  configure-onos.sh: |
    #!/usr/bin/env bash

    HOST=`hostname -s`
    DOMAIN=`hostname -d`
    ID=onos-`hostname -s | awk -F '-' '{print $NF}'`

    function print_config() {
        echo "{"
        print_name
        print_node
        print_storage
        echo "}"
    }

    function print_name() {
        echo "  \"name\": \"atomix\","
    }

    function print_node() {
        echo "  \"node\": {"
        echo "    \"id\": \"$ID\","
        echo "    \"host\": \"$HOST.$DOMAIN\","
        echo "    \"port\": 9876"
        echo "  },"
    }

    function print_storage() {
        echo "  \"storage\": ["
        for (( i=1; i<=$ATOMIX_REPLICAS; i++ ))
        do
            if [ $i -eq $ATOMIX_REPLICAS ]; then
                echo "    {"
                echo "      \"id\": \"$ATOMIX_SERVICE-$((i))\","
                echo "      \"host\": \"$ATOMIX_SERVICE-$((i-1)).$ATOMIX_SERVICE-hs\","
                echo "      \"port\": 5679"
                echo "    }"
            else
                echo "    {"
                echo "      \"id\": \"$ATOMIX_SERVICE-$((i))\","
                echo "      \"host\": \"$ATOMIX_SERVICE-$((i-1)).$ATOMIX_SERVICE-hs\","
                echo "      \"port\": 5679"
                echo "    },"
            fi
        done
        echo "  ]"
    }

    ATOMIX_SERVICE=$1
    ATOMIX_REPLICAS=$2

    until nslookup "$ATOMIX_SERVICE-api" > /dev/null 2>&1; do sleep 2; done;

    print_config
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-onos-classic-probe-scripts
  labels:
    app: onos-classic
    chart: onos-classic-0.1.29
    release: release-name
    heritage: Helm
data:
  check-onos-status: |
    #!/bin/bash

    # Wait for 5s before timing out
    host=`hostname -s`
    id=onos-`hostname -s | awk -F '-' '{print $NF}'`
    result=$(curl -m 5 -s -f http://$host:8181/onos/v1/cluster/$id --user onos:rocks)
    echo $result

    if ! printf '%q' $result | grep -q -i "READY"; then
      echo "Cluster is not yet ready"
      exit -1
    fi

    apps=()
    apps+=("org.onosproject.lldpprovider")
    apps+=("org.onosproject.openflow-base")
    apps+=("org.onosproject.gui2")
    apps+=("org.onosproject.drivers")
    apps+=("org.onosproject.mcast")
    apps+=("org.onosproject.segmentrouting")
    apps+=("org.opencord.kafka")
    apps+=("org.opencord.sadis")
    apps+=("org.opencord.dhcpl2relay")
    apps+=("org.opencord.igmpproxy")
    apps+=("org.opencord.mcast")
    apps+=("org.opencord.olt")
    apps+=("org.opencord.aaa")

    for app in ${apps[@]}; do
      result=$(curl -m 5 -s -f http://$host:8181/onos/v1/applications/$app/health --user onos:rocks)
      echo $result
      if ! printf %q $result | grep -q -i "READY"; then
        echo "$app is not yet ready"
        exit -1
      fi
    done
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-onos-configs-data
data:
  netcfg.json: |
    {
      "apps" : {
        "org.opencord.dhcpl2relay" : {
          "dhcpl2relay" : {
            "useOltUplinkForServerPktInOut" : true
          }
        },
        "org.opencord.kafka": {
          "kafka" : {
            "bootstrapServers" : "release-name-kafka.default.svc:9092"
          }
        },
        "org.opencord.aaa" : {
          "AAA": {
            "radiusConnectionType" : "socket",
            "radiusHost": "release-name-freeradius.default.svc",
            "radiusServerPort": "1812",
            "radiusSecret": "SECRET"
          }
        },
        "org.opencord.sadis": {
          "sadis": {
            "integration": {
              "url": "http://bbsim-sadis-server.default.svc:58080/subscribers/%s",
              "cache": {
                "enabled": true,
                "maxsize": 50,
                "ttl": "PT1m"
              }
            }
          },
          "bandwidthprofile": {
            "integration": {
              "url": "http://bbsim-sadis-server.default.svc:58080/profiles/%s",
              "cache": {
                "enabled": true,
                "maxsize": 50,
                "ttl": "PT1m"
              }
            }
          }
        }
      }
    }
  org.onosproject.net.flow.impl.FlowRuleManager: |
    {
      "purgeOnDisconnection": "false"
    }
  org.onosproject.net.group.impl.GroupManager: |
    {
      "purgeOnDisconnection": "false"
    }
  org.onosproject.net.meter.impl.MeterManager: |
    {
      "purgeOnDisconnection": "false"
    }
  org.onosproject.provider.lldp.impl.LldpLinkProvider: |
    {
      "enabled": "false"
    }
  org.opencord.olt.impl.OltFlowService: |
    {
      "enableDhcpOnNni": "true",
      "defaultTechProfileId": "64",
      "enableIgmpOnNni": "false",
      "enableEapol": "true",
      "enableDhcpV6": "false",
      "enableDhcpV4": "true"
    }
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-onos-configs-loader
data:
  status_check.sh: |
    #!/bin/sh
    set -euo pipefail

    REPLICAS=$(kubectl -n default get --no-headers sts -lapp=onos-classic -o custom-columns=':.spec.replicas')
    RUNNING_POD=$(kubectl -n default get --no-headers pods -lapp=onos-classic | grep Running | wc -l)
    READY_POD=$(kubectl -n default get --no-headers pods -lapp=onos-classic -o custom-columns=':.status.containerStatuses[*].ready' | grep true | wc -l)
    if ! [ $REPLICAS -eq $RUNNING_POD ]; then
      echo "Not all ONOS Pods are running"
      exit 1
    fi
    if ! [ $REPLICAS -eq $READY_POD ]; then
      echo "Not all ONOS Pods are ready"
      exit 1
    fi
    exit 0
  loader.sh: |
    #/bin/sh set -euo pipefail
    USER="karaf" PASSWORD="karaf" ONOS_V1_URL="http://release-name-onos-classic-hs:8181/onos/v1" NETCFG_URL="${ONOS_V1_URL}/network/configuration/" COMPONENT_CFG_URL="${ONOS_V1_URL}/configuration" NETCFG="/opt/configs/netcfg.json" RECONCILE_MODE="false";
    while true; do
      echo "-----------------------------------------"
      echo "Timestamp is "; date
      echo "-----------------------------------------"
      echo 'Waiting for ONOS Cluster to be ready';
      if ! sh "/opt/loader/status_check.sh" ; then
        sleep 10s;
        continue
      fi

      echo "-----------------------------------------"
      echo "Loading netcfg into ONOS";
      echo "-----------------------------------------"
      cat ${NETCFG}
      # Upper all MAC addresses
      # ONOS will convert all MAC addresses to upper so we have to ensure all MAC addresses are upper in netcfg
      sed -E "s/([[:xdigit:]]{2}:)[[:xdigit:]]{2}/\U&/g" ${NETCFG} > expect_config
      curl -sSL -u $USER:$PASSWORD -X GET --header 'Accept: application/json' $NETCFG_URL | jq > actual_config
      # Update if there is difference between two configs(actual_config and expect_config)
      # Do the one-direction comparison
      # Everything in expect_config should be in the actual_config
      if jd actual_config expect_config | grep "^+"; then
          echo "Update netcfg due to the difference"
          curl -sSL -u $USER:$PASSWORD -X POST -H 'Content-Type: application/json' $NETCFG_URL -d@${NETCFG}
          responseCode=$(curl --write-out '%{http_code}' --fail -sSL -u $USER:$PASSWORD -X POST -H 'Content-Type: application/json' $NETCFG_URL -d@${NETCFG})
          if [[ $responseCode == 207 ]]; then
            echo "Failed to load NETCFG, skip this round and try again later"
          else
            echo "Updated netcfg is:";
            curl -sSL -u $USER:$PASSWORD -X GET --header 'Accept: application/json' $NETCFG_URL | jq
          fi
      else
        echo "Update is unnecessary since the netcfg on the ONOS cluster is up to date"
      fi

      echo "-----------------------------------------"
      echo "Loading component configs into ONOS";
      echo "-----------------------------------------"
      CFGS=$(ls /opt/configs | grep -v netcfg.json);
      for CFG in ${CFGS};
      do
        CFG_FILE="/opt/configs/$CFG"
        echo "-----------------------------------------"
        echo "Check that component $CFG is active";
        echo "-----------------------------------------"
        until curl --fail -sSL -u $USER:$PASSWORD -X GET "${COMPONENT_CFG_URL}/$CFG";
        do
          echo "Waiting for $CFG to be active";
          sleep 5;
        done

        echo "Prepare to update $CFG config"
        cat $CFG_FILE
        curl --fail -sSL -u $USER:$PASSWORD -X GET "${COMPONENT_CFG_URL}/$CFG"  | jq --arg key "${CFG}" '.[$key]' > actual_config
        # Update if there is difference between two configs(actual_config and expect_config)
        # Do the one-direction comparison
        # Everything in expect_config should be in the actual_config
        if jd actual_config ${CFG_FILE} | grep "^+"; then
          echo "Update $CFG config"
          responseCode=$(curl --write-out '%{http_code}' --fail -sSL -u $USER:$PASSWORD -X POST "${COMPONENT_CFG_URL}/$CFG" -H Content-type:application/json -d @${CFG_FILE});
          if [[ $responseCode == 207 ]]; then
            echo "Failed to load $CFG, skip this round and try again later"
          else
            echo "Updated Component config for $CFG is:";
            curl --fail -sSL -u $USER:$PASSWORD -X GET "${COMPONENT_CFG_URL}/$CFG";
          fi
        else
          echo "Update is unnecessary since the $CFG on the ONOS cluster is up to date"
        fi
      done

      if [ "${RECONCILE_MODE}" != "true" ]; then
        echo "Break from reconcile loop";
        break;
      fi

      echo "Completed on: ";
      date;
      sleep 30s
    done
    echo "For non-reconcile_mode, keep running to act as daemon;"
    sleep infinite
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: release-name-bbsim-sadis-server-pod-svc-reader
  namespace: default
rules:
  - apiGroups:
      - ""
    resources:
      - pods
      - services
    verbs:
      - get
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: release-name-etcd-defrag
  namespace: default
rules:
  - apiGroups:
      - ""
    resources:
      - pods
      - pods/exec
    verbs:
      - get
      - list
      - watch
      - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: release-name-bbsim-sadis-server-pod-svc-reader-binding
  namespace: default
subjects:
  - kind: ServiceAccount
    name: bbsim-sadis-server
    namespace: default
roleRef:
  kind: ClusterRole
  name: release-name-bbsim-sadis-server-pod-svc-reader
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: release-name-etcd-defrag
  namespace: default
subjects:
  - kind: ServiceAccount
    name: release-name-service-account
    namespace: default
roleRef:
  kind: ClusterRole
  name: release-name-etcd-defrag
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: list-pods
rules:
  - apiGroups:
      - ""
    resources:
      - pods
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - apps
    resources:
      - statefulsets
    verbs:
      - get
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: list-pods-to-sa
subjects:
  - kind: ServiceAccount
    name: onos-config-loader
roleRef:
  kind: Role
  name: list-pods
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: v1
kind: Service
metadata:
  name: bbsim-sadis-server
  namespace: 9oD12SZW
spec:
  ports:
    - name: http
      port: 58080
      targetPort: 8080
  selector:
    app: bbsim-sadis-server
    release: release-name
---
apiVersion: v1
kind: Service
metadata:
  name: release-name-etcd-headless
  namespace: 9oD12SZW
  labels:
    app.kubernetes.io/name: etcd
    helm.sh/chart: etcd-6.13.9
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: client
      port: 2379
      targetPort: client
    - name: peer
      port: 2380
      targetPort: peer
  selector:
    app.kubernetes.io/name: etcd
    app.kubernetes.io/instance: release-name
---
apiVersion: v1
kind: Service
metadata:
  name: release-name-etcd
  namespace: 9oD12SZW
  labels:
    app.kubernetes.io/name: etcd
    helm.sh/chart: etcd-6.13.9
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
  annotations: null
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: client
      port: 2379
      targetPort: client
      nodePort: null
    - name: peer
      port: 2380
      targetPort: peer
      nodePort: null
  selector:
    app.kubernetes.io/name: etcd
    app.kubernetes.io/instance: release-name
---
apiVersion: v1
kind: Service
metadata:
  name: release-name-freeradius
  namespace: 9oD12SZW
spec:
  ports:
    - name: radius-auth
      protocol: UDP
      port: 1812
      targetPort: 1812
    - name: radius-acc
      protocol: UDP
      port: 1813
      targetPort: 1813
    - name: radius
      port: 18120
      targetPort: 18120
  selector:
    app: radius
    release: release-name
---
apiVersion: v1
kind: Service
metadata:
  name: release-name-zookeeper-headless
  namespace: 9oD12SZW
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-7.5.1
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: tcp-client
      port: 2181
      targetPort: client
    - name: follower
      port: 2888
      targetPort: follower
    - name: tcp-election
      port: 3888
      targetPort: election
  selector:
    app.kubernetes.io/name: zookeeper
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: zookeeper
---
apiVersion: v1
kind: Service
metadata:
  name: release-name-zookeeper
  namespace: 9oD12SZW
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-7.5.1
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
spec:
  type: ClusterIP
  ports:
    - name: tcp-client
      port: 2181
      targetPort: client
      nodePort: null
    - name: follower
      port: 2888
      targetPort: follower
    - name: tcp-election
      port: 3888
      targetPort: election
  selector:
    app.kubernetes.io/name: zookeeper
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: zookeeper
---
apiVersion: v1
kind: Service
metadata:
  name: release-name-kafka-headless
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-14.9.0
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: kafka
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: tcp-client
      port: 9092
      protocol: TCP
      targetPort: kafka-client
    - name: tcp-internal
      port: 9093
      protocol: TCP
      targetPort: kafka-internal
  selector:
    app.kubernetes.io/name: kafka
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: kafka
---
apiVersion: v1
kind: Service
metadata:
  name: release-name-kafka
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-14.9.0
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: kafka
spec:
  type: ClusterIP
  ports:
    - name: tcp-client
      port: 9092
      protocol: TCP
      targetPort: kafka-client
      nodePort: null
  selector:
    app.kubernetes.io/name: kafka
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: kafka
---
apiVersion: v1
kind: Service
metadata:
  name: release-name-atomix-hs
  labels:
    app: release-name-atomix
    chart: atomix-0.1.9
    release: release-name
    heritage: Helm
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  ports:
    - name: release-name-atomix-node
      port: 5679
  publishNotReadyAddresses: true
  clusterIP: None
  selector:
    app: release-name-atomix
---
apiVersion: v1
kind: Service
metadata:
  name: release-name-atomix-api
  labels:
    app: release-name-atomix
    chart: atomix-0.1.9
    release: release-name
    heritage: Helm
spec:
  ports:
    - name: release-name-atomix-api
      port: 5678
  selector:
    app: release-name-atomix
---
apiVersion: v1
kind: Service
metadata:
  name: release-name-onos-classic-hs
  labels:
    app: onos-classic
    name: release-name-onos-classic
    chart: onos-classic-0.1.29
    release: release-name
    heritage: Helm
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  ports:
    - name: openflow
      port: 6653
      protocol: TCP
    - name: ovsdb
      port: 6640
      protocol: TCP
    - name: east-west
      port: 9876
      protocol: TCP
    - name: cli
      port: 8101
      protocol: TCP
    - name: ui
      port: 8181
      protocol: TCP
  publishNotReadyAddresses: true
  clusterIP: None
  selector:
    app: onos-classic
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: bbsim-sadis-server
  namespace: 9oD12SZW
spec:
  replicas: 1
  selector:
    matchLabels:
      app: bbsim-sadis-server
      release: release-name
  template:
    metadata:
      namespace: default
      labels:
        app: bbsim-sadis-server
        release: release-name
        app.kubernetes.io/name: bbsim-sadis-server
        app.kubernetes.io/version: 0.3.4
        app.kubernetes.io/component: sadis-server
        app.kubernetes.io/part-of: voltha
        app.kubernetes.io/managed-by: Helm
        helm.sh/chart: bbsim-sadis-server-0.3.1
    spec:
      serviceAccountName: bbsim-sadis-server
      containers:
        - name: sadis
          image: voltha/bbsim-sadis-server:0.3.4
          imagePullPolicy: Always
          command:
            - /app/bbsim-sadis-server
          args:
            - -log_level=WARN
            - -log_format=json
            - -bbsim_sadis_port=50074
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-freeradius
  namespace: 9oD12SZW
spec:
  replicas: 1
  selector:
    matchLabels:
      app: radius
      release: release-name
  template:
    metadata:
      labels:
        app: radius
        release: release-name
    spec:
      serviceAccountName: null
      containers:
        - name: radius
          image: freeradius/freeradius-server:3.0.21
          imagePullPolicy: Always
          env:
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: RADIUS_LISTEN_IP
              value: '*'
            - name: USERS_FILE
              value: /etc/raddb/users
            - name: RADIUS_CLIENTS
              value: SECRET@0.0.0.0/0
          ports:
            - containerPort: 1812
              name: radauth-port
            - containerPort: 1813
              name: radacc-port
            - containerPort: 18120
              name: radius-port
          volumeMounts:
            - name: freeradius-config
              mountPath: /etc/raddb/clients.conf
              subPath: clients.conf
            - name: freeradius-config
              mountPath: /etc/raddb/users
              subPath: users
      volumes:
        - name: freeradius-config
          configMap:
            name: freeradius-config
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-onos-classic-onos-config-loader
  labels:
    app: onos-config-loader
    chart: onos-classic
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: onos-config-loader
  template:
    metadata:
      labels:
        app: onos-config-loader
        release: release-name
      annotations:
        checksum/config: 92af3ee1113a26649809cfe03f3a8c3dea55d644fe38bbd7ba0a233f59472983
    spec:
      serviceAccountName: onos-config-loader
      containers:
        - name: onos-config-loader
          image: opencord/onos-classic-helm-utils:0.1.0
          imagePullPolicy: IfNotPresent
          command:
            - /bin/bash
            - /opt/loader/loader.sh
          volumeMounts:
            - name: onos-configs
              mountPath: /opt/configs
            - name: onos-loader
              mountPath: /opt/loader
          readinessProbe:
            exec:
              command:
                - sh
                - -c
                - /opt/loader/status_check.sh
            initialDelaySeconds: 20
            periodSeconds: 15
            failureThreshold: 1
            timeoutSeconds: 5
      volumes:
        - name: onos-configs
          configMap:
            name: release-name-onos-configs-data
        - name: onos-loader
          configMap:
            name: release-name-onos-configs-loader
            defaultMode: 511
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-etcd
  namespace: 9oD12SZW
  labels:
    app.kubernetes.io/name: etcd
    helm.sh/chart: etcd-6.13.9
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: etcd
      app.kubernetes.io/instance: release-name
  serviceName: release-name-etcd-headless
  podManagementPolicy: Parallel
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: etcd
        helm.sh/chart: etcd-6.13.9
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
      annotations:
        checksum/token-secret: 7f9de2b21fe3f12470a30828e6911a18b7cbee7376dfa773c516e7f1dcec7155
    spec:
      affinity:
        podAffinity: null
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: etcd
                    app.kubernetes.io/instance: release-name
                namespaces:
                  - default
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity: null
      securityContext:
        fsGroup: 1001
      serviceAccountName: default
      containers:
        - name: etcd
          image: docker.io/bitnami/etcd:3.5.2-debian-10-r52
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsNonRoot: true
            runAsUser: 10276
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                "": NET_RAW
            readOnlyRootFilesystem: true
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: ETCDCTL_API
              value: "3"
            - name: ETCD_ON_K8S
              value: "yes"
            - name: ETCD_START_FROM_SNAPSHOT
              value: "no"
            - name: ETCD_DISASTER_RECOVERY
              value: "no"
            - name: ETCD_NAME
              value: $(MY_POD_NAME)
            - name: ETCD_DATA_DIR
              value: /bitnami/etcd/data
            - name: ETCD_LOG_LEVEL
              value: info
            - name: ALLOW_NONE_AUTHENTICATION
              value: "yes"
            - name: ETCD_AUTH_TOKEN
              value: jwt,priv-key=/opt/bitnami/etcd/certs/token/jwt-token.pem,sign-method=RS256,ttl=10m
            - name: ETCD_ADVERTISE_CLIENT_URLS
              value: http://$(MY_POD_NAME).release-name-etcd-headless.default.svc.cluster.local:2379,http://release-name-etcd.default.svc.cluster.local:2379
            - name: ETCD_LISTEN_CLIENT_URLS
              value: http://0.0.0.0:2379
            - name: ETCD_INITIAL_ADVERTISE_PEER_URLS
              value: http://$(MY_POD_NAME).release-name-etcd-headless.default.svc.cluster.local:2380
            - name: ETCD_LISTEN_PEER_URLS
              value: http://0.0.0.0:2380
            - name: ETCD_AUTO_COMPACTION_MODE
              value: revision
            - name: ETCD_AUTO_COMPACTION_RETENTION
              value: "1"
            - name: ETCD_ELECTION_TIMEOUT
              value: "5000"
            - name: ETCD_HEARTBEAT_INTERVAL
              value: "1000"
          envFrom: null
          ports:
            - name: client
              containerPort: 2379
              protocol: TCP
            - name: peer
              containerPort: 2380
              protocol: TCP
          livenessProbe:
            exec:
              command:
                - /opt/bitnami/scripts/etcd/healthcheck.sh
            initialDelaySeconds: 60
            periodSeconds: 30
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
          readinessProbe:
            exec:
              command:
                - /opt/bitnami/scripts/etcd/healthcheck.sh
            initialDelaySeconds: 60
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
          resources:
            limits: {}
            requests: {}
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - name: data
              mountPath: /bitnami/etcd
            - name: etcd-jwt-token
              mountPath: /opt/bitnami/etcd/certs/token/
              readOnly: true
      volumes:
        - name: etcd-jwt-token
          secret:
            secretName: release-name-etcd-jwt-token
            defaultMode: 256
        - name: data
          emptyDir: {}
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-zookeeper
  namespace: 9oD12SZW
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-7.5.1
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
    role: zookeeper
spec:
  serviceName: release-name-zookeeper-headless
  replicas: 1
  podManagementPolicy: Parallel
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: zookeeper
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: zookeeper
  template:
    metadata:
      name: release-name-zookeeper
      labels:
        app.kubernetes.io/name: zookeeper
        helm.sh/chart: zookeeper-7.5.1
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: zookeeper
    spec:
      serviceAccountName: default
      securityContext:
        fsGroup: 1001
      affinity:
        podAffinity: null
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: zookeeper
                    app.kubernetes.io/instance: release-name
                    app.kubernetes.io/component: zookeeper
                namespaces:
                  - default
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity: null
      containers:
        - name: zookeeper
          image: docker.io/bitnami/zookeeper:3.7.0-debian-10-r215
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsNonRoot: true
            runAsUser: 11068
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                "": NET_RAW
            readOnlyRootFilesystem: true
          command:
            - bash
            - -ec
            - |
              # Execute entrypoint as usual after obtaining ZOO_SERVER_ID
              # check ZOO_SERVER_ID in persistent volume via myid
              # if not present, set based on POD hostname
              if [[ -f "/bitnami/zookeeper/data/myid" ]]; then
                export ZOO_SERVER_ID="$(cat /bitnami/zookeeper/data/myid)"
              else
                HOSTNAME=`hostname -s`
                if [[ $HOSTNAME =~ (.*)-([0-9]+)$ ]]; then
                  ORD=${BASH_REMATCH[2]}
                  export ZOO_SERVER_ID=$((ORD + 1 ))
                else
                  echo "Failed to get index from hostname $HOST"
                  exit 1
                fi
              fi
              exec /entrypoint.sh /run.sh
          resources:
            requests:
              cpu: 250m
              memory: 256Mi
            seccompProfile:
              type: RuntimeDefault
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: ZOO_DATA_LOG_DIR
              value: ""
            - name: ZOO_PORT_NUMBER
              value: "2181"
            - name: ZOO_TICK_TIME
              value: "2000"
            - name: ZOO_INIT_LIMIT
              value: "10"
            - name: ZOO_SYNC_LIMIT
              value: "5"
            - name: ZOO_PRE_ALLOC_SIZE
              value: "65536"
            - name: ZOO_SNAPCOUNT
              value: "100000"
            - name: ZOO_MAX_CLIENT_CNXNS
              value: "60"
            - name: ZOO_4LW_COMMANDS_WHITELIST
              value: srvr, mntr, ruok
            - name: ZOO_LISTEN_ALLIPS_ENABLED
              value: "no"
            - name: ZOO_AUTOPURGE_INTERVAL
              value: "0"
            - name: ZOO_AUTOPURGE_RETAIN_COUNT
              value: "3"
            - name: ZOO_MAX_SESSION_TIMEOUT
              value: "40000"
            - name: ZOO_SERVERS
              value: release-name-zookeeper-0.release-name-zookeeper-headless.default.svc.cluster.local:2888:3888::1
            - name: ZOO_ENABLE_AUTH
              value: "no"
            - name: ZOO_HEAP_SIZE
              value: "1024"
            - name: ZOO_LOG_LEVEL
              value: ERROR
            - name: ALLOW_ANONYMOUS_LOGIN
              value: "yes"
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.name
          ports:
            - name: client
              containerPort: 2181
            - name: follower
              containerPort: 2888
            - name: election
              containerPort: 3888
          livenessProbe:
            exec:
              command:
                - /bin/bash
                - -c
                - echo "ruok" | timeout 2 nc -w 2 localhost 2181 | grep imok
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          readinessProbe:
            exec:
              command:
                - /bin/bash
                - -c
                - echo "ruok" | timeout 2 nc -w 2 localhost 2181 | grep imok
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          volumeMounts:
            - name: data
              mountPath: /bitnami/zookeeper
      volumes:
        - name: data
          emptyDir: {}
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-kafka
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-14.9.0
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: kafka
spec:
  podManagementPolicy: Parallel
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: kafka
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: kafka
  serviceName: release-name-kafka-headless
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: kafka
        helm.sh/chart: kafka-14.9.0
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: kafka
    spec:
      affinity:
        podAffinity: null
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: kafka
                    app.kubernetes.io/instance: release-name
                    app.kubernetes.io/component: kafka
                namespaces:
                  - default
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity: null
      securityContext:
        fsGroup: 1001
        runAsUser: 1001
      serviceAccountName: release-name-kafka
      containers:
        - name: kafka
          image: docker.io/bitnami/kafka:2.8.1-debian-10-r73
          imagePullPolicy: IfNotPresent
          command:
            - /scripts/setup.sh
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: KAFKA_CFG_ZOOKEEPER_CONNECT
              value: release-name-zookeeper
            - name: KAFKA_INTER_BROKER_LISTENER_NAME
              value: INTERNAL
            - name: KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP
              value: INTERNAL:PLAINTEXT,CLIENT:PLAINTEXT
            - name: KAFKA_CFG_LISTENERS
              value: INTERNAL://:9093,CLIENT://:9092
            - name: KAFKA_CFG_ADVERTISED_LISTENERS
              value: INTERNAL://$(MY_POD_NAME).release-name-kafka-headless.default.svc.cluster.local:9093,CLIENT://$(MY_POD_NAME).release-name-kafka-headless.default.svc.cluster.local:9092
            - name: ALLOW_PLAINTEXT_LISTENER
              value: "yes"
            - name: KAFKA_VOLUME_DIR
              value: /bitnami/kafka
            - name: KAFKA_LOG_DIR
              value: /opt/bitnami/kafka/logs
            - name: KAFKA_CFG_DELETE_TOPIC_ENABLE
              value: "false"
            - name: KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE
              value: "true"
            - name: KAFKA_HEAP_OPTS
              value: -Xmx1024m -Xms1024m
            - name: KAFKA_CFG_LOG_FLUSH_INTERVAL_MESSAGES
              value: "10000"
            - name: KAFKA_CFG_LOG_FLUSH_INTERVAL_MS
              value: "1000"
            - name: KAFKA_CFG_LOG_RETENTION_BYTES
              value: "1073741824"
            - name: KAFKA_CFG_LOG_RETENTION_CHECK_INTERVALS_MS
              value: "300000"
            - name: KAFKA_CFG_LOG_RETENTION_HOURS
              value: "168"
            - name: KAFKA_CFG_MESSAGE_MAX_BYTES
              value: "1000012"
            - name: KAFKA_CFG_LOG_SEGMENT_BYTES
              value: "1073741824"
            - name: KAFKA_CFG_LOG_DIRS
              value: /bitnami/kafka/data
            - name: KAFKA_CFG_DEFAULT_REPLICATION_FACTOR
              value: "1"
            - name: KAFKA_CFG_OFFSETS_TOPIC_REPLICATION_FACTOR
              value: "1"
            - name: KAFKA_CFG_TRANSACTION_STATE_LOG_REPLICATION_FACTOR
              value: "1"
            - name: KAFKA_CFG_TRANSACTION_STATE_LOG_MIN_ISR
              value: "1"
            - name: KAFKA_CFG_NUM_IO_THREADS
              value: "8"
            - name: KAFKA_CFG_NUM_NETWORK_THREADS
              value: "3"
            - name: KAFKA_CFG_NUM_PARTITIONS
              value: "1"
            - name: KAFKA_CFG_NUM_RECOVERY_THREADS_PER_DATA_DIR
              value: "1"
            - name: KAFKA_CFG_SOCKET_RECEIVE_BUFFER_BYTES
              value: "102400"
            - name: KAFKA_CFG_SOCKET_REQUEST_MAX_BYTES
              value: "104857600"
            - name: KAFKA_CFG_SOCKET_SEND_BUFFER_BYTES
              value: "102400"
            - name: KAFKA_CFG_ZOOKEEPER_CONNECTION_TIMEOUT_MS
              value: "6000"
            - name: KAFKA_CFG_AUTHORIZER_CLASS_NAME
              value: ""
            - name: KAFKA_CFG_ALLOW_EVERYONE_IF_NO_ACL_FOUND
              value: "true"
            - name: KAFKA_CFG_SUPER_USERS
              value: User:admin
          ports:
            - name: kafka-client
              containerPort: 9092
            - name: kafka-internal
              containerPort: 9093
          livenessProbe:
            tcpSocket:
              port: kafka-client
            initialDelaySeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
          readinessProbe:
            tcpSocket:
              port: kafka-client
            initialDelaySeconds: 5
            timeoutSeconds: 5
            failureThreshold: 6
            periodSeconds: 10
            successThreshold: 1
          resources:
            limits: {}
            requests: {}
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - name: data
              mountPath: /bitnami/kafka
            - name: logs
              mountPath: /opt/bitnami/kafka/logs
            - name: scripts
              mountPath: /scripts/setup.sh
              subPath: setup.sh
      volumes:
        - name: scripts
          configMap:
            name: release-name-kafka-scripts
            defaultMode: 493
        - name: data
          emptyDir: {}
        - name: logs
          emptyDir: {}
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-atomix
  labels:
    app: release-name-atomix
    chart: atomix-0.1.9
    release: release-name
    heritage: Helm
spec:
  serviceName: release-name-atomix-hs
  selector:
    matchLabels:
      app: release-name-atomix
  replicas: 0
  updateStrategy:
    type: RollingUpdate
  podManagementPolicy: Parallel
  template:
    metadata:
      labels:
        app: release-name-atomix
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - release-name-atomix
              topologyKey: kubernetes.io/hostname
      initContainers:
        - name: configure
          image: ubuntu:16.04
          env:
            - name: ATOMIX_NODES
              value: "1"
          command:
            - sh
            - -c
            - /scripts/create-config.sh --nodes=$ATOMIX_NODES > /config/atomix.properties
          volumeMounts:
            - name: init-scripts
              mountPath: /scripts
            - name: system-config
              mountPath: /config
      containers:
        - name: atomix
          image: atomix/atomix:3.1.12
          imagePullPolicy: IfNotPresent
          env:
            - name: JAVA_OPTS
              value: -Xmx2G -XX:-UseContainerSupport
          resources:
            requests:
              memory: 512Mi
              cpu: 0.5
            limits: null
            seccompProfile:
              type: RuntimeDefault
          ports:
            - name: client
              containerPort: 5678
            - name: server
              containerPort: 5679
          args:
            - --config
            - /etc/atomix/system/atomix.properties
            - /etc/atomix/user/atomix.conf
            - --ignore-resources
            - --data-dir=/var/lib/atomix/data
            - --log-level=INFO
            - --file-log-level=OFF
            - --console-log-level=INFO
          readinessProbe:
            httpGet:
              path: /v1/status
              port: 5678
            initialDelaySeconds: 10
            timeoutSeconds: 10
            failureThreshold: 6
          livenessProbe:
            httpGet:
              path: /v1/status
              port: 5678
            initialDelaySeconds: 60
            timeoutSeconds: 10
          volumeMounts:
            - name: system-config
              mountPath: /etc/atomix/system
            - name: user-config
              mountPath: /etc/atomix/user
      volumes:
        - name: init-scripts
          configMap:
            name: release-name-atomix-init-scripts
            defaultMode: 484
        - name: user-config
          configMap:
            name: release-name-atomix-config
        - name: system-config
          emptyDir: {}
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-onos-classic
  labels:
    app: onos-classic
    name: release-name-onos-classic
    chart: onos-classic-0.1.29
    release: release-name
    heritage: Helm
spec:
  serviceName: release-name-onos-classic-hs
  selector:
    matchLabels:
      app: onos-classic
  replicas: 1
  updateStrategy:
    type: RollingUpdate
  podManagementPolicy: Parallel
  template:
    metadata:
      labels:
        app: onos-classic
        name: release-name-onos-classic
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - onos-classic
              topologyKey: kubernetes.io/hostname
      initContainers:
        - name: onos-classic-init
          image: tutum/dnsutils:latest
          imagePullPolicy: IfNotPresent
          env:
            - name: ATOMIX_SERVICE
              value: release-name-atomix
            - name: ATOMIX_REPLICAS
              value: "0"
          command:
            - sh
            - -c
            - /scripts/configure-onos.sh $ATOMIX_SERVICE $ATOMIX_REPLICAS > /config/cluster.json && touch /config/active
          volumeMounts:
            - name: init-scripts
              mountPath: /scripts
            - name: config
              mountPath: /config
      containers:
        - name: onos-classic
          image: voltha/voltha-onos:5.0.5
          imagePullPolicy: Always
          resources:
            requests:
              memory: 512Mi
              cpu: 0.5
            limits: null
            seccompProfile:
              type: RuntimeDefault
          env:
            - name: JAVA_OPTS
              value: -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:-UseContainerSupport -Dkaraf.log.console=INFO -Dds.lock.timeout.milliseconds=10000 -Dlog4j2.formatMsgNoLookups=true
            - name: ONOS_APPS
              value: org.onosproject.lldpprovider,org.onosproject.openflow-base,org.onosproject.gui2,org.onosproject.drivers,org.onosproject.mcast,org.onosproject.segmentrouting,org.opencord.kafka,org.opencord.sadis,org.opencord.dhcpl2relay,org.opencord.igmpproxy,org.opencord.mcast,org.opencord.olt,org.opencord.aaa
          ports:
            - name: openflow
              containerPort: 6653
            - name: ovsdb
              containerPort: 6640
            - name: east-west
              containerPort: 9876
            - name: cli
              containerPort: 8101
            - name: ui
              containerPort: 8181
          readinessProbe:
            exec:
              command:
                - sh
                - -c
                - /root/onos/bin/check-onos-status
            initialDelaySeconds: 30
            periodSeconds: 15
            failureThreshold: 10
            timeoutSeconds: 10
          livenessProbe:
            exec:
              command:
                - sh
                - -c
                - /root/onos/bin/check-onos-status
            initialDelaySeconds: 300
            periodSeconds: 15
            timeoutSeconds: 10
          volumeMounts:
            - name: probe-scripts
              mountPath: /root/onos/bin/check-onos-status
              subPath: check-onos-status
            - name: config
              mountPath: /root/onos/config/cluster.json
              subPath: cluster.json
      volumes:
        - name: init-scripts
          configMap:
            name: release-name-onos-classic-init-scripts
            defaultMode: 484
        - name: probe-scripts
          configMap:
            name: release-name-onos-classic-probe-scripts
            defaultMode: 484
        - name: config
          emptyDir: {}
---
null
---
null
---
null
---
null
---
null
---
null
---
null
---
null
---
null
---
null
---
null
---
null
---
null
---
null
---
null
---
null
---
null
