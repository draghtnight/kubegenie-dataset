apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-kibana
  namespace: zLKEla
  labels:
    app.kubernetes.io/name: kibana
    helm.sh/chart: kibana-9.3.17
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-kafka
  namespace: zLKEla
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-16.2.6
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: kafka
  annotations: null
automountServiceAccountToken: true
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-logstash
  namespace: zLKEla
  labels:
    app.kubernetes.io/name: logstash
    helm.sh/chart: logstash-3.8.8
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
automountServiceAccountToken: true
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-spark
  namespace: zLKEla
  labels:
    app.kubernetes.io/name: spark
    helm.sh/chart: spark-5.9.11
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
  annotations: null
automountServiceAccountToken: true
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-dataplatform-bp2
  labels:
    app.kubernetes.io/name: dataplatform-bp2
    helm.sh/chart: dataplatform-bp2-12.0.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: dataplatform
automountServiceAccountToken: true
---
apiVersion: v1
kind: Secret
metadata:
  name: release-name-spark-secret
  labels:
    app.kubernetes.io/name: spark
    helm.sh/chart: spark-5.9.11
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
type: Opaque
data: null
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-kibana-conf
  labels:
    app.kubernetes.io/name: kibana
    helm.sh/chart: kibana-9.3.17
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
data:
  kibana.yml: |
    pid.file: /opt/bitnami/kibana/tmp/kibana.pid
    server.host: "::"
    server.port: 5601
    elasticsearch.hosts: [http://release-name-coordinating-only:9200]
    server.rewriteBasePath: false
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-zookeeper-scripts
  namespace: zLKEla
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-9.0.6
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
data:
  init-certs.sh: '#!/bin/bash'
  setup.sh: |-
    #!/bin/bash

    # Execute entrypoint as usual after obtaining ZOO_SERVER_ID
    # check ZOO_SERVER_ID in persistent volume via myid
    # if not present, set based on POD hostname
    if [[ -f "/bitnami/zookeeper/data/myid" ]]; then
        export ZOO_SERVER_ID="$(cat /bitnami/zookeeper/data/myid)"
    else
        HOSTNAME="$(hostname -s)"
        if [[ $HOSTNAME =~ (.*)-([0-9]+)$ ]]; then
            ORD=${BASH_REMATCH[2]}
            export ZOO_SERVER_ID="$((ORD + 1 ))"
        else
            echo "Failed to get index from hostname $HOST"
            exit 1
        fi
    fi
    exec /entrypoint.sh /run.sh
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-kafka-scripts
  namespace: zLKEla
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-16.2.6
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
data:
  setup.sh: |-
    #!/bin/bash

    ID="${MY_POD_NAME#"release-name-kafka-"}"
    if [[ -f "/bitnami/kafka/data/meta.properties" ]]; then
        export KAFKA_CFG_BROKER_ID="$(grep "broker.id" /bitnami/kafka/data/meta.properties | awk -F '=' '{print $2}')"
    else
        export KAFKA_CFG_BROKER_ID="$((ID + 0))"
    fi

    exec /entrypoint.sh /run.sh
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-logstash
  labels:
    app.kubernetes.io/name: logstash
    helm.sh/chart: logstash-3.8.8
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
data:
  logstash.conf: |-
    input {
      # udp {
      #   port => 1514
      #   type => syslog
      # }
      # tcp {
      #   port => 1514
      #   type => syslog
      # }
      http { port => 8080 }
    }
    output {
      # elasticsearch {
      #   hosts => ["${ELASTICSEARCH_HOST}:${ELASTICSEARCH_PORT}"]
      #   manage_template => false
      #   index => "%{[@metadata][beat]}-%{+YYYY.MM.dd}"
      # }
      # gelf {
      #   host => "${GRAYLOG_HOST}"
      #   port => ${GRAYLOG_PORT}
      # }
      stdout {}
    }
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-dataplatform-bp2-exporter-configuration
  labels:
    app.kubernetes.io/name: dataplatform-bp2
    helm.sh/chart: dataplatform-bp2-12.0.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
data:
  bp.json: |-
    {
      "blueprintName": "bp2",
      "metrics": [
        {
          "name": "zookeeper_desired_nodes",
          "type": "gauge",
          "helpMessage": "Desired number of zookeeper nodes in the data platform",
          "key": "zookeeper",
          "dataComponent": "DesiredNodes"
        },
        {
          "name": "zookeeper_available_nodes",
          "type": "gauge",
          "helpMessage": "Available number of zookeeper nodes in the data platform",
          "key": "zookeeper",
          "dataComponent": "AvailableNodes"
        },
        {
          "name": "kafka_desired_nodes",
          "type": "gauge",
          "helpMessage": "Desired number of kafka nodes in the data platform",
          "key": "kafka",
          "dataComponent": "DesiredNodes"
        },
        {
          "name": "kafka_available_nodes",
          "type": "gauge",
          "helpMessage": "Available number of kafka nodes in the data platform",
          "key": "kafka",
          "dataComponent": "AvailableNodes"
        },
        {
          "name": "elasticsearch_master_desired_nodes",
          "type": "gauge",
          "helpMessage": "Desired number of elasticsearch master nodes in the data platform",
          "key": "elasticsearch-master",
          "dataComponent": "DesiredNodes"
        },
        {
          "name": "elasticsearch_master_available_nodes",
          "type": "gauge",
          "helpMessage": "Available number of elasticsearch master nodes in the data platform",
          "key": "elasticsearch-master",
          "dataComponent": "AvailableNodes"
        },
        {
          "name": "elasticsearch_data_desired_nodes",
          "type": "gauge",
          "helpMessage": "Desired number of elasticsearch data nodes in the data platform",
          "key": "elasticsearch-data",
          "dataComponent": "DesiredNodes"
        },
        {
          "name": "elasticsearch_data_available_nodes",
          "type": "gauge",
          "helpMessage": "Available number of elasticsearch data nodes in the data platform",
          "key": "elasticsearch-data",
          "dataComponent": "AvailableNodes"
        },
        {
          "name": "spark_master_desired_nodes",
          "type": "gauge",
          "helpMessage": "Desired number of spark master nodes in the data platform",
          "key": "spark-master",
          "dataComponent": "DesiredNodes"
        },
        {
          "name": "spark_master_available_nodes",
          "type": "gauge",
          "helpMessage": "Available number of spark master nodes in the data platform",
          "key": "spark-master",
          "dataComponent": "AvailableNodes"
        },
        {
          "name": "spark_worker_desired_nodes",
          "type": "gauge",
          "helpMessage": "Desired number of spark worker nodes in the data platform",
          "key": "spark-worker",
          "dataComponent": "DesiredNodes"
        },
        {
          "name": "spark_worker_available_nodes",
          "type": "gauge",
          "helpMessage": "Available number of spark worker nodes in the data platform",
          "key": "spark-worker",
          "dataComponent": "AvailableNodes"
        },
        {
          "name": "logstash_desired_nodes",
          "type": "gauge",
          "helpMessage": "Desired number of logstash nodes in the data platform",
          "key": "logstash",
          "dataComponent": "DesiredNodes"
        },
        {
          "name": "logstash_available_nodes",
          "type": "gauge",
          "helpMessage": "Available number of logstash nodes in the data platform",
          "key": "logstash",
          "dataComponent": "AvailableNodes"
        }
      ]
    }
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: release-name-kibana
  labels:
    app.kubernetes.io/name: kibana
    helm.sh/chart: kibana-9.3.17
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: release-name-dataplatform-bp2
  labels:
    app.kubernetes.io/name: dataplatform-bp2
    helm.sh/chart: dataplatform-bp2-12.0.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: dataplatform
rules:
  - apiGroups:
      - ""
    resources:
      - statefulsets
      - pods
      - services
      - secrets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - namespaces
      - namespaces/status
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - apps
    resources:
      - controllerrevisions
      - daemonsets
      - daemonsets/status
      - deployments
      - deployments/scale
      - deployments/status
      - replicasets
      - replicasets/scale
      - replicasets/status
      - statefulsets
      - statefulsets/scale
      - statefulsets/status
    verbs:
      - get
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: release-name-dataplatform-bp2
  labels:
    app.kubernetes.io/name: dataplatform-bp2
    helm.sh/chart: dataplatform-bp2-12.0.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: dataplatform
roleRef:
  kind: Role
  name: release-name-dataplatform-bp2
  apiGroup: rbac.authorization.k8s.io
subjects:
  - kind: ServiceAccount
    name: release-name-dataplatform-bp2
    namespace: default
---
apiVersion: v1
kind: Service
metadata:
  name: release-name-kibana
  labels:
    app.kubernetes.io/name: kibana
    helm.sh/chart: kibana-9.3.17
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 5601
      targetPort: http
      nodePort: null
  selector:
    app.kubernetes.io/name: kibana
    app.kubernetes.io/instance: release-name
---
apiVersion: v1
kind: Service
metadata:
  name: release-name-coordinating-only
  labels:
    app.kubernetes.io/name: elasticsearch
    helm.sh/chart: elasticsearch-17.9.28
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: coordinating-only
  annotations: {}
spec:
  type: ClusterIP
  publishNotReadyAddresses: true
  ports:
    - name: http
      port: 9200
      targetPort: http
      nodePort: null
    - name: tcp-transport
      port: 9300
  selector:
    app.kubernetes.io/name: elasticsearch
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: coordinating-only
---
apiVersion: v1
kind: Service
metadata:
  name: release-name-elasticsearch-data
  labels:
    app.kubernetes.io/name: elasticsearch
    helm.sh/chart: elasticsearch-17.9.28
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: data
  annotations: {}
spec:
  type: ClusterIP
  publishNotReadyAddresses: true
  ports:
    - name: http
      port: 9200
      targetPort: http
    - name: tcp-transport
      port: 9300
      targetPort: transport
      nodePort: null
  selector:
    app.kubernetes.io/name: elasticsearch
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: data
---
apiVersion: v1
kind: Service
metadata:
  name: release-name-elasticsearch-master
  labels:
    app.kubernetes.io/name: elasticsearch
    helm.sh/chart: elasticsearch-17.9.28
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: master
  annotations: {}
spec:
  type: ClusterIP
  publishNotReadyAddresses: true
  ports:
    - name: http
      port: 9200
      targetPort: http
    - name: tcp-transport
      port: 9300
      targetPort: transport
      nodePort: null
  selector:
    app.kubernetes.io/name: elasticsearch
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: master
---
apiVersion: v1
kind: Service
metadata:
  name: release-name-zookeeper-headless
  namespace: zLKEla
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-9.0.6
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: tcp-client
      port: 2181
      targetPort: client
    - name: tcp-follower
      port: 2888
      targetPort: follower
    - name: tcp-election
      port: 3888
      targetPort: election
  selector:
    app.kubernetes.io/name: zookeeper
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: zookeeper
---
apiVersion: v1
kind: Service
metadata:
  name: release-name-zookeeper
  namespace: zLKEla
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-9.0.6
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-client
      port: 2181
      targetPort: client
      nodePort: null
    - name: tcp-follower
      port: 2888
      targetPort: follower
    - name: tcp-election
      port: 3888
      targetPort: election
  selector:
    app.kubernetes.io/name: zookeeper
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: zookeeper
---
apiVersion: v1
kind: Service
metadata:
  name: release-name-kafka-headless
  namespace: zLKEla
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-16.2.6
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: kafka
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: tcp-client
      port: 9092
      protocol: TCP
      targetPort: kafka-client
    - name: tcp-internal
      port: 9093
      protocol: TCP
      targetPort: kafka-internal
  selector:
    app.kubernetes.io/name: kafka
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: kafka
---
apiVersion: v1
kind: Service
metadata:
  name: release-name-kafka
  namespace: zLKEla
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-16.2.6
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: kafka
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-client
      port: 9092
      protocol: TCP
      targetPort: kafka-client
      nodePort: null
  selector:
    app.kubernetes.io/name: kafka
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: kafka
---
apiVersion: v1
kind: Service
metadata:
  name: release-name-logstash-headless
  labels:
    app.kubernetes.io/name: logstash
    helm.sh/chart: logstash-3.8.8
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: http
      port: 8080
      protocol: TCP
      targetPort: http
  selector:
    app.kubernetes.io/name: logstash
    app.kubernetes.io/instance: release-name
---
apiVersion: v1
kind: Service
metadata:
  name: release-name-logstash
  labels:
    app.kubernetes.io/name: logstash
    helm.sh/chart: logstash-3.8.8
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 8080
      protocol: TCP
      targetPort: http
  selector:
    app.kubernetes.io/name: logstash
    app.kubernetes.io/instance: release-name
---
apiVersion: v1
kind: Service
metadata:
  name: release-name-spark-headless
  labels:
    app.kubernetes.io/name: spark
    helm.sh/chart: spark-5.9.11
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  clusterIP: None
  selector:
    app.kubernetes.io/name: spark
    app.kubernetes.io/instance: release-name
---
apiVersion: v1
kind: Service
metadata:
  name: release-name-spark-master-svc
  labels:
    app.kubernetes.io/name: spark
    helm.sh/chart: spark-5.9.11
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
  annotations: null
spec:
  type: ClusterIP
  ports:
    - port: 7077
      targetPort: cluster
      name: cluster
      nodePort: null
    - port: 80
      targetPort: http
      name: http
      protocol: TCP
      nodePort: null
  selector:
    app.kubernetes.io/name: spark
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: master
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/name: dataplatform-bp2
    helm.sh/chart: dataplatform-bp2-12.0.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: dataplatform-emitter
  name: release-name-dataplatform-bp2-emitter
  namespace: zLKEla
spec:
  type: ClusterIP
  ports:
    - name: tcp-client
      port: 8091
      protocol: TCP
      targetPort: emitter-port
      nodePort: null
  selector:
    app.kubernetes.io/name: dataplatform-bp2
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: dataplatform-emitter
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/name: dataplatform-bp2
    helm.sh/chart: dataplatform-bp2-12.0.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: dataplatform-exporter
  name: release-name-dataplatform-bp2-exporter
  annotations:
    prometheus.io/path: /metrics
    prometheus.io/port: "9090"
    prometheus.io/scrape: "true"
  namespace: zLKEla
spec:
  type: ClusterIP
  ports:
    - name: tcp-client
      port: 9090
      protocol: TCP
      targetPort: exporter-port
      nodePort: null
  selector:
    app.kubernetes.io/name: dataplatform-bp2
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: dataplatform-exporter
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-kibana
  labels:
    app.kubernetes.io/name: kibana
    helm.sh/chart: kibana-9.3.17
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: kibana
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      labels:
        app.kubernetes.io/name: kibana
        helm.sh/chart: kibana-9.3.17
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app: kibana
    spec:
      affinity:
        podAffinity: null
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: kibana
                    app.kubernetes.io/instance: release-name
                namespaces:
                  - default
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity: null
      serviceAccountName: release-name-kibana
      securityContext:
        fsGroup: 1001
      containers:
        - name: kibana
          image: docker.io/bitnami/kibana:7.17.2-debian-10-r3
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 11703
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                "": NET_RAW
            readOnlyRootFilesystem: true
          env:
            - name: KIBANA_PORT_NUMBER
              value: "5601"
            - name: KIBANA_ELASTICSEARCH_URL
              value: http://release-name-coordinating-only:9200
            - name: KIBANA_ELASTICSEARCH_PORT_NUMBER
              value: "9200"
            - name: KIBANA_FORCE_INITSCRIPTS
              value: "false"
            - name: KIBANA_SERVER_ENABLE_TLS
              value: "false"
            - name: KIBANA_ELASTICSEARCH_ENABLE_TLS
              value: "false"
            - name: KIBANA_ELASTICSEARCH_TLS_USE_PEM
              value: "false"
            - name: KIBANA_ELASTICSEARCH_TLS_VERIFICATION_MODE
              value: full
          ports:
            - name: http
              containerPort: 5601
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /login
              port: http
              scheme: HTTP
            initialDelaySeconds: 120
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          readinessProbe:
            httpGet:
              path: /status
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          resources:
            limits: {}
            requests: {}
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - name: kibana-data
              mountPath: /bitnami/kibana
            - name: kibana-config
              mountPath: /bitnami/kibana/conf
      volumes:
        - name: kibana-data
          persistentVolumeClaim:
            claimName: release-name-kibana
        - name: kibana-config
          configMap:
            name: release-name-kibana-conf
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/name: dataplatform-bp2
    helm.sh/chart: dataplatform-bp2-12.0.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: dataplatform-emitter
  name: release-name-dataplatform-bp2-emitter
  namespace: zLKEla
spec:
  replicas: 1
  strategy:
    rollingUpdate: {}
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: dataplatform-bp2
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: dataplatform-emitter
  template:
    metadata:
      annotations: null
      labels:
        app.kubernetes.io/name: dataplatform-bp2
        helm.sh/chart: dataplatform-bp2-12.0.3
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: dataplatform-emitter
    spec:
      serviceAccountName: release-name-dataplatform-bp2
      affinity:
        podAffinity: null
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: dataplatform-bp2
                    app.kubernetes.io/instance: release-name
                    app.kubernetes.io/component: dataplatform-emitter
                namespaces:
                  - default
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity: null
      securityContext:
        fsGroup: 1001
      initContainers: null
      containers:
        - name: dataplatform-emitter
          image: docker.io/bitnami/dataplatform-emitter:1.0.1-scratch-r31
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsNonRoot: true
            runAsUser: 10116
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                "": NET_RAW
            readOnlyRootFilesystem: true
          env:
            - name: BP_NAME
              value: release-name-dataplatform-bp2
            - name: BP_RELEASE_NAME
              value: release-name
            - name: BP_NAMESPACE
              value: default
          ports:
            - name: emitter-port
              containerPort: 8091
          resources:
            limits: {}
            requests: {}
            seccompProfile:
              type: RuntimeDefault
          livenessProbe:
            httpGet:
              path: /v1/health
              port: 8091
            initialDelaySeconds: 10
            periodSeconds: 5
            timeoutSeconds: 15
            failureThreshold: 15
            successThreshold: 1
          readinessProbe:
            httpGet:
              path: /v1/health
              port: 8091
            initialDelaySeconds: 10
            periodSeconds: 5
            timeoutSeconds: 15
            failureThreshold: 15
            successThreshold: 15
          volumeMounts: null
      volumes: null
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/name: dataplatform-bp2
    helm.sh/chart: dataplatform-bp2-12.0.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: dataplatform-exporter
  name: release-name-dataplatform-bp2-exporter
  namespace: zLKEla
spec:
  replicas: 1
  strategy:
    rollingUpdate: {}
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: dataplatform-bp2
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: dataplatform-exporter
  template:
    metadata:
      annotations: null
      labels:
        app.kubernetes.io/name: dataplatform-bp2
        helm.sh/chart: dataplatform-bp2-12.0.3
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: dataplatform-exporter
    spec:
      serviceAccountName: release-name-dataplatform-bp2
      affinity:
        podAffinity: null
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: dataplatform-bp2
                    app.kubernetes.io/instance: release-name
                    app.kubernetes.io/component: dataplatform-exporter
                namespaces:
                  - default
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity: null
      securityContext:
        fsGroup: 1001
      initContainers: null
      containers:
        - name: dataplatform-exporter
          image: docker.io/bitnami/dataplatform-exporter:1.0.1-scratch-r26
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsNonRoot: true
            runAsUser: 11235
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                "": NET_RAW
            readOnlyRootFilesystem: true
          env:
            - name: METRIC_CONFIG_PATH
              value: /data/bp.json
            - name: DP_URI
              value: http://release-name-dataplatform-bp2-emitter:8091
          ports:
            - name: exporter-port
              containerPort: 9090
          resources:
            limits: {}
            requests: {}
            seccompProfile:
              type: RuntimeDefault
          livenessProbe:
            httpGet:
              path: /metrics
              port: 9090
            initialDelaySeconds: 10
            periodSeconds: 5
            timeoutSeconds: 15
            failureThreshold: 15
            successThreshold: 1
          readinessProbe:
            httpGet:
              path: /metrics
              port: 9090
            initialDelaySeconds: 10
            periodSeconds: 5
            timeoutSeconds: 15
            failureThreshold: 15
            successThreshold: 15
          volumeMounts:
            - name: exporter-config
              mountPath: /data/bp.json
              subPath: bp.json
      volumes:
        - name: exporter-config
          configMap:
            name: release-name-dataplatform-bp2-exporter-configuration
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-coordinating-only
  labels:
    app.kubernetes.io/name: elasticsearch
    helm.sh/chart: elasticsearch-17.9.28
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: coordinating-only
    app: coordinating-only
spec:
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: elasticsearch
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: coordinating-only
  podManagementPolicy: Parallel
  replicas: 2
  serviceName: release-name-coordinating-only
  template:
    metadata:
      labels:
        app.kubernetes.io/name: elasticsearch
        helm.sh/chart: elasticsearch-17.9.28
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: coordinating-only
        app: coordinating-only
      annotations: null
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app.kubernetes.io/component
                    operator: In
                    values:
                      - coordinating-only
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                      - elasticsearch
                  - key: app.kubernetes.io/instance
                    operator: In
                    values:
                      - release-name
              topologyKey: kubernetes.io/hostname
      serviceAccountName: default
      securityContext:
        fsGroup: 1001
      initContainers:
        - name: sysctl
          image: docker.io/bitnami/bitnami-shell:10-debian-10-r402
          imagePullPolicy: IfNotPresent
          command:
            - /bin/bash
            - -ec
            - |
              CURRENT=`sysctl -n vm.max_map_count`;
              DESIRED="262144";
              if [ "$DESIRED" -gt "$CURRENT" ]; then
                  sysctl -w vm.max_map_count=262144;
              fi;
              CURRENT=`sysctl -n fs.file-max`;
              DESIRED="65536";
              if [ "$DESIRED" -gt "$CURRENT" ]; then
                  sysctl -w fs.file-max=65536;
              fi;
          securityContext:
            privileged: true
          resources:
            limits: {}
            requests: {}
      containers:
        - name: elasticsearch
          image: docker.io/bitnami/elasticsearch:7.17.2-debian-10-r20
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 11217
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                "": NET_RAW
            readOnlyRootFilesystem: true
          env:
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: BITNAMI_DEBUG
              value: "false"
            - name: ELASTICSEARCH_CLUSTER_NAME
              value: elastic
            - name: ELASTICSEARCH_CLUSTER_HOSTS
              value: release-name-elasticsearch-master.default.svc.cluster.local,release-name-coordinating-only.default.svc.cluster.local,release-name-elasticsearch-data.default.svc.cluster.local,
            - name: ELASTICSEARCH_TOTAL_NODES
              value: "5"
            - name: ELASTICSEARCH_CLUSTER_MASTER_HOSTS
              value: release-name-elasticsearch-master-0 release-name-elasticsearch-master-1 release-name-elasticsearch-master-2
            - name: ELASTICSEARCH_MINIMUM_MASTER_NODES
              value: "2"
            - name: ELASTICSEARCH_ADVERTISED_HOSTNAME
              value: $(MY_POD_NAME).release-name-coordinating-only.default.svc.cluster.local
            - name: ELASTICSEARCH_HEAP_SIZE
              value: 768m
            - name: ELASTICSEARCH_IS_DEDICATED_NODE
              value: "yes"
            - name: ELASTICSEARCH_NODE_TYPE
              value: coordinating
          ports:
            - name: http
              containerPort: 9200
            - name: transport
              containerPort: 9300
          livenessProbe:
            initialDelaySeconds: 90
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - /opt/bitnami/scripts/elasticsearch/healthcheck.sh
          readinessProbe:
            initialDelaySeconds: 90
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - /opt/bitnami/scripts/elasticsearch/healthcheck.sh
          resources:
            limits: {}
            requests:
              cpu: 250m
              memory: 1Gi
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - name: data
              mountPath: /bitnami/elasticsearch/data
      volumes:
        - name: data
          emptyDir: {}
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-elasticsearch-data
  labels:
    app.kubernetes.io/name: elasticsearch
    helm.sh/chart: elasticsearch-17.9.28
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: data
    app: data
spec:
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: elasticsearch
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: data
  podManagementPolicy: Parallel
  replicas: 2
  serviceName: release-name-elasticsearch-data
  template:
    metadata:
      labels:
        app.kubernetes.io/name: elasticsearch
        helm.sh/chart: elasticsearch-17.9.28
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: data
        app: data
      annotations: null
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app.kubernetes.io/component
                    operator: In
                    values:
                      - data
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                      - elasticsearch
                  - key: app.kubernetes.io/instance
                    operator: In
                    values:
                      - release-name
              topologyKey: kubernetes.io/hostname
      serviceAccountName: default
      securityContext:
        fsGroup: 1001
      initContainers:
        - name: sysctl
          image: docker.io/bitnami/bitnami-shell:10-debian-10-r402
          imagePullPolicy: IfNotPresent
          command:
            - /bin/bash
            - -ec
            - |
              CURRENT=`sysctl -n vm.max_map_count`;
              DESIRED="262144";
              if [ "$DESIRED" -gt "$CURRENT" ]; then
                  sysctl -w vm.max_map_count=262144;
              fi;
              CURRENT=`sysctl -n fs.file-max`;
              DESIRED="65536";
              if [ "$DESIRED" -gt "$CURRENT" ]; then
                  sysctl -w fs.file-max=65536;
              fi;
          securityContext:
            privileged: true
          resources:
            limits: {}
            requests: {}
      containers:
        - name: elasticsearch
          image: docker.io/bitnami/elasticsearch:7.17.2-debian-10-r20
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 10025
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                "": NET_RAW
            readOnlyRootFilesystem: true
          env:
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: BITNAMI_DEBUG
              value: "false"
            - name: ELASTICSEARCH_CLUSTER_NAME
              value: elastic
            - name: ELASTICSEARCH_CLUSTER_HOSTS
              value: release-name-elasticsearch-master.default.svc.cluster.local,release-name-coordinating-only.default.svc.cluster.local,release-name-elasticsearch-data.default.svc.cluster.local,
            - name: ELASTICSEARCH_TOTAL_NODES
              value: "5"
            - name: ELASTICSEARCH_HEAP_SIZE
              value: 4096m
            - name: ELASTICSEARCH_IS_DEDICATED_NODE
              value: "yes"
            - name: ELASTICSEARCH_NODE_TYPE
              value: data
            - name: ELASTICSEARCH_ADVERTISED_HOSTNAME
              value: $(MY_POD_NAME).release-name-elasticsearch-data.default.svc.cluster.local
          ports:
            - name: http
              containerPort: 9200
            - name: transport
              containerPort: 9300
          livenessProbe:
            initialDelaySeconds: 90
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - /opt/bitnami/scripts/elasticsearch/healthcheck.sh
          readinessProbe:
            initialDelaySeconds: 90
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - /opt/bitnami/scripts/elasticsearch/healthcheck.sh
          resources:
            limits: {}
            requests:
              cpu: 250m
              memory: 5Gi
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - name: data
              mountPath: /bitnami/elasticsearch/data
      volumes: null
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 8Gi
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-elasticsearch-master
  labels:
    app.kubernetes.io/name: elasticsearch
    helm.sh/chart: elasticsearch-17.9.28
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: master
    app: master
spec:
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: elasticsearch
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: master
  podManagementPolicy: Parallel
  replicas: 3
  serviceName: release-name-elasticsearch-master
  template:
    metadata:
      labels:
        app.kubernetes.io/name: elasticsearch
        helm.sh/chart: elasticsearch-17.9.28
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: master
        app: master
      annotations: null
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app.kubernetes.io/component
                    operator: In
                    values:
                      - master
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                      - elasticsearch
                  - key: app.kubernetes.io/instance
                    operator: In
                    values:
                      - release-name
              topologyKey: kubernetes.io/hostname
      serviceAccountName: default
      securityContext:
        fsGroup: 1001
      initContainers:
        - name: sysctl
          image: docker.io/bitnami/bitnami-shell:10-debian-10-r402
          imagePullPolicy: IfNotPresent
          command:
            - /bin/bash
            - -ec
            - |
              CURRENT=`sysctl -n vm.max_map_count`;
              DESIRED="262144";
              if [ "$DESIRED" -gt "$CURRENT" ]; then
                  sysctl -w vm.max_map_count=262144;
              fi;
              CURRENT=`sysctl -n fs.file-max`;
              DESIRED="65536";
              if [ "$DESIRED" -gt "$CURRENT" ]; then
                  sysctl -w fs.file-max=65536;
              fi;
          securityContext:
            privileged: true
          resources:
            limits: {}
            requests: {}
      containers:
        - name: elasticsearch
          image: docker.io/bitnami/elasticsearch:7.17.2-debian-10-r20
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 10109
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                "": NET_RAW
            readOnlyRootFilesystem: true
          env:
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: BITNAMI_DEBUG
              value: "false"
            - name: ELASTICSEARCH_CLUSTER_NAME
              value: elastic
            - name: ELASTICSEARCH_CLUSTER_HOSTS
              value: release-name-elasticsearch-master.default.svc.cluster.local,release-name-coordinating-only.default.svc.cluster.local,release-name-elasticsearch-data.default.svc.cluster.local,
            - name: ELASTICSEARCH_TOTAL_NODES
              value: "5"
            - name: ELASTICSEARCH_CLUSTER_MASTER_HOSTS
              value: release-name-elasticsearch-master-0 release-name-elasticsearch-master-1 release-name-elasticsearch-master-2
            - name: ELASTICSEARCH_MINIMUM_MASTER_NODES
              value: "2"
            - name: ELASTICSEARCH_ADVERTISED_HOSTNAME
              value: $(MY_POD_NAME).release-name-elasticsearch-master.default.svc.cluster.local
            - name: ELASTICSEARCH_HEAP_SIZE
              value: 768m
            - name: ELASTICSEARCH_IS_DEDICATED_NODE
              value: "yes"
            - name: ELASTICSEARCH_NODE_TYPE
              value: master
          ports:
            - name: http
              containerPort: 9200
            - name: transport
              containerPort: 9300
          livenessProbe:
            initialDelaySeconds: 90
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - /opt/bitnami/scripts/elasticsearch/healthcheck.sh
          readinessProbe:
            initialDelaySeconds: 90
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - /opt/bitnami/scripts/elasticsearch/healthcheck.sh
          resources:
            limits: {}
            requests:
              cpu: 250m
              memory: 1Gi
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - name: data
              mountPath: /bitnami/elasticsearch/data
      volumes: null
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 8Gi
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-zookeeper
  namespace: zLKEla
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-9.0.6
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
    role: zookeeper
spec:
  replicas: 3
  podManagementPolicy: Parallel
  selector:
    matchLabels:
      app.kubernetes.io/name: zookeeper
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: zookeeper
  serviceName: release-name-zookeeper-headless
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  template:
    metadata:
      annotations: null
      labels:
        app.kubernetes.io/name: zookeeper
        helm.sh/chart: zookeeper-9.0.6
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: zookeeper
    spec:
      serviceAccountName: default
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                      - zookeeper
                  - key: app.kubernetes.io/instance
                    operator: In
                    values:
                      - release-name
              topologyKey: kubernetes.io/hostname
      securityContext:
        fsGroup: 1001
      initContainers: null
      containers:
        - name: zookeeper
          image: docker.io/bitnami/zookeeper:3.8.0-debian-10-r34
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsNonRoot: true
            runAsUser: 10966
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                "": NET_RAW
            readOnlyRootFilesystem: true
          command:
            - /scripts/setup.sh
          resources:
            limits: {}
            requests:
              cpu: 250m
              memory: 5Gi
            seccompProfile:
              type: RuntimeDefault
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: ZOO_DATA_LOG_DIR
              value: ""
            - name: ZOO_PORT_NUMBER
              value: "2181"
            - name: ZOO_TICK_TIME
              value: "2000"
            - name: ZOO_INIT_LIMIT
              value: "10"
            - name: ZOO_SYNC_LIMIT
              value: "5"
            - name: ZOO_PRE_ALLOC_SIZE
              value: "65536"
            - name: ZOO_SNAPCOUNT
              value: "100000"
            - name: ZOO_MAX_CLIENT_CNXNS
              value: "60"
            - name: ZOO_4LW_COMMANDS_WHITELIST
              value: srvr, mntr, ruok
            - name: ZOO_LISTEN_ALLIPS_ENABLED
              value: "no"
            - name: ZOO_AUTOPURGE_INTERVAL
              value: "0"
            - name: ZOO_AUTOPURGE_RETAIN_COUNT
              value: "3"
            - name: ZOO_MAX_SESSION_TIMEOUT
              value: "40000"
            - name: ZOO_SERVERS
              value: release-name-zookeeper-0.release-name-zookeeper-headless.default.svc.cluster.local:2888:3888::1 release-name-zookeeper-1.release-name-zookeeper-headless.default.svc.cluster.local:2888:3888::2 release-name-zookeeper-2.release-name-zookeeper-headless.default.svc.cluster.local:2888:3888::3
            - name: ZOO_ENABLE_AUTH
              value: "no"
            - name: ZOO_HEAP_SIZE
              value: "4096"
            - name: ZOO_LOG_LEVEL
              value: ERROR
            - name: ALLOW_ANONYMOUS_LOGIN
              value: "yes"
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.name
          ports:
            - name: client
              containerPort: 2181
            - name: follower
              containerPort: 2888
            - name: election
              containerPort: 3888
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /bin/bash
                - -c
                - echo "ruok" | timeout 2 nc -w 2 localhost 2181 | grep imok
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /bin/bash
                - -c
                - echo "ruok" | timeout 2 nc -w 2 localhost 2181 | grep imok
          volumeMounts:
            - name: scripts
              mountPath: /scripts/setup.sh
              subPath: setup.sh
            - name: data
              mountPath: /bitnami/zookeeper
      volumes:
        - name: scripts
          configMap:
            name: release-name-zookeeper-scripts
            defaultMode: 493
  volumeClaimTemplates:
    - metadata:
        name: data
        annotations: null
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 8Gi
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-kafka
  namespace: zLKEla
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-16.2.6
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: kafka
spec:
  podManagementPolicy: Parallel
  replicas: 3
  selector:
    matchLabels:
      app.kubernetes.io/name: kafka
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: kafka
  serviceName: release-name-kafka-headless
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: kafka
        helm.sh/chart: kafka-16.2.6
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: kafka
      annotations: null
    spec:
      hostNetwork: false
      hostIPC: false
      affinity:
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app.kubernetes.io/name
                      operator: In
                      values:
                        - zookeeper
                    - key: app.kubernetes.io/instance
                      operator: In
                      values:
                        - release-name
                topologyKey: kubernetes.io/hostname
              weight: 50
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app.kubernetes.io/component
                    operator: In
                    values:
                      - kafka
                  - key: app.kubernetes.io/instance
                    operator: In
                    values:
                      - release-name
              topologyKey: kubernetes.io/hostname
      securityContext:
        fsGroup: 1001
      serviceAccountName: release-name-kafka
      containers:
        - name: kafka
          image: docker.io/bitnami/kafka:3.1.0-debian-10-r84
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsNonRoot: true
            runAsUser: 10851
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                "": NET_RAW
            readOnlyRootFilesystem: true
          command:
            - /scripts/setup.sh
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: KAFKA_CFG_ZOOKEEPER_CONNECT
              value: release-name-zookeeper
            - name: KAFKA_INTER_BROKER_LISTENER_NAME
              value: INTERNAL
            - name: KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP
              value: INTERNAL:PLAINTEXT,CLIENT:PLAINTEXT
            - name: KAFKA_CFG_LISTENERS
              value: INTERNAL://:9093,CLIENT://:9092
            - name: KAFKA_CFG_ADVERTISED_LISTENERS
              value: INTERNAL://$(MY_POD_NAME).release-name-kafka-headless.default.svc.cluster.local:9093,CLIENT://$(MY_POD_NAME).release-name-kafka-headless.default.svc.cluster.local:9092
            - name: ALLOW_PLAINTEXT_LISTENER
              value: "yes"
            - name: KAFKA_VOLUME_DIR
              value: /bitnami/kafka
            - name: KAFKA_LOG_DIR
              value: /opt/bitnami/kafka/logs
            - name: KAFKA_CFG_DELETE_TOPIC_ENABLE
              value: "false"
            - name: KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE
              value: "true"
            - name: KAFKA_HEAP_OPTS
              value: -Xmx4096m -Xms4096m
            - name: KAFKA_CFG_LOG_FLUSH_INTERVAL_MESSAGES
              value: "10000"
            - name: KAFKA_CFG_LOG_FLUSH_INTERVAL_MS
              value: "1000"
            - name: KAFKA_CFG_LOG_RETENTION_BYTES
              value: "1073741824"
            - name: KAFKA_CFG_LOG_RETENTION_CHECK_INTERVALS_MS
              value: "300000"
            - name: KAFKA_CFG_LOG_RETENTION_HOURS
              value: "168"
            - name: KAFKA_CFG_MESSAGE_MAX_BYTES
              value: "1000012"
            - name: KAFKA_CFG_LOG_SEGMENT_BYTES
              value: "1073741824"
            - name: KAFKA_CFG_LOG_DIRS
              value: /bitnami/kafka/data
            - name: KAFKA_CFG_DEFAULT_REPLICATION_FACTOR
              value: "1"
            - name: KAFKA_CFG_OFFSETS_TOPIC_REPLICATION_FACTOR
              value: "1"
            - name: KAFKA_CFG_TRANSACTION_STATE_LOG_REPLICATION_FACTOR
              value: "1"
            - name: KAFKA_CFG_TRANSACTION_STATE_LOG_MIN_ISR
              value: "1"
            - name: KAFKA_CFG_NUM_IO_THREADS
              value: "8"
            - name: KAFKA_CFG_NUM_NETWORK_THREADS
              value: "3"
            - name: KAFKA_CFG_NUM_PARTITIONS
              value: "1"
            - name: KAFKA_CFG_NUM_RECOVERY_THREADS_PER_DATA_DIR
              value: "1"
            - name: KAFKA_CFG_SOCKET_RECEIVE_BUFFER_BYTES
              value: "102400"
            - name: KAFKA_CFG_SOCKET_REQUEST_MAX_BYTES
              value: "104857600"
            - name: KAFKA_CFG_SOCKET_SEND_BUFFER_BYTES
              value: "102400"
            - name: KAFKA_CFG_ZOOKEEPER_CONNECTION_TIMEOUT_MS
              value: "6000"
            - name: KAFKA_CFG_AUTHORIZER_CLASS_NAME
              value: ""
            - name: KAFKA_CFG_ALLOW_EVERYONE_IF_NO_ACL_FOUND
              value: "true"
            - name: KAFKA_CFG_SUPER_USERS
              value: User:admin
          ports:
            - name: kafka-client
              containerPort: 9092
            - name: kafka-internal
              containerPort: 9093
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            tcpSocket:
              port: kafka-client
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            tcpSocket:
              port: kafka-client
          resources:
            limits: {}
            requests:
              cpu: 250m
              memory: 5120Mi
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - name: data
              mountPath: /bitnami/kafka
            - name: logs
              mountPath: /opt/bitnami/kafka/logs
            - name: scripts
              mountPath: /scripts/setup.sh
              subPath: setup.sh
      volumes:
        - name: scripts
          configMap:
            name: release-name-kafka-scripts
            defaultMode: 493
        - name: logs
          emptyDir: {}
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 8Gi
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-logstash
  labels:
    app.kubernetes.io/name: logstash
    helm.sh/chart: logstash-3.8.8
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  serviceName: release-name-logstash-headless
  replicas: 2
  podManagementPolicy: OrderedReady
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: logstash
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      labels:
        app.kubernetes.io/name: logstash
        helm.sh/chart: logstash-3.8.8
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
      annotations:
        checksum/configuration: 5011455b6931eda3c4a8a6a1f5ff9a9e9305753fcaf324d45c1f67c0c7589a02
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                      - logstash
                  - key: app.kubernetes.io/instance
                    operator: In
                    values:
                      - release-name
              topologyKey: kubernetes.io/hostname
      securityContext:
        runAsUser: 1001
        fsGroup: 1001
      serviceAccountName: release-name-logstash
      containers:
        - name: logstash
          image: docker.io/bitnami/logstash:7.17.2-debian-10-r5
          imagePullPolicy: IfNotPresent
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: LOGSTASH_CONF_FILENAME
              value: logstash.conf
            - name: LOGSTASH_ENABLE_MULTIPLE_PIPELINES
              value: "false"
            - name: LOGSTASH_EXPOSE_API
              value: "yes"
            - name: LOGSTASH_API_PORT_NUMBER
              value: "9600"
            - name: LS_JAVA_OPTS
              value: -Xmx1g -Xms1g
          envFrom: null
          ports:
            - containerPort: 8080
              name: http
              protocol: TCP
            - containerPort: 9600
              name: monitoring
              protocol: TCP
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /
              port: monitoring
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          readinessProbe:
            failureThreshold: 6
            httpGet:
              path: /
              port: monitoring
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits: {}
            requests:
              cpu: 250m
              memory: 1500Mi
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - name: configurations
              mountPath: /bitnami/logstash/config
      volumes:
        - name: configurations
          configMap:
            name: release-name-logstash
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-spark-master
  labels:
    app.kubernetes.io/name: spark
    helm.sh/chart: spark-5.9.11
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: master
spec:
  serviceName: release-name-spark-headless
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: spark
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: master
  template:
    metadata:
      labels:
        app.kubernetes.io/name: spark
        helm.sh/chart: spark-5.9.11
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: master
    spec:
      serviceAccountName: release-name-spark
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app.kubernetes.io/component
                    operator: In
                    values:
                      - worker
                  - key: app.kubernetes.io/instance
                    operator: In
                    values:
                      - release-name
              topologyKey: kubernetes.io/hostname
      securityContext:
        fsGroup: 1001
        runAsUser: 1001
        runAsGroup: 0
      containers:
        - name: spark-master
          image: docker.io/bitnami/spark:3.2.1-debian-10-r78
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
            - name: cluster
              containerPort: 7077
          volumeMounts: null
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: SPARK_MODE
              value: master
            - name: SPARK_DAEMON_MEMORY
              value: ""
            - name: SPARK_MASTER_PORT
              value: "7077"
            - name: SPARK_MASTER_WEBUI_PORT
              value: "8080"
          livenessProbe:
            httpGet:
              path: /
              port: 8080
            initialDelaySeconds: 180
            periodSeconds: 20
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          readinessProbe:
            httpGet:
              path: /
              port: 8080
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          resources:
            limits: {}
            requests:
              cpu: 250m
              memory: 5Gi
            seccompProfile:
              type: RuntimeDefault
      volumes: null
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-spark-worker
  labels:
    app.kubernetes.io/name: spark
    helm.sh/chart: spark-5.9.11
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: worker
spec:
  serviceName: release-name-spark-headless
  replicas: 2
  podManagementPolicy: OrderedReady
  selector:
    matchLabels:
      app.kubernetes.io/name: spark
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: worker
  template:
    metadata:
      labels:
        app.kubernetes.io/name: spark
        helm.sh/chart: spark-5.9.11
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: worker
    spec:
      serviceAccountName: release-name-spark
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app.kubernetes.io/component
                    operator: In
                    values:
                      - worker
                      - master
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                      - spark
                  - key: app.kubernetes.io/instance
                    operator: In
                    values:
                      - release-name
              topologyKey: kubernetes.io/hostname
      securityContext:
        fsGroup: 1001
        runAsUser: 1001
        runAsGroup: 0
      containers:
        - name: spark-worker
          image: docker.io/bitnami/spark:3.2.1-debian-10-r78
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 8081
              protocol: TCP
          volumeMounts: null
          env:
            - name: SPARK_MODE
              value: worker
            - name: BITNAMI_DEBUG
              value: "false"
            - name: SPARK_DAEMON_MEMORY
              value: ""
            - name: SPARK_WORKER_WEBUI_PORT
              value: "8081"
            - name: SPARK_DAEMON_JAVA_OPTS
              value: ""
            - name: SPARK_MASTER_URL
              value: spark://release-name-spark-master-svc:7077
            - name: SPARK_WORKER_OPTS
              value: null
          livenessProbe:
            httpGet:
              path: /
              port: 8081
            initialDelaySeconds: 180
            periodSeconds: 20
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          readinessProbe:
            httpGet:
              path: /
              port: 8081
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          resources:
            limits: {}
            requests:
              cpu: 250m
              memory: 5Gi
            seccompProfile:
              type: RuntimeDefault
      volumes: null
