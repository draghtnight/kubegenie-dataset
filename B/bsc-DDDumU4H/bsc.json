[
  {
    "apiVersion": "v1",
    "kind": "ServiceAccount",
    "metadata": {
      "name": "release-name-bsc",
      "labels": {
        "app.kubernetes.io/name": "bsc",
        "helm.sh/chart": "bsc-0.6.10",
        "app.kubernetes.io/instance": "release-name",
        "app.kubernetes.io/version": "v1.1.8",
        "app.kubernetes.io/managed-by": "Helm"
      }
    }
  },
  {
    "apiVersion": "v1",
    "kind": "ConfigMap",
    "metadata": {
      "name": "release-name-config",
      "labels": {
        "app.kubernetes.io/name": "bsc",
        "helm.sh/chart": "bsc-0.6.10",
        "app.kubernetes.io/instance": "release-name",
        "app.kubernetes.io/version": "v1.1.8",
        "app.kubernetes.io/managed-by": "Helm"
      }
    },
    "data": {
      "config.toml": "[Eth]\nNetworkId = 56\nNoPruning = false\nNoPrefetch = false\nLightPeers = 100\nUltraLightFraction = 75\nTrieTimeout = 100000000000\nEnablePreimageRecording = false\nEWASMInterpreter = \"\"\nEVMInterpreter = \"\"\nDisablePeerTxBroadcast = true\n\n[Eth.Miner]\nGasFloor = 30000000\nGasCeil = 40000000\nGasPrice = 1000000000\nRecommit = 10000000000\nNoverify = false\n\n[Eth.TxPool]\nLocals = []\nNoLocals = true\nJournal = \"transactions.rlp\"\nRejournal = 3600000000000\nPriceLimit = 1000000000\nPriceBump = 10\nAccountSlots = 512\nGlobalSlots = 10000\nAccountQueue = 256\nGlobalQueue = 5000\nLifetime = 10800000000000\n\n[Eth.GPO]\nBlocks = 20\nPercentile = 60\nOracleThreshold = 20\n\n[Node]\nIPCPath = \"geth.ipc\"\nHTTPHost = \"0.0.0.0\"\nNoUSB = true\nInsecureUnlockAllowed = false\nHTTPPort = 8575\nHTTPVirtualHosts = [\"*\"]\nHTTPModules = [\"eth\", \"net\", \"web3\", \"txpool\", \"parlia\"]\nWSPort = 8576\nWSOrigins = [\"*\"]\nWSModules = [\"net\", \"web3\", \"eth\"]\n\n[Node.HTTPTimeouts]\nReadTimeout = 30000000000\nWriteTimeout = 30000000000\nIdleTimeout = 120000000000\n\n[Node.LogConfig]\nFilePath = \"bsc.log\"\nMaxBytesSize = 10485760\nLevel = \"info\"\nFileRoot = \"\"\n\n# keep this section the last one, as we may append trusted nodes via config generation\n[Node.P2P]\nEnableMsgEvents = false\nMaxPeers = 50\nNoDiscovery = false\nListenAddr = \":30311\"\nBootstrapNodes = [\"enode://1cc4534b14cfe351ab740a1418ab944a234ca2f702915eadb7e558a02010cb7c5a8c295a3b56bcefa7701c07752acd5539cb13df2aab8ae2d98934d712611443@52.71.43.172:30311\",\"enode://28b1d16562dac280dacaaf45d54516b85bc6c994252a9825c5cc4e080d3e53446d05f63ba495ea7d44d6c316b54cd92b245c5c328c37da24605c4a93a0d099c4@34.246.65.14:30311\",\"enode://5a7b996048d1b0a07683a949662c87c09b55247ce774aeee10bb886892e586e3c604564393292e38ef43c023ee9981e1f8b335766ec4f0f256e57f8640b079d5@35.73.137.11:30311\"]\nStaticNodes = [\"enode://f3cfd69f2808ef64838abd8786342c0b22fdd28268703c8d6812e26e109f9a7cb2b37bd49724ebb46c233289f22da82991c87345eb9a2dadeddb8f37eeb259ac@18.180.28.21:30311\",\"enode://ae74385270d4afeb953561603fcedc4a0e755a241ffdea31c3f751dc8be5bf29c03bf46e3051d1c8d997c45479a92632020c9a84b96dcb63b2259ec09b4fde38@54.178.30.104:30311\",\"enode://d1cabe083d5fc1da9b510889188f06dab891935294e4569df759fc2c4d684b3b4982051b84a9a078512202ad947f9240adc5b6abea5320fb9a736d2f6751c52e@54.238.28.14:30311\",\"enode://f420209bac5324326c116d38d83edfa2256c4101a27cd3e7f9b8287dc8526900f4137e915df6806986b28bc79b1e66679b544a1c515a95ede86f4d809bd65dab@54.178.62.117:30311\",\"enode://c0e8d1abd27c3c13ca879e16f34c12ffee936a7e5d7b7fb6f1af5cc75c6fad704e5667c7bbf7826fcb200d22b9bf86395271b0f76c21e63ad9a388ed548d4c90@54.65.247.12:30311\",\"enode://f1b49b1cf536e36f9a56730f7a0ece899e5efb344eec2fdca3a335465bc4f619b98121f4a5032a1218fa8b69a5488d1ec48afe2abda073280beec296b104db31@13.114.199.41:30311\",\"enode://4924583cfb262b6e333969c86eab8da009b3f7d165cc9ad326914f576c575741e71dc6e64a830e833c25e8c45b906364e58e70cdf043651fd583082ea7db5e3b@18.180.17.171:30311\",\"enode://4d041250eb4f05ab55af184a01aed1a71d241a94a03a5b86f4e32659e1ab1e144be919890682d4afb5e7afd837146ce584d61a38837553d95a7de1f28ea4513a@54.178.99.222:30311\",\"enode://b5772a14fdaeebf4c1924e73c923bdf11c35240a6da7b9e5ec0e6cbb95e78327690b90e8ab0ea5270debc8834454b98eca34cc2a19817f5972498648a6959a3a@54.170.158.102:30311\",\"enode://f329176b187cec87b327f82e78b6ece3102a0f7c89b92a5312e1674062c6e89f785f55fb1b167e369d71c66b0548994c6035c6d85849eccb434d4d9e0c489cdd@34.253.94.130:30311\",\"enode://cbfd1219940d4e312ad94108e7fa3bc34c4c22081d6f334a2e7b36bb28928b56879924cf0353ad85fa5b2f3d5033bbe8ad5371feae9c2088214184be301ed658@54.75.11.3:30311\",\"enode://c64b0a0c619c03c220ea0d7cac754931f967665f9e148b92d2e46761ad9180f5eb5aaef48dfc230d8db8f8c16d2265a3d5407b06bedcd5f0f5a22c2f51c2e69f@54.216.208.163:30311\",\"enode://352a361a9240d4d23bb6fab19cc6dc5a5fc6921abf19de65afe13f1802780aecd67c8c09d8c89043ff86947f171d98ab06906ef616d58e718067e02abea0dda9@79.125.105.65:30311\",\"enode://bb683ef5d03db7d945d6f84b88e5b98920b70aecc22abed8c00d6db621f784e4280e5813d12694c7a091543064456ad9789980766f3f1feb38906cf7255c33d6@54.195.127.237:30311\",\"enode://11dc6fea50630b68a9289055d6b0fb0e22fb5048a3f4e4efd741a7ab09dd79e78d383efc052089e516f0a0f3eacdd5d3ffbe5279b36ecc42ad7cd1f2767fdbdb@46.137.182.25:30311\",\"enode://21530e423b42aed17d7eef67882ebb23357db4f8b10c94d4c71191f52955d97dc13eec03cfeff0fe3a1c89c955e81a6970c09689d21ecbec2142b26b7e759c45@54.216.119.18:30311\",\"enode://d61a31410c365e7fcd50e24d56a77d2d9741d4a57b295cc5070189ad90d0ec749d113b4b0432c6d795eb36597efce88d12ca45e645ec51b3a2144e1c1c41b66a@34.204.129.242:30311\",\"enode://bb91215b1d77c892897048dd58f709f02aacb5355aa8f50f00b67c879c3dffd7eef5b5a152ac46cdfb255295bec4d06701a8032456703c6b604a4686d388ea8f@75.101.197.198:30311\",\"enode://786acbdf5a3cf91b99047a0fd8305e11e54d96ea3a72b1527050d3d6f8c9fc0278ff9ef56f3e56b3b70a283d97c309065506ea2fc3eb9b62477fd014a3ec1a96@107.23.90.162:30311\",\"enode://4653bc7c235c3480968e5e81d91123bc67626f35c207ae4acab89347db675a627784c5982431300c02f547a7d33558718f7795e848d547a327abb111eac73636@54.144.170.236:30311\",\"enode://c6ffd994c4ef130f90f8ee2fc08c1b0f02a6e9b12152092bf5a03dd7af9fd33597d4b2e2000a271cc0648d5e55242aeadd6d5061bb2e596372655ba0722cc704@54.147.151.108:30311\",\"enode://99b07e9dc5f204263b87243146743399b2bd60c98f68d1239a3461d09087e6c417e40f1106fa606ccf54159feabdddb4e7f367559b349a6511e66e525de4906e@54.81.225.170:30311\",\"enode://1479af5ea7bda822e8747d0b967309bced22cad5083b93bc6f4e1d7da7be067cd8495dc4c5a71579f2da8d9068f0c43ad6933d2b335a545b4ae49a846122b261@52.7.247.132:30311\",\"enode://43562d35f274d9e93f5ccac484c7cb185eabc746dbc9f3a56c36dc5a9ef05a3282695de7694a71c0bf4600651f49395b2ee7a6aaef857db2ac896e0fcbe6b518@35.73.15.198:30311\",\"enode://08867e57849456fc9b0b00771f53e87ca6f2dd618c23b34a35d0c851cd484a4b7137905c5b357795025b368e4f8fe4c841b752b0c28cc2dbbf41a03d048e0e24@35.74.39.234:30311\"]"
    }
  },
  {
    "apiVersion": "v1",
    "kind": "ConfigMap",
    "metadata": {
      "name": "release-name-env",
      "labels": {
        "app.kubernetes.io/name": "bsc",
        "helm.sh/chart": "bsc-0.6.10",
        "app.kubernetes.io/instance": "release-name",
        "app.kubernetes.io/version": "v1.1.8",
        "app.kubernetes.io/managed-by": "Helm"
      }
    },
    "data": {
      "SNAPSHOT_URL_123": "rsync://bsc-bootnode.bsc:1873/bsc"
    }
  },
  {
    "apiVersion": "v1",
    "kind": "ConfigMap",
    "metadata": {
      "name": "release-name-probe-env",
      "labels": {
        "app.kubernetes.io/name": "bsc",
        "helm.sh/chart": "bsc-0.6.10",
        "app.kubernetes.io/instance": "release-name",
        "app.kubernetes.io/version": "v1.1.8",
        "app.kubernetes.io/managed-by": "Helm"
      }
    },
    "data": {
      "env.txt": "ReadinessProbeTimestampDistinct=300\nLivenessProbeTimestampDistinct=300\nStartupProbeTimestampDistinct=300"
    }
  },
  {
    "apiVersion": "v1",
    "kind": "ConfigMap",
    "metadata": {
      "name": "release-name-scripts",
      "labels": {
        "app.kubernetes.io/name": "bsc",
        "helm.sh/chart": "bsc-0.6.10",
        "app.kubernetes.io/instance": "release-name",
        "app.kubernetes.io/version": "v1.1.8",
        "app.kubernetes.io/managed-by": "Helm"
      }
    },
    "data": {
      "check_node_health.sh": "#!/usr/bin/env sh\nset -ex # -e exits on error\n\nusage() { echo \"Usage: $0 <rpc_endpoint> <max_lag_in_seconds> <last_synced_block_file>]\" 1>&2; exit 1; }\n\nrpc_endpoint=\"$1\"\nmax_lag_in_seconds=\"$2\"\nlast_synced_block_file=\"$3\"\n\nif [ -z \"${rpc_endpoint}\" ] || [ -z \"${max_lag_in_seconds}\" ] || [ -z \"${last_synced_block_file}\" ]; then\n    usage\nfi\n\nblock_number=$(geth --datadir=/data attach --exec \"eth.blockNumber\")\n\nif [ -z \"${block_number}\" ] || [ \"${block_number}\" == \"null\" ]; then\n    echo \"Block number returned by the node is empty or null\"\n    exit 1\nfi\n\nif [ ! -f ${last_synced_block_file} ]; then\n    old_block_number=\"\";\nelse\n    old_block_number=$(cat ${last_synced_block_file});\nfi;\n\nif [ \"${block_number}\" != \"${old_block_number}\" ]; then\n  mkdir -p $(dirname \"${last_synced_block_file}\")\n  echo ${block_number} > ${last_synced_block_file}\nfi\n\nfile_age=$(($(date +%s) - $(date -r ${last_synced_block_file} +%s)));\nmax_age=${max_lag_in_seconds};\necho \"${last_synced_block_file} age is $file_age seconds. Max healthy age is $max_age seconds\";\nif [ ${file_age} -lt ${max_age} ]; then exit 0; else exit 1; fi",
      "check_node_readiness.sh": "#!/usr/bin/env sh\nset -ex\n\npublic_bsc_node=$3\nallowed_number_of_distinct_between_blocks=$2\nallowed_number_of_time_gap_between_blocks=$2\nlocal_node_endpoint=${4:-}\n\n# Retrieving latest block timestamp from public bsc node\nfunction get_public_block {\n    geth --datadir=/tmp attach $public_bsc_node --exec \"eth.blockNumber\" || exit 0\n}\n\n# Retrieving latest local block number\nfunction get_local_block {\n    geth --config=/config/config.toml --datadir=/data attach $local_node_endpoint --exec \"eth.blockNumber\"\n}\n\n# Retrieving latest block timestamp of a local bsc node\nfunction get_local_timestamp {\n    geth --config=/config/config.toml --datadir=/data attach $local_node_endpoint --exec \"eth.getBlock(eth.blockNumber).timestamp\"\n}\n\ncase \"$1\" in\n        # If public latest block number differs with local latest block for more than 10 blocks => fail, otherwise okay.\n        --distinct-blocks)\n            if [[ $(expr  $(get_public_block) - $(get_local_block)) -le $allowed_number_of_distinct_between_blocks ]]\n            then\n              echo \"Current block gap is lower that $allowed_number_of_distinct_between_blocks\"\n              exit 0\n            else\n              echo \"Current block gap is higher that $allowed_number_of_distinct_between_blocks.\"\n              exit 1\n            fi\n            ;;\n        # If local latest block's timestamp lower than current timestamp for more than 600 seconds (10 minutes)\n        --timestamp-distinct)\n            if [[ $(expr $(date +%s) - $(get_local_timestamp)) -le $allowed_number_of_time_gap_between_blocks ]]\n            then\n              echo  \"Current timestamp gap is lower that $allowed_number_of_time_gap_between_blocks\"\n              exit 0\n            else\n              echo  \"Current timestamp gap is higher that $allowed_number_of_time_gap_between_blocks\"\n              exit 1\n            fi\n            ;;\n        *)\n            echo \"Usage: $0 {--distinct-blocks|--timestamp-distinct} {blocks-distinct,|time-range-distinct-seconds} {public-bsc-node-endpoint}\n                  Blocks check:\n                          $0 --distinct-blocks 10 https://bsc-dataseed1.binance.org\n                  Timestamp check:\n                          $0 --timestamp-distinct 300\"\n            exit 1\nesac",
      "get_nodekey.py": "#!/usr/bin/env python\nimport os\nimport re\nimport sys\n\nKEYS = ['']\n\ndef get_nodekey():\n    node_id = re.search(r'^.*-(\\d+)$', os.uname()[1]).group(1)\n    return KEYS[int(node_id)]\n\nif __name__ == '__main__':\n    nodekey = get_nodekey()\n    with open(\"/data/geth/nodekey\", \"w\") as f:\n        sys.stdout.write(\"Node key: {}\".format(nodekey))\n        f.write(nodekey)",
      "get_nodekey_ip.py": "#!/usr/bin/env python\nimport os\nimport sys\nimport ipaddress\nimport json\n\nnodeKeysFileName = \"/generated-config/nodekeys\"\n\ndef get_nodekey(nodeKeysFileName):\n    addr=ipaddress.ip_address(os.environ['MY_POD_IP'])\n    net=ipaddress.ip_network(\"192.168.0.0/20\")\n    if addr in net:\n      node_id=int(addr)-int(net[0])\n    else:\n      sys.stdout.write(\"Pod address \"+str(addr)+\" in not inside network \"+str(net))\n      sys.exit(1)\n    with open(nodeKeysFileName, \"r\") as f:\n      KEYS=json.load(f)\n      return KEYS[node_id]\n\nif __name__ == '__main__':\n    nodekey = get_nodekey(nodeKeysFileName)\n    with open(\"/data/geth/nodekey\", \"w\") as f:\n        sys.stdout.write(\"Node key: {}\".format(nodekey))\n        f.write(nodekey)",
      "generate_node_config.sh": "#!/usr/bin/env sh\n\nset -ex # -e exits on error\nSRC_DIR=/config\nDST_DIR=/generated-config\nCONFIG_NAME=config.toml\nTRUSTED_NODES_SRC_URL=gs://bucket/trusted_nodes\nNODEKEYS_SRC_URL=gs://bucket/nodekeys\nNODEKEYS=nodekeys\nTRUSTED_NODES=trusted_nodes\n\n\n\n# check if we really need to generate config\nif [ \"${GENERATE_CONFIG}\" != \"true\" ];then\n  echo \"Config generation disabled, copying instead\"\n  cp -f \"${SRC_DIR}/${CONFIG_NAME}\" \"${DST_DIR}/${CONFIG_NAME}\"\n  exit 0\nfi\n\n# config generation\ncd /tmp\n\ngsutil cp \"${TRUSTED_NODES_SRC_URL}\" \"${TRUSTED_NODES}\"\n\n# # https://askubuntu.com/a/1175271\n# # replace a matching line with a file content\n# sed  -e \"/^TrustedNodes.*/{r${TRUSTED_NODES}\" -e \"d}\" \"${SRC_DIR}/${CONFIG_NAME}\" > \"${DST_DIR}/${CONFIG_NAME}\"\n#\n# if [ -s \"${DST_DIR}/${CONFIG_NAME}\" ];then\n#   echo \"Resulting config is empty\"\n#   exit 1\n# fi\n\ncp \"${SRC_DIR}/${CONFIG_NAME}\" \"${DST_DIR}/${CONFIG_NAME}\"\necho >> \"${DST_DIR}/${CONFIG_NAME}\"\ncat \"${TRUSTED_NODES}\" >> \"${DST_DIR}/${CONFIG_NAME}\"",
      "init_from_snaphot.sh": "#!/usr/bin/env sh\nset -ex # -e exits on error\n\nDATA_DIR=\"/data\"\nTEST_FILE=\"${DATA_DIR}/.initialized\"\nSNAPSHOT_URL=\"\"\n\nif [ -f ${TEST_FILE} ]; then\n    echo \"Blockchain already initialized. Exiting...\"\n    exit 0\nfi\n\n# Cleanup\nrm -rf ${DATA_DIR}/geth\n\n# Download & extract snapshot\n# special handling of zstd\nif [[ \"${SNAPSHOT_URL}\" =~ \"\\.zst$\" ]]; then\n  wget ${SNAPSHOT_URL} -O - | mbuffer -m5% -q -l /tmp/m1.log | zstd -d -T0 | mbuffer -m5% -q -l /tmp/m2.log | tar -b 2048 --overwrite -x -C ${DATA_DIR}\nelse\n  wget ${SNAPSHOT_URL} -O - | tar --overwrite -x -C ${DATA_DIR}\nfi\n\n\n# Mark data dir as initialized\ntouch ${TEST_FILE}",
      "init_from_rsync.sh": "#!/usr/bin/env sh\nset -ex # -e exits on error\n\nDATA_DIR=\"/data\"\nTEST_FILE=\"${DATA_DIR}/.initialized\"\nSNAPSHOT_URL=\"rsync://192.168.8.4/snapshot/geth/node/geth\"\n\n# get statefulset pod number from pre-defined env var\nSTS_POD_NUMBER=$(echo $MY_POD_NAME|sed -r 's/^.+\\-([0-9]+)$/\\1/')\n#generate variable name to check for URL override, it should be like SNAPSHOT_URL_123, if present\nVAR_NAME=\\$SNAPSHOT_URL_${STS_POD_NUMBER}\n#get the URL, if any\nNEW_URL=$(eval echo $VAR_NAME)\nif [ ! -z \"$NEW_URL\" ];then\n  SNAPSHOT_URL=$NEW_URL\nfi\n\nif [ -f ${TEST_FILE} ]; then\n    echo \"Blockchain already initialized. Exiting...\"\n    exit 0\nfi\n\nset +e\n# remove missing files to cleanup\nrsync -av --inplace --delete-before ${SNAPSHOT_URL}/ ${DATA_DIR}/\n# add more times to catch up\nrsync -av --inplace ${SNAPSHOT_URL}/ ${DATA_DIR}/\nrsync -av --inplace ${SNAPSHOT_URL}/ ${DATA_DIR}/\nrsync -av --inplace ${SNAPSHOT_URL}/ ${DATA_DIR}/\nset -e\n\n# Mark data dir as initialized\ntouch ${TEST_FILE}",
      "init_from_gcs.sh": "#!/usr/bin/env sh\nset -ex # -e exits on error\n\n# env required\n# S3_ENDPOINT_URL # f.e. \"https://storage.googleapis.com\"\n# AWS_ACCESS_KEY_ID\n# AWS_SECRET_ACCESS_KEY\n\nDATA_DIR=\"/data\"\nCHAINDATA_DIR=\"${DATA_DIR}/geth/chaindata\"\nINITIALIZED_FILE=\"${DATA_DIR}/.initialized\"\n#without gs:// or s3://, just a bucket name and path\nINDEX_URL=\"bucket/path/to/file\"\nS5CMD=/s5cmd\nEXCLUDE_ANCIENT='--exclude \"*.cidx\" --exclude \"*.ridx\" --exclude \"*.cdat\" --exclude \"*.rdat\"'\nEXCLUDE_STATE='--exclude \"*.ldb\"'\nINDEX=\"index\"\nS_UPDATING=\"/updating\"\nS_TIMESTAMP=\"/timestamp\"\nS_STATE_URL=\"/state_url\"\nS_ANCIENT_URL=\"/ancient_url\"\nS_STATS=\"/stats\"\n\nif [ -f \"${INITIALIZED_FILE}\" ]; then\n    echo \"Blockchain already initialized. Exiting...\"\n    exit 0\nfi\n\n# we need to create temp files\ncd /tmp\n\n# get index of source base dirs\n${S5CMD} cp \"s3://${INDEX_URL}\" \"${INDEX}\"\n\n# get the most fresh datadir\n# prune time is ignored here, we assume that all datadirs are pruned frequently enough\nGCS_BASE_URL=\"\"\nMAX_TIMESTAMP=1\nfor _GCS_BASE_URL in $(cat ${INDEX});do\n  _TIMESTAMP_URL=\"${_GCS_BASE_URL}${S_TIMESTAMP}\"\n  _TIMESTAMP=$(${S5CMD} cat s3://${_TIMESTAMP_URL})\n  if [ \"${_TIMESTAMP}\" -gt \"${MAX_TIMESTAMP}\" ];then\n    GCS_BASE_URL=\"${_GCS_BASE_URL}\"\n    MAX_TIMESTAMP=${_TIMESTAMP}\n  fi\ndone\n\nif [ \"${GCS_BASE_URL}\" == \"\" ];then\n  echo \"Fatal: cannot pick up correct base url, exiting\"\n  exit 1\nfi\n\n\nUPDATING_URL=\"${GCS_BASE_URL}${S_UPDATING}\"\nTIMESTAMP_URL=\"${GCS_BASE_URL}${S_TIMESTAMP}\"\nSTATS_URL=\"${GCS_BASE_URL}${S_STATS}\"\n\n# get state and ancient sources\nSTATE_URL=\"${GCS_BASE_URL}${S_STATE_URL}\"\nANCIENT_URL=\"${GCS_BASE_URL}${S_ANCIENT_URL}\"\n\n\nSTATE_SRC=\"$(${S5CMD} cat s3://${STATE_URL})\"\nANCIENT_SRC=\"$(${S5CMD} cat s3://${ANCIENT_URL})\"\nREMOTE_STATS=\"$(${S5CMD} cat s3://${STATS_URL})\"\n\n# create dst dirs\nmkdir -p \"${CHAINDATA_DIR}/ancient\"\n\n# save sync source\necho \"${GCS_BASE_URL}\" > \"${DATA_DIR}/source\"\n\nset +e\n\n# some background monitoring for humans\nset +x\nwhile true;do\n  INODES=$(df -Phi ${DATA_DIR} | tail -n 1 | awk '{print $3}')\n  SIZE=$(df -P -BG ${DATA_DIR} | tail -n 1 | awk '{print $3}')G\n  echo -e \"$(date -Iseconds) | SOURCE TOTAL ${REMOTE_STATS} | DST USED Inodes:\\t${INODES} Size:\\t${SIZE}\"\n  sleep 2\ndone &\nMON_PID=$!\nset -x\n\n# get start and stop timestamps from the cloud\nUPDATING_0=\"$(${S5CMD} cat s3://${UPDATING_URL})\"\nTIMESTAMP_0=\"$(${S5CMD} cat s3://${TIMESTAMP_URL})\"\n\n\n# we're ready to perform actual data sync\n\n# we're done when both are true\n# 1) start and stop timestamps did not changed during data sync - no process started or finished updating the cloud\n# 2) 0 objects copied\nSYNC=2\nCLEANUP=1\nwhile [ \"${SYNC}\" -gt 0 ] ; do\n\n    # Cleanup\n    if [ ${CLEANUP} -eq 1 ];then\n      echo \"$(date -Iseconds) Cleaning up local dir ...\"\n      mkdir -p ${DATA_DIR}/geth\n      mv ${DATA_DIR}/geth ${DATA_DIR}/geth.old && rm -rf ${DATA_DIR}/geth.old &\n      CLEANUP=0\n    fi\n\n    # sync from cloud to local disk, without removing existing [missing in the cloud] files\n    # run multiple syncs in background\n\n    # we don't wanna sync ancient data here\n    time ${S5CMD} cp -n -s -u ${EXCLUDE_ANCIENT} s3://${STATE_SRC}/* ${CHAINDATA_DIR}/ > cplist_state.txt &\n    STATE_CP_PID=$!\n    time nice ${S5CMD} cp -n -s -u --part-size 200 --concurrency 2 ${EXCLUDE_STATE} s3://${ANCIENT_SRC}/* ${CHAINDATA_DIR}/ancient/ > cplist_ancient.txt &\n    ANCIENT_CP_PID=$!\n\n    # wait for all syncs to complete\n    # TODO any errors handling here?\n    wait ${STATE_CP_PID} ${ANCIENT_CP_PID}\n\n    # get start and stop timestamps from the cloud after sync\n    UPDATING_1=\"$(${S5CMD} cat s3://${UPDATING_URL})\"\n    TIMESTAMP_1=\"$(${S5CMD} cat s3://${TIMESTAMP_URL})\"\n\n    # compare timestamps before and after sync\n    if [ \"${UPDATING_0}\" -eq \"${UPDATING_1}\" ] && [ \"${TIMESTAMP_0}\" -eq \"${TIMESTAMP_1}\" ];then\n      echo \"Timestamps are equal\"\n      echo -e \"U_0=${UPDATING_0}\\tU_1=${UPDATING_1},\\tT_0=${TIMESTAMP_0}\\tT_1=${TIMESTAMP_1}\"\n      let SYNC=SYNC-1\n    else\n      echo \"Timestamps changed, running sync again ...\"\n      echo -e \"U_0=${UPDATING_0}\\tU_1=${UPDATING_1},\\tT_0=${TIMESTAMP_0}\\tT_1=${TIMESTAMP_1}\"\n      # end  timestamps -> begin timestamps\n      UPDATING_0=${UPDATING_1}\n      TIMESTAMP_0=${TIMESTAMP_1}\n      SYNC=2\n      # hack until we resolve sync up without full destination cleanup\n      CLEANUP=1\n      continue\n    fi\n\n    # stop monitoring\n    if [ ${MON_PID} -ne 0 ];then\n      kill ${MON_PID}\n      MON_PID=0\n    fi\n\n    # get number of objects copied\n    CP_OBJ_NUMBER_STATE=$(wc -l <  cplist_state.txt )\n    CP_OBJ_NUMBER_ANCIENT=$(wc -l < cplist_ancient.txt )\n    #  0 objects copied ?\n    if [ \"${CP_OBJ_NUMBER_STATE}\" -eq 0 ] && [ \"${CP_OBJ_NUMBER_ANCIENT}\" -eq 0 ];then\n      echo -e \"State objects copied:\\t${CP_OBJ_NUMBER_STATE}, ancient objects copied:\\t${CP_OBJ_NUMBER_ANCIENT}\"\n      let SYNC=SYNC-1\n    else\n      echo -e \"State objects copied:\\t${CP_OBJ_NUMBER_STATE}, ancient objects copied:\\t${CP_OBJ_NUMBER_ANCIENT}, running sync again ... \"\n      SYNC=2\n      continue\n    fi\ndone\n\n# Mark data dir as initialized\ntouch ${INITIALIZED_FILE}",
      "sync_to_gcs.sh": "#!/usr/bin/env sh\nset -ex # -e exits on error\n\n# env required\n# S3_ENDPOINT_URL # f.e. \"https://storage.googleapis.com\"\n# AWS_ACCESS_KEY_ID\n# AWS_SECRET_ACCESS_KEY\n# SYNC_TO_GCS True or any other value\n\n# enable sync via env\nif [ \"${SYNC_TO_GCS}\" != \"True\" ];then\n  exit 0\nfi\n\nDATA_DIR=\"/data\"\nCHAINDATA_DIR=\"${DATA_DIR}/geth/chaindata\"\n#without gs:// or s3://, just a bucket name and path\nGCS_BASE_URL=\"bucket/path/to/dir\"\n# GSUTIL=$(which gsutil)\n\nS5CMD=/s5cmd\nCPLOG=\"${DATA_DIR}/cplog.txt\"\nCPLIST=\"${DATA_DIR}/cplist.txt\"\nRMLOG=\"${DATA_DIR}/rmlog.txt\"\nRMLIST=\"${DATA_DIR}/rmlist.txt\"\n\n# s5cmd excludes just by file extension, not by file path\nEXCLUDE_ANCIENT='--exclude \"*.cidx\" --exclude \"*.ridx\" --exclude \"*.cdat\" --exclude \"*.rdat\"'\nEXCLUDE_STATE='--exclude \"*.ldb\"'\n\nS_UPDATING=\"/updating\"\nS_TIMESTAMP=\"/timestamp\"\nS_STATE_URL=\"/state_url\"\nS_ANCIENT_URL=\"/ancient_url\"\nS_STATS=\"/stats\"\n\nif [ \"${GCS_BASE_URL}\" == \"\" ];then\n  echo \"Fatal: cannot use empty base url, exiting\"\n  exit 1\nfi\n\n# we need to create temp files\ncd /tmp\n\n# get timestamp, state and ancient DSTs\nUPDATING_URL=\"${GCS_BASE_URL}${S_UPDATING}\"\nTIMESTAMP_URL=\"${GCS_BASE_URL}${S_TIMESTAMP}\"\nSTATS_URL=\"${GCS_BASE_URL}${S_STATS}\"\n\nSTATE_URL=\"${GCS_BASE_URL}${S_STATE_URL}\"\nANCIENT_URL=\"${GCS_BASE_URL}${S_ANCIENT_URL}\"\n\nSTATE_DST=\"$(${S5CMD} cat s3://${STATE_URL})\"\nANCIENT_DST=\"$(${S5CMD} cat s3://${ANCIENT_URL})\"\n\n# mark begin of sync in the cloud\ndate +%s > updating\n${S5CMD} cp updating \"s3://${UPDATING_URL}\"\n\n# we're ready to perform actual data copy\n\n# sync from local disk to cloud, without removing existing [missing on local disk] files\n# run multiple syncs in background\n# cp is recursive by default, thus we need to exclude ancient data here\ntime ${S5CMD} cp -n -s -u ${EXCLUDE_ANCIENT} \"${CHAINDATA_DIR}/\" \"s3://${STATE_DST}/\"  > cplist_state.txt &\ntime nice ${S5CMD} cp -n -s -u --part-size 200 --concurrency 2 ${EXCLUDE_STATE} \"${CHAINDATA_DIR}/ancient/\" \"s3://${ANCIENT_DST}/\"  > cplist_ancient.txt &\n# wait for all syncs to complete\n# TODO any errors handling here?\nwait\n\n# update timestamp\n# TODO store timestamp inside readinnes check and use it instead of now()\ndate +%s > timestamp\n${S5CMD} cp timestamp \"s3://${TIMESTAMP_URL}\"\n\n# update stats\nINODES=$(df -Phi \"${DATA_DIR}\" | tail -n 1 | awk '{print $3}')\n# force GB output\nSIZE=$(df -P -BG \"${DATA_DIR}\" | tail -n 1 | awk '{print $3}')G\necho -ne \"Inodes:\\t${INODES} Size:\\t${SIZE}\" > stats\n${S5CMD} cp stats \"s3://${STATS_URL}\"\n\n# get number of objects copied\ncat cplist_state.txt cplist_ancient.txt > \"${CPLIST}\"\n# we use a heuristic here - lot of uploaded objects => lot of object to remove in the cloud => we need to generate removal list\nCP_OBJ_NUMBER=$(wc -l < \"${CPLIST}\")\necho \"$(date -Iseconds) Uploaded objects: ${CP_OBJ_NUMBER}\" | tee -a \"${CPLOG}\"\nset +e\nif [ \"${CP_OBJ_NUMBER}\" -gt 1000 ] ;then\n  set -e\n  # s5cmd doesn't support GCS object removal, just generate a list of files to remove via gsutil\n  # removal should be done in another sidecar\n  time $S5CMD --dry-run cp -n ${EXCLUDE_ANCIENT} \"s3://${STATE_DST}/*\" \"${CHAINDATA_DIR}/\" | awk '{print $2}'|sed 's/^s3/gs/' > rmlist.txt\n  time $S5CMD --dry-run cp -n ${EXCLUDE_STATE} \"s3://${ANCIENT_DST}/*\" \"${CHAINDATA_DIR}/ancient/\" | awk '{print $2}'|sed 's/^s3/gs/' >> rmlist.txt\n  echo \"$(date -Iseconds) Objects to remove: $(wc -l < rmlist.txt)\" | tee -a \"${RMLOG}\"\n  cp rmlist.txt \"${RMLIST}\"\nfi",
      "gcs_cleanup.sh": "#!/usr/bin/env sh\nset -ex\n\n# env required\n# AWS_ACCESS_KEY_ID\n# AWS_SECRET_ACCESS_KEY\n\nDATA_DIR=\"/data\"\nRMLIST=\"${DATA_DIR}/rmlist.txt\"\nRMLOG=\"${DATA_DIR}/rmlog.txt\"\nGSUTIL=\"/google-cloud-sdk/bin/gsutil\"\nGCLOUD=\"/google-cloud-sdk/bin/gcloud\"\nBOTOCONFIG=\"${HOME}/.boto\"\n\n# allow container interrupt\ntrap \"{ exit 1; }\" INT TERM\n\n# disable gcloud auth\n${GCLOUD} config set pass_credentials_to_gsutil false\n# using env vars to auth to GCS, as we use these env vars for s5cmd already\nset +x\necho -e \"[Credentials]\\ngs_access_key_id = ${AWS_ACCESS_KEY_ID}\\ngs_secret_access_key = ${AWS_SECRET_ACCESS_KEY}\" > \"${BOTOCONFIG}\"\nset -x\n\n# creating empty list if missing, f.e .no sync-to-gcs were running\ntouch \"${RMLIST}\"\n\nOBJ_TO_REMOVE=$(wc -l < \"${RMLIST}\")\nset +e\nif [ \"${OBJ_TO_REMOVE}\" -gt \"10000\" ];then\n  split -l 2000 \"${RMLIST}\" /tmp/rmlist-\n  find /tmp -name \"rmlist-*\" -print0| xargs -0 -P 50 -n1 -I{} bash -c \"nice ${GSUTIL} rm -f -I < {}\"\n  echo \"$(date -Iseconds) Removed objects: ${OBJ_TO_REMOVE}\" | tee -a \"${RMLOG}\"\nelse\n  if [ \"${OBJ_TO_REMOVE}\" -gt \"0\" ];then\n    time ${GSUTIL} -m rm -I < \"${RMLIST}\"\n    echo \"$(date -Iseconds) Removed objects: ${OBJ_TO_REMOVE}\" | tee -a \"${RMLOG}\"\n  else\n    echo \"No objects to remove\"\n  fi\nfi\n\n# cleanup removal list\ntrue > \"${RMLIST}\"\n\n# sleep in an endless cycle to allow container interrupt\nset +x\nwhile true; do sleep 10;done",
      "prune.sh": "#!/usr/bin/env sh\n\nset -x\n# env required\n# BSC_PRUNE = True or any other value\n\nDATA_DIR=\"/data\"\n\n# TODO\n# mark cloud timestamp as outdated, as we're not going to provide a fresh snapshot soon\n# it should be done by operator - just keep pod up & running for some time and cloud timestamp will lag on it's own\n\nGETH=/usr/local/bin/geth\n\nret=0\n# we need to check env var to start pruning\nif [ \"${BSC_PRUNE}\" == \"True\" ] ; then\n  # background logging\n  tail -F \"${DATA_DIR}/bsc.log\" &\n  $GETH --config=/config/config.toml --datadir=${DATA_DIR} --cache 8192 snapshot prune-state\n  # prune-block will turn our full node into light one actually\n  # $GETH --config=/config/config.toml --datadir=${DATA_DIR} --datadir.ancient=${DATA_DIR}/geth/chaindata/ancient --cache 8192 snapshot prune-block\n  ret=$?\n  if [ \"${ret}\" -eq \"0\" ];then\n    # update timestamp\n    date +%s > \"${DATA_DIR}/prune_timestamp\"\n  fi\nfi\n\nexit $ret",
      "prune_block.sh": "#!/usr/bin/env sh\n\n# script performs BSC block prune from ancientDB, removing [unneeded in some cases] historical blocks\n# good use case is a bootnode w/o RPC\n# block prune should speed up node in runtime as well as allow to use less disk space\n# after block prune the node will not be able to serve RPC on pruned blocks\n# Around last 90000 blocks are kept in the node state before moving on to ancientDB\n\nset -x\n\nDATA_DIR=\"/data\"\n\nGETH=/usr/local/bin/geth\n# how much recent blocks do we need to keep. Default 0 means we clean up ancientDB completely\nBLOCKS_RESERVED=${1:-0}\n\nret=0\n  # background logging\n  tail -F \"${DATA_DIR}/bsc.log\" &\n  # prune-block will turn our full node into light one actually\n  $GETH --config=/config/config.toml --datadir=${DATA_DIR} --datadir.ancient=${DATA_DIR}/geth/chaindata/ancient --cache 8192 snapshot prune-block --block-amount-reserved=${BLOCKS_RESERVED}\n  ret=$?\n  if [ \"${ret}\" -eq \"0\" ];then\n    # update timestamp\n    date +%s > \"${DATA_DIR}/block_prune_timestamp\"\n  fi\n\nexit $ret",
      "cron.sh": "#!/usr/bin/env sh\n\n# possible values are\n# \"sync\" to trigger sync-to-gcs\n# \"prune\" to trigger prune\nMODE=\"${1}\"\nPOD_NAME=release-name-bsc-0\nCONFIGMAP_NAME=\"release-name-env\"\nKUBECTL=$(which kubectl)\n# wait timeout, f.e \"30s\"\nWAIT_TIMEOUT=\"${2}\"\nPATCH_DATA=\"\"\n\ncheck_ret(){\n        ret=\"${1}\"\n        msg=\"${2}\"\n        # allow to override exit code, default value is ret\n        exit_code=${3:-${ret}}\n        if [ ! \"${ret}\" -eq 0 ];then\n                echo \"${msg}\"\n                echo \"return code ${ret}, exit code ${exit_code}\"\n                exit \"${exit_code}\"\n        fi\n}\n\n# input data checking\n[ \"${MODE}\" = \"prune\" ] || [ \"${MODE}\" = \"sync\" ]\ncheck_ret $? \"$(date -Iseconds) Mode value \\\"${MODE}\\\" is incorrect, abort\"\n\n# wait for pod to become ready\necho \"$(date -Iseconds) Waiting ${WAIT_TIMEOUT} for pod ${POD_NAME} to become ready ...\"\n${KUBECTL} wait --timeout=\"${WAIT_TIMEOUT}\" --for=condition=Ready pod \"${POD_NAME}\"\nret=$?\n# exit code override to \"success\"\ncheck_ret \"${ret}\" \"$(date -Iseconds) Pod ${POD_NAME} is not ready, nothing to do, exiting\" 0\n\n# ensuring pod is not terminating now\n# https://github.com/kubernetes/kubernetes/issues/22839\necho \"$(date -Iseconds) Checking for pod ${POD_NAME} to not terminate ...\"\nDELETION_TIMESTAMP=$(${KUBECTL} get -o jsonpath='{.metadata.deletionTimestamp}' pod \"${POD_NAME}\")\nret=$?\ncheck_ret \"${ret}\" \"$(date -Iseconds) Cannot get pod ${POD_NAME}, abort\"\n\n# empty timestamp means that pod is not terminating now\nif [ \"${DELETION_TIMESTAMP}\" = \"\" ];then\n  # we're good to go\n  echo \"$(date -Iseconds) Pod ${POD_NAME} is ready, continuing\"\n\n  case \"${MODE}\" in\n  \"sync\")\n    echo \"$(date -Iseconds) Patching configmap ${CONFIGMAP_NAME} to enable sync and disable prune\"\n    # disable prune, enable sync-to-gcs\n    PATCH_DATA='{\"data\":{\"BSC_PRUNE\":\"False\",\"SYNC_TO_GCS\":\"True\"}}'\n    ;;\n  \"prune\")\n    echo \"$(date -Iseconds) Patching configmap ${CONFIGMAP_NAME} to enable prune and disable sync\"\n    # disable sync-to-gcs, enable prune\n    PATCH_DATA='{\"data\":{\"BSC_PRUNE\":\"True\",\"SYNC_TO_GCS\":\"False\"}}'\n    ;;\n  \"*\")\n     check_ret 1 \"$(date -Iseconds) Mode value \\\"${MODE}\\\" is incorrect, abort\"\n     ;;\n  esac\n  ${KUBECTL} patch configmap \"${CONFIGMAP_NAME}\"  --type merge --patch ${PATCH_DATA}\n  ret=$?\n  check_ret \"${ret}\" \"$(date -Iseconds) Fatal: cannot patch configmap ${CONFIGMAP_NAME}, abort\"\n\n  echo \"$(date -Iseconds) Deleting pod ${POD_NAME} to trigger action inside init container ...\"\n  # delete the pod to trigger action inside init container\n  ${KUBECTL} delete pod \"${POD_NAME}\" --wait=false\n  ret=$?\n  check_ret \"${ret}\" \"$(date -Iseconds) Fatal: cannot delete pod ${POD_NAME}, abort\"\n  echo \"$(date -Iseconds) Pod ${POD_NAME} deleted successfully, exiting. Check pod logs after respawn.\"\nelse\n  # pod is terminating now, try later on next iteration\n  echo \"$(date -Iseconds) Pod ${POD_NAME} is terminating now, nothing to do, exiting\"\n  exit 0\nfi",
      "update-pod-deletion-cost.sh": "#!/usr/bin/env sh\n\n# this script updates \"pod-deletion-cost\" pod annotation based on disk usage\n\n# required env\n# MY_POD_NAME - pod name to annotate\n\n# get used space from this dir's mount point\nDATA_DIR=\"${1}\"\n# sleep between annotate iterations, in 10*second\nINTERVAL=\"${2}\"\n\nKUBECTL=$(which kubectl)\nANNOTATION=\"\"\n\n# allow container interrupts\ntrap \"{ exit 1; }\" INT TERM\n\nwhile [ true ]; do\n  # get dir's mount point usage in MB\n  SIZE=$(df -P -BM \"${DATA_DIR}\" | tail -n 1 | awk '{print $3}'|sed 's/M//g')\n  # use negative values, the bigger is the disk usage the lower is the pod deletion cost\n  # thus the most \"heavy\" pod will be removed first\n  ANNOTATION=\"controller.kubernetes.io/pod-deletion-cost=-${SIZE}\"\n  ${KUBECTL} annotate --overwrite=true pod \"${MY_POD_NAME}\" \"${ANNOTATION}\"\n  ret=$?\n  if [ ${ret} -eq 0 ];then\n    echo \"$(date -Iseconds) Annotated pod ${MY_POD_NAME} with ${ANNOTATION}\"\n  else\n    echo \"$(date -Iseconds) Error annotating pod ${MY_POD_NAME} with ${ANNOTATION}\"\n  fi\n  # we need to sleep <short-delay> inside cycle to handle pod termination w/o delays\n  for i in $(seq 2 \"${INTERVAL}\");do sleep 10;done\ndone",
      "controller-hook.sh": "#!/usr/bin/env bash\n\n# This is needed to work-around GKE local SSD scale up issue https://github.com/kubernetes/autoscaler/issues/2145\n\n# watch for scale-related changes in the specified controller\n# in scale up case -> scale up secondary controller to the same replica value\n# in scale down case -> scale down secondary controller to 0\n\ncontroller=StatefulSet\napiVersion=\"apps/v1\"\n\nwatchName=\"OnModified${controller}\"\ndebugLog=\"/tmp/debug.log\"\n\nif [[ $1 == \"--config\" ]] ; then\n  cat <<EOF\nconfigVersion: v1\nkubernetes:\n- name: \"${watchName}\"\n  apiVersion: ${apiVersion}\n  kind: $controller\n  executeHookOnEvent: [\"Modified\"]\n  labelSelector:\n    matchLabels:\n      app.kubernetes.io/name: bsc\n      app.kubernetes.io/instance: release-name\n  namespace:\n    nameSelector:\n      matchNames: [\"default\"]\n  jqFilter: \".spec.replicas\"\nEOF\nelse\n\t# ignore Synchronization for simplicity\n\ttype=$(jq -r '.[0].type' \"${BINDING_CONTEXT_PATH}\")\n\tif [[ \"${type}\" == \"Synchronization\" ]] ; then\n\t  echo Got Synchronization event\n\t  exit 0\n\tfi\n  \tARRAY_COUNT=$(jq -r '. | length-1' \"${BINDING_CONTEXT_PATH}\")\n\tfor i in $(seq 0 \"${ARRAY_COUNT}\");do\n\t\tbindingName=$(jq -r \".[$i].binding\" \"${BINDING_CONTEXT_PATH}\")\n\t\tresourceEvent=$(jq -r \".[$i].watchEvent\" \"${BINDING_CONTEXT_PATH}\")\n\t\tresourceName=$(jq -r \".[$i].object.metadata.name\" \"${BINDING_CONTEXT_PATH}\")\n\t\tif [[ \"${bindingName}\" == \"${watchName}\" && \"${resourceEvent}\" == \"Modified\" ]] ; then\n\t\t  \techo \"${controller} ${resourceName} was scaled\"\n\t\t  \tnewReplicas=$(jq -r \".[$i].object.spec.replicas\" \"${BINDING_CONTEXT_PATH}\")\n        oldReplicas=$(jq -r \".[$i].object.status.replicas\" \"${BINDING_CONTEXT_PATH}\")\n        addController=$(jq -r \".[$i].object.metadata.annotations.additionalControllerName\" \"${BINDING_CONTEXT_PATH}\")\n        namespace=$(jq -r \".[$i].object.metadata.namespace\" \"${BINDING_CONTEXT_PATH}\")\n        if [[ \"${newReplicas}\" -gt \"${oldReplicas}\" ]];then\n          echo \"$(date -Iseconds) scale UP ${controller} ${addController}, new=${newReplicas}, old=${oldReplicas}\" | tee -a \"${debugLog}\"\n          kubectl --namespace \"${namespace}\" scale \"${controller}\" \"${addController}\" --replicas=\"${newReplicas}\"\n        fi\n        if [[ \"${newReplicas}\" -lt \"${oldReplicas}\" ]];then\n          echo \"$(date -Iseconds) scale DOWN ${controller} ${addController} to 0, new=${newReplicas}, old=${oldReplicas}\" | tee -a \"${debugLog}\"\n          kubectl --namespace \"${namespace}\" scale \"${controller}\" \"${addController}\" --replicas=0\n        fi\n\t\tfi\n\tdone\n#\techo  \"$(date -Iseconds)\"  >> \"${debugLog}\"\n#\tcat \"${BINDING_CONTEXT_PATH}\" | jq . >> \"${debugLog}\"\nfi",
      "rotate.sh": "#!/usr/bin/env bash\n\n# this script rotates bootnodes - replaces old node with a new one\n# scale up a new controller\n# wait a couple hours for bootnode to grab some p2p peers\n# ensure node is up & synced using JSON RPC via k8s service\n# scale down an old controller\n\n# We have to use 1 controller per bootnode due to external static IP and static node ID\n\n# f.e statefulset/bootnode-0 or cloneset/bootnode-1\nNEW_CONTROLLER=\"${1}\"\n#\nOLD_CONTROLLER=\"${2}\"\n# bsc rpc endpoint to check, f.e. bsc-bootnode-0:8575, where bsc-bootnode-0 is a k8s service name pointing to a single node\nRPC_ENDPOINT=\"${3}\"\n# node is allowed to lag for seconds\nMAX_NODE_LAG=120\n# wait for the new node to spin up and sync up\n# ensure that cronjob's activeDeadlineSeconds is greater than this value\nWAIT_TIME=7200\n\nKUBECTL=$(which kubectl)\nCURL=$(which curl)\nJQ=$(which jq)\n\ncheck_ret(){\n        ret=\"${1}\"\n        msg=\"${2}\"\n        # allow to override exit code, default value is ret\n        exit_code=${3:-${ret}}\n        if [ ! \"${ret}\" -eq 0 ];then\n                echo \"${msg}\"\n                echo \"return code ${ret}, exit code ${exit_code}\"\n                exit \"${exit_code}\"\n        fi\n}\n\n# spinning up a new node\necho \"$(date -Iseconds) scaling ${NEW_CONTROLLER} up to 1\"\n${KUBECTL} scale \"${NEW_CONTROLLER}\" --replicas=1\ncheck_ret $? \"$(date -Iseconds) FATAL: cannot scale ${NEW_CONTROLLER} to 1 replica\" 1\n\n# wait for a new node to sync up\necho \"$(date -Iseconds) sleeping ${WAIT_TIME} seconds\"\nsleep ${WAIT_TIME}\necho \"$(date -Iseconds) check node readinnes\"\n\n# health check\n# cannot use existing check_node_readiness.sh as there is no bsc binary in kubectl docker image\n# get latest block and parse it's timestamp via jq\nJSON_RPC_REQUEST='{\"jsonrpc\":\"2.0\",\"method\":\"eth_getBlockByNumber\",\"params\":[\"latest\",false],\"id\":1}'\nLATEST_BLOCK_TIMESTAMP_HEX=$(${CURL} -s --data-binary ${JSON_RPC_REQUEST} -H 'Content-Type: application/json' \"${RPC_ENDPOINT}\"|${JQ} -r .result.timestamp)\n[ -n \"${LATEST_BLOCK_TIMESTAMP_HEX}\" ]\ncheck_ret $? \"$(date -Iseconds) FATAL: node ${NEW_CONTROLLER} did not pass the health check - empty LATEST_BLOCK_TIMESTAMP_HEX\" 2\necho \"$(date -Iseconds) Latest block timestamp hex ${LATEST_BLOCK_TIMESTAMP_HEX}\"\n\n# convert timestamp from hex to dec\nLATEST_BLOCK_TIMESTAMP=$(printf '%d' \"${LATEST_BLOCK_TIMESTAMP_HEX}\")\n[ -n \"${LATEST_BLOCK_TIMESTAMP}\" ]\ncheck_ret $? \"$(date -Iseconds) FATAL: node ${NEW_CONTROLLER} did not pass the health check - empty LATEST_BLOCK_TIMESTAMP\" 3\necho \"$(date -Iseconds) Latest block timestamp ${LATEST_BLOCK_TIMESTAMP}\"\n\n# is node synced up ?\n[[ $(($(date +%s) - ${LATEST_BLOCK_TIMESTAMP})) -le ${MAX_NODE_LAG} ]]\ncheck_ret $? \"$(date -Iseconds) FATAL: node ${NEW_CONTROLLER} timestamp lag is greater than ${MAX_NODE_LAG}, ts=${LATEST_BLOCK_TIMESTAMP}, now=$(date +%s)\" 4\necho \"$(date -Iseconds) node ${NEW_CONTROLLER} timestamp ${LATEST_BLOCK_TIMESTAMP} is fresh, now=$(date +%s)\"\n\n# scaling down an old node\necho \"$(date -Iseconds) scaling ${OLD_CONTROLLER} down to 0\"\n${KUBECTL} scale \"${OLD_CONTROLLER}\" --replicas=0\ncheck_ret $? \"$(date -Iseconds) FATAL: cannot scale ${OLD_CONTROLLER} to 0 replica\" 5\nexit 0"
    }
  },
  {
    "apiVersion": "v1",
    "kind": "Service",
    "metadata": {
      "name": "release-name",
      "labels": {
        "app.kubernetes.io/name": "bsc",
        "helm.sh/chart": "bsc-0.6.10",
        "app.kubernetes.io/instance": "release-name",
        "app.kubernetes.io/version": "v1.1.8",
        "app.kubernetes.io/managed-by": "Helm"
      }
    },
    "spec": {
      "type": "ClusterIP",
      "ports": [
        {
          "port": 8575,
          "name": "jsonrpc",
          "protocol": "TCP"
        },
        {
          "port": 8576,
          "name": "web-socket",
          "protocol": "TCP"
        },
        {
          "port": 8577,
          "name": "qraphql",
          "protocol": "TCP"
        },
        {
          "port": 30311,
          "name": "p2p",
          "protocol": "TCP"
        },
        {
          "port": 30311,
          "name": "p2p-discovery",
          "protocol": "UDP"
        },
        {
          "port": 9368,
          "name": "metrics",
          "protocol": "TCP"
        },
        {
          "port": 6060,
          "name": "bsc-metrics",
          "protocol": "TCP",
          "targetPort": 6060
        }
      ],
      "selector": {
        "app.kubernetes.io/name": "bsc",
        "app.kubernetes.io/instance": "release-name",
        "manualstatus": "in-service"
      }
    }
  },
  {
    "apiVersion": "apps/v1",
    "kind": "StatefulSet",
    "metadata": {
      "name": "release-name-bsc",
      "labels": {
        "app.kubernetes.io/name": "bsc",
        "helm.sh/chart": "bsc-0.6.10",
        "app.kubernetes.io/instance": "release-name",
        "app.kubernetes.io/version": "v1.1.8",
        "app.kubernetes.io/managed-by": "Helm"
      },
      "annotations": null
    },
    "spec": {
      "replicas": 1,
      "updateStrategy": {
        "type": "RollingUpdate"
      },
      "serviceName": "release-name-service",
      "podManagementPolicy": "OrderedReady",
      "selector": {
        "matchLabels": {
          "app.kubernetes.io/name": "bsc",
          "app.kubernetes.io/instance": "release-name"
        }
      },
      "template": {
        "metadata": {
          "labels": {
            "app.kubernetes.io/name": "bsc",
            "app.kubernetes.io/instance": "release-name",
            "bsc/chain": "mainnet",
            "bsc/role": "rpc",
            "manualstatus": "in-service"
          },
          "annotations": {
            "checksum/config": "83b9ad4b1cec55f9fa8ee3f42487964d78ba4739b5690d468fd05004e60cf465"
          }
        },
        "spec": {
          "serviceAccountName": "release-name-bsc",
          "securityContext": {
            "fsGroup": 101,
            "runAsGroup": 101,
            "runAsUser": 101
          },
          "affinity": {
            "podAntiAffinity": {
              "preferredDuringSchedulingIgnoredDuringExecution": [
                {
                  "podAffinityTerm": {
                    "labelSelector": {
                      "matchLabels": {
                        "app.kubernetes.io/name": "bsc",
                        "bsc/chain": "mainnet"
                      }
                    },
                    "topologyKey": "failure-domain.beta.kubernetes.io/zone"
                  },
                  "weight": 100
                }
              ]
            }
          },
          "terminationGracePeriodSeconds": 180,
          "containers": [
            {
              "name": "bsc",
              "image": "dysnix/bsc:latest",
              "imagePullPolicy": "Always",
              "args": [
                "--config=/config/config.toml",
                "--datadir=/data",
                "--syncmode=full",
                "--gcmode=full",
                "--maxpeers=50",
                "--cache=8192",
                "--snapshot=false",
                "--pipecommit=false",
                "--persistdiff=false",
                "--diffblock=86400",
                "--port=30311",
                "--rpc.allow-unprotected-txs",
                "--txlookuplimit=0",
                "--cache.preimages",
                "--diffsync",
                "--metrics",
                "--pprof",
                "--pprof.addr=0.0.0.0",
                "--pprof.port=6060"
              ],
              "envFrom": [
                {
                  "configMapRef": {
                    "name": "release-name-env"
                  }
                }
              ],
              "workingDir": "/data",
              "resources": null,
              "ports": [
                {
                  "containerPort": 8575,
                  "name": "jsonrpc",
                  "protocol": "TCP"
                },
                {
                  "containerPort": 8576,
                  "name": "web-socket",
                  "protocol": "TCP"
                },
                {
                  "containerPort": 8577,
                  "name": "qraphql",
                  "protocol": "TCP"
                },
                {
                  "containerPort": 30311,
                  "hostPort": 30311,
                  "name": "p2p",
                  "protocol": "TCP"
                },
                {
                  "containerPort": 30311,
                  "hostPort": 30311,
                  "name": "p2p-discovery",
                  "protocol": "UDP"
                },
                {
                  "containerPort": 9368,
                  "name": "metrics",
                  "protocol": "TCP"
                }
              ],
              "volumeMounts": [
                {
                  "name": "generated-bsc-config",
                  "mountPath": "/config"
                },
                {
                  "name": "scripts",
                  "mountPath": "/scripts"
                },
                {
                  "name": "probe-env",
                  "mountPath": "/env"
                },
                {
                  "name": "bsc-pvc",
                  "mountPath": "/data"
                }
              ],
              "startupProbe": {
                "exec": {
                  "command": [
                    "/bin/sh",
                    "-c",
                    "source /env/env.txt && /bin/sh /scripts/check_node_health.sh http://127.0.0.1:8575 $StartupProbeTimestampDistinct last_synced_block.txt"
                  ]
                },
                "initialDelaySeconds": 60,
                "periodSeconds": 10,
                "timeoutSeconds": 10,
                "failureThreshold": 12
              }
            },
            {
              "name": "logger",
              "image": "krallin/ubuntu-tini:latest",
              "imagePullPolicy": "IfNotPresent",
              "args": [
                "tail",
                "-F",
                "/data/bsc.log"
              ],
              "lifecycle": {
                "preStop": {
                  "exec": {
                    "command": [
                      "/bin/sh",
                      "-c",
                      "sleep 30"
                    ]
                  }
                }
              },
              "workingDir": "/data",
              "volumeMounts": [
                {
                  "name": "bsc-pvc",
                  "mountPath": "/data",
                  "readOnly": true
                }
              ]
            }
          ],
          "initContainers": [
            {
              "name": "remove-lock",
              "command": [
                "rm",
                "-f",
                "/data/geth/LOCK"
              ],
              "image": "busybox",
              "imagePullPolicy": "IfNotPresent",
              "volumeMounts": [
                {
                  "name": "bsc-pvc",
                  "mountPath": "/data"
                }
              ]
            },
            {
              "name": "generate-bsc-config",
              "command": [
                "sh",
                "/scripts/generate_node_config.sh"
              ],
              "env": [
                {
                  "name": "GENERATE_CONFIG",
                  "value": "false"
                },
                {
                  "name": "HOME",
                  "value": "/tmp"
                }
              ],
              "image": "google/cloud-sdk:alpine",
              "imagePullPolicy": "IfNotPresent",
              "volumeMounts": [
                {
                  "name": "bsc-config",
                  "mountPath": "/config"
                },
                {
                  "name": "generated-bsc-config",
                  "mountPath": "/generated-config"
                },
                {
                  "name": "scripts",
                  "mountPath": "/scripts"
                }
              ]
            }
          ],
          "volumes": [
            {
              "name": "bsc-config",
              "configMap": {
                "name": "release-name-config"
              }
            },
            {
              "name": "generated-bsc-config",
              "emptyDir": {}
            },
            {
              "name": "scripts",
              "configMap": {
                "name": "release-name-scripts"
              }
            },
            {
              "name": "probe-env",
              "configMap": {
                "name": "release-name-probe-env"
              }
            }
          ]
        }
      },
      "volumeClaimTemplates": [
        {
          "metadata": {
            "name": "bsc-pvc",
            "labels": {
              "app": "bsc"
            }
          },
          "spec": {
            "accessModes": [
              "ReadWriteOnce"
            ],
            "resources": {
              "requests": {
                "storage": "1000Gi"
              }
            },
            "volumeMode": "Filesystem"
          }
        }
      ]
    }
  },
  null,
  null
]